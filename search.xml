<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>经典面试题：用户反映你开发的网站访问很慢可能会是什么原因</title>
      <link href="/2019/09/26/%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E7%94%A8%E6%88%B7%E5%8F%8D%E6%98%A0%E4%BD%A0%E5%BC%80%E5%8F%91%E7%9A%84%E7%BD%91%E7%AB%99%E8%AE%BF%E9%97%AE%E5%BE%88%E6%85%A2%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%98%AF%E4%BB%80%E4%B9%88%E5%8E%9F%E5%9B%A0/"/>
      <url>/2019/09/26/%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%9A%E7%94%A8%E6%88%B7%E5%8F%8D%E6%98%A0%E4%BD%A0%E5%BC%80%E5%8F%91%E7%9A%84%E7%BD%91%E7%AB%99%E8%AE%BF%E9%97%AE%E5%BE%88%E6%85%A2%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%98%AF%E4%BB%80%E4%B9%88%E5%8E%9F%E5%9B%A0/</url>
      
        <content type="html"><![CDATA[<p>一、针对这个题目我们可以简单理解成是server端出现的问题，而不是client端出现了问题（用户网络不好包括域名服务器解析等可能），当然面试官要考你用户端的知识，例如域名解析，也是有挺多可以考到的知识点，但单就这个问题，更强调的是server端的知识点。下面逐一来剖析可能的原因：<br><a id="more"></a><br>（1）可能的原因一：服务器出口带宽不够用。这是一个很常见的瓶颈。一方面，可能是本身购买的服务器出口带宽就很小（企业购买带宽相当昂贵），一旦用户访问量上来了，并发量大了，自然均分给用户的出口带宽就更小了，所以某些用户的访问速度就会下降了很多。另一个，就是跨运营商网络导致带宽缩减，例如很多公司的网站（服务器）是放在电信的网络上的，而如果用户这边对接的是长城或者说联通的宽带，运营商之间网络传输在对接时是会有限制的，这就可能导致带宽的缩减。</p><p>（2）可能原因二：服务器负载过大忙不过来，比如说CPU和内存消耗完了，这个容易理解，不展开。</p><p>（3）可能原因三：网站的开发代码没写好，例如mysql语句没有进行优化，导致数据库的读写相当耗费时间。</p><p>（4）可能原因四：数据库的瓶颈，也是很常见的一个瓶颈，这点跟上面第三个原因可以一起来说。当我们的数据库变得愈发庞大，比如好多G好多T这么大，那对于数据库的读写就会变得相当缓慢了，索引优化固然能提升一些效率，但数据库已经如此庞大的话，如果每次查询都对这么大的数据库进行全局查询，自然会很慢。这个学过数据库的话也是挺容易理解的。</p><p>二、针对上面可能的原因，有哪些方法和工具去检测呢：</p><p>（1）某个用户反馈网站访问变慢，怎么去定位问题。首先你自己也打开下网站，看是否会出现用户反映的问题，如果你这边访问没问题，那就可能是用户那边的问题了，这块就是要先确定是用户那一方的问题还是自身比如说服务器或者网站的问题。</p><p>（2）发现确实是自己服务器或者网站的问题，那么可以利用浏览器的调试功能（一般浏览器都会有），调试网络看看各种数据加载的速度，哪一项消耗了多少时间都可以看到，是哪块数据耗时过多，是图片加载太慢，还是某些数据加载老半天都查不出来。</p><p>（3）然后针对服务器的负载情况，可以去查看下服务器硬件（网络带宽、CPU、内存）的消耗状况。带宽方面查看流量监控看是不是已经到了峰值，带宽不够用了，如果是公司自己买服务器搭的网站服务器的话，需要自己搭建监控环境；如果用的是阿里云腾讯云这些的，那这些平台那边会提供各方面的监控比如CPU、带宽等等，在后台就可以看到了。</p><p>（4）如果发现硬件资源消耗都不高，都比较充裕，那要去看看是不是程序的问题了。这个可以通过查日志来找，比如PHP日志、Apache日志、mysql日志等等的错误日志，特别如mysql有个慢查询的日志功能，可以看到是不是某条mysql语句特别慢，如果某条语句花的时间太长，那这条语句很有可能有问题。</p><p>（5）至于说到的数据库太庞大，这个直接看就看得到了，比如一个表的文件大小变得特别大了。</p><p>三、针对上面的这些问题，有哪些解决和优化的办法呢：</p><p>（1）出口带宽的问题，这个很简单，加带宽，有钱就多买带宽，很简单。</p><p>（2）mysql语句优化，开发人员职责。</p><p>（3）数据库太庞大，为了读写速度，进行“拆表”、“拆库”，就是把数据表或者数据库进行拆分。</p><p>（4）上面的拆库拆表都是针对数据库实在太庞大才会这样做，一般在此之前会有其他优化方法，比如mysql的主从复制，一台主服务器专门用于写，然后其他从服务器用来读，写完之后会同步更新到其他读的服务器中。例如阿里的双十一活动，都不知道用了多少万台服务器一起在扛着。</p><p>（6）还有这几年用得比较多的非关系型数据库，它使用了缓存机制，它把数据缓存到了内存，用户访问数据直接从内存读取，读取速度就比在磁盘中读取快了很多，还有它的一个key-value读取机制，这个听师兄说之后没听懂。</p><p>（7）CDN（content-delivery-network：内容分发网络），鸡蛋放在多个篮子里，把数据放在离用户更近的位置（例如网站的一些静态文件比如图片或者js脚本），用户访问时判断IP来源是广州，那就通过智能DNS解析到广州的服务器上，直接从广州的篮子里去获取数据，速度就快了。这里有个静态数据和动态数据的概念，例如图片和一些js文件一般是不变的，那就可以把它们的映像分布到全国各地，加快速度，而一些需要在网站后台动态产生的一些数据，则需要去到网站所在的服务器去产生并得到。这个涉及到两种数据的显示的问题，这就交由浏览器处理了。同时异步加载的技术例如前端的Ajax技术，异步请求数据，可以使这些动态数据延迟加载，这块自己不怎么了解，可能表述不好。前端开发的人员应该更懂一些。</p><p>（8）上面都没有说到架构的优化，如果网站扛不住，是不是网站架构已经不能适应了，比如做个小博客把数据库服务器和web服务器都用同一台服务器，那所有负载都在同一台服务器上了。但是访问量上来扛不住了，就得加服务器了，就得在架构上优化了，比如在数据库上做集群，在web服务器上也做集群，比如web服务器集群，在服务器前面加一个负载均衡，负载均衡就是专门负责分发，把用户的请求均匀分布到各个服务器上。</p><p>原文地址：<a href="https://www.cnblogs.com/xmyfsj/p/10955175.html" target="_blank" rel="noopener">https://www.cnblogs.com/xmyfsj/p/10955175.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息中间件这么多，到底应该如何选型</title>
      <link href="/2019/09/11/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E8%BF%99%E4%B9%88%E5%A4%9A%EF%BC%8C%E5%88%B0%E5%BA%95%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E9%80%89%E5%9E%8B/"/>
      <url>/2019/09/11/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E8%BF%99%E4%B9%88%E5%A4%9A%EF%BC%8C%E5%88%B0%E5%BA%95%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E9%80%89%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>当前使用较多的消息队列有 RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、MetaMQ 等，而部分数据库如 Redis、MySQL 以及 PhxSQL 也可实现消息队列的功能。<br><a id="more"></a><br><a href="https://s4.51cto.com/oss/201909/11/bb354b49f3f12a40d6ca7ee9fdf8ea66.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201909/11/bb354b49f3f12a40d6ca7ee9fdf8ea66.jpg" alt=""></a></p><p><strong>消息队列概述</strong></p><p>消息队列是指利用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。</p><p><a href="https://s2.51cto.com/oss/201909/11/4ce7cdf77a72323f1c091943a4ca7f68.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201909/11/4ce7cdf77a72323f1c091943a4ca7f68.jpg" alt=""></a></p><p>通过提供消息传递和消息排队模型，它可以在分布式环境下提供应用解耦、弹性伸缩、冗余存储、流量削峰、异步通信、数据同步等等功能，其作为分布式系统架构中的一个重要组件，有着举足轻重的地位。</p><p><strong>消息队列的特点</strong></p><p><strong>采用异步处理模式</strong></p><p>消息发送者可以发送一个消息而无须等待响应。消息发送者将消息发送到一条虚拟的通道(主题或队列)上，消息接收者则订阅或是监听该通道。</p><p>一条信息可能最终转发给一个或多个消息接收者，这些接收者都无需对消息发送者做出同步回应。整个过程都是异步的。</p><p><strong>应用系统之间解耦合</strong></p><p>主要体现在如下两点：</p><ul><li>发送者和接受者不必了解对方、只需要确认消息。</li><li>发送者和接受者不必同时在线。</li></ul><p>比如在线交易系统为了保证数据的最终一致，在支付系统处理完成后会把支付结果放到消息中间件里，通知订单系统修改订单支付状态。两个系统是通过消息中间件解耦的。</p><p><strong>消息队列的传递服务模型</strong></p><p>消息队列的传递服务模型如下图所示：</p><p><a href="https://s3.51cto.com/oss/201909/11/c9cc86ce0cc53bef41fc692b2e63245a.jpg-wh_600x-s_2859406581.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201909/11/c9cc86ce0cc53bef41fc692b2e63245a.jpg-wh_600x-s_2859406581.jpg" alt=""></a></p><p><strong>消息队列的的传输模式</strong></p><p><strong>点对点模型</strong></p><p>点对点模型用于消息生产者和消息消费者之间点到点的通信。消息生产者将消息发送到由某个名字标识的特定消费者。</p><p>这个名字实际上对于消费服务中的一个 队列( Queue)，在消息传递给消费者之前它被存储在这个队列中。</p><p>队列消息可以放在内存中也可以持久化，以保证在消息服务出现故障时仍然能够传递消息。</p><p>传统的点对点消息中间件通常由消息队列服务、消息传递服务、消息队列和消息应用程序接口 API 组成。</p><p>其典型的结构如下图所示：</p><p><a href="https://s5.51cto.com/oss/201909/11/19ab0e9a60ebe5eae4fd83ec99a8c520.jpg-wh_600x-s_1486546129.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201909/11/19ab0e9a60ebe5eae4fd83ec99a8c520.jpg-wh_600x-s_1486546129.jpg" alt=""></a></p><p>特点如下：</p><ul><li>每个消息只用一个消费者。</li><li>发送者和接受者没有时间依赖。</li><li>接受者确认消息接受和处理成功。</li></ul><p>示意图如下所示：</p><p><a href="https://s5.51cto.com/oss/201909/11/430e208cdf7e7e33f132b88d08480041.jpg-wh_600x-s_1939230683.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201909/11/430e208cdf7e7e33f132b88d08480041.jpg-wh_600x-s_1939230683.jpg" alt=""></a></p><p><strong>发布/订阅模型(Pub/Sub)</strong></p><p>发布者/订阅者模型支持向一个特定的消息主题生产消息。0 或多个订阅者可能对接收来自特定消息主题的消息感兴趣。</p><p>在这种模型下，发布者和订阅者彼此不知道对方，就好比是匿名公告板。这种模式被概括为：多个消费者可以获得消息，在发布者和订阅者之间存在时间依赖性。</p><p>发布者需要建立一个订阅( Subscription)，以便消费者能够订阅。订阅者必须保持持续的活动状态并接收消息。</p><p>在这种情况下，在订阅者未连接时，发布的消息将在订阅者重新连接时重新发布，如下图所示：</p><p><a href="https://s5.51cto.com/oss/201909/11/5cf1de6d8ba4a364244007aec7f665a1.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201909/11/5cf1de6d8ba4a364244007aec7f665a1.jpg" alt=""></a></p><p>特性如下：</p><ul><li>每个消息可以有多个订阅者。</li><li>客户端只有订阅后才能接收到消息。</li><li>持久订阅和非持久订阅。</li></ul><p>注意以下三点：</p><ul><li>发布者和订阅者有时间依赖：接受者和发布者只有建立订阅关系才能收到消息。</li><li>持久订阅：订阅关系建立后，消息就不会消失，不管订阅者是否都在线。</li><li>非持久订阅：订阅者为了接受消息，必须一直在线。当只有一个订阅者时约等于点对点模式。</li></ul><p><strong>消息队列应用场景</strong></p><p>当你需要使用消息队列时，首先需要考虑它的必要性。可以使用消息队列的场景有很多，最常用的几种，是做应用程序松耦合、异步处理模式、发布与订阅、最终一致性、错峰流控和日志缓冲等。</p><p>反之，如果需要强一致性，关注业务逻辑的处理结果，则使用 RPC 显得更为合适。</p><p><strong>异步处理</strong></p><p>非核心流程异步化，减少系统响应时间，提高吞吐量。例如：短信通知、终端状态推送、App 推送、用户注册等。</p><p>消息队列 一般都内置了高效的通信机制，因此也可以用于单纯的消息通讯，比如实现点对点消息队列或者聊天室等。</p><p>应用案例：网站用户注册，注册成功后会过一会发送邮件确认或者短信。</p><p><a href="https://s4.51cto.com/oss/201909/11/dd7b6f707993d6a26d614b988158cd2b.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201909/11/dd7b6f707993d6a26d614b988158cd2b.jpg" alt=""></a></p><p><strong>系统解耦</strong></p><p>系统之间不是强耦合的，消息接受者可以随意增加，而不需要修改消息发送者的代码。</p><p>消息发送者的成功不依赖消息接受者(比如：有些银行接口不稳定，但调用方并不需要依赖这些接口)。</p><p>不强依赖于非本系统的核心流程，对于非核心流程，可以放到消息队列中让消息消费者去按需消费，而不影响核心主流程。</p><p><strong>最终一致性</strong></p><p>最终一致性不是消息队列的必备特性，但确实可以依靠消息队列来做最终一致性的事情：</p><p>先写消息再操作，确保操作完成后再修改消息状态。定时任务补偿机制实现消息可靠发送接收、业务操作的可靠执行，要注意消息重复与幂等设计。</p><p>所有不保证 100% 不丢消息的消息队列，理论上无法实现最终一致性。</p><p>像 Kafka 一类的设计，在设计层面上就有丢消息的可能(比如定时刷盘，如果掉电就会丢消息)。哪怕只丢千分之一的消息，业务也必须用其他的手段来保证结果正确。</p><p><strong>广播</strong></p><p>生产者/消费者模式，只需要关心消息是否送达队列，至于谁希望订阅和需要消费，是下游的事情，无疑极大地减少了开发和联调的工作量。</p><p><a href="https://s2.51cto.com/oss/201909/11/271d0e11f3454f29754211824446021e.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201909/11/271d0e11f3454f29754211824446021e.jpg" alt=""></a></p><p><strong>流量削峰和流控</strong></p><p>当上下游系统处理能力存在差距的时候，利用消息队列做一个通用的 “漏斗”，进行限流控制。在下游有能力处理的时候，再进行分发。</p><p>举个例子：用户在支付系统成功结账后，订单系统会通过短信系统向用户推送扣费通知。</p><p>短信系统可能由于短板效应，速度卡在网关上(每秒几百次请求)，跟前端的并发量不是一个数量级。于是，就造成支付系统和短信系统的处理能力出现差异化。</p><p>然而用户晚上个半分钟左右收到短信，一般是不会有太大问题的。如果没有消息队列，两个系统之间通过协商、滑动窗口等复杂的方案也不是说不能实现。</p><p>但系统复杂性指数级增长，势必在上游或者下游做存储，并且要处理定时、拥塞等一系列问题。</p><p>而且每当有处理能力有差距的时候，都需要单独开发一套逻辑来维护这套逻辑。</p><p>所以，利用中间系统转储两个系统的通信内容，并在下游系统有能力处理这些消息的时候，再处理这些消息，是一套相对较通用的方式。</p><p><a href="https://s3.51cto.com/oss/201909/11/6d49e986702ed7ba71d4df0bb9d278a4.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201909/11/6d49e986702ed7ba71d4df0bb9d278a4.jpg" alt=""></a></p><p>应用案例：</p><ul><li>把消息队列当成可靠的消息暂存地，进行一定程度的消息堆积。</li><li>定时进行消息投递，比如模拟用户秒杀访问，进行系统性能压测。</li></ul><p><strong>日志处理</strong></p><p>将消息队列用在日志处理中，比如 Kafka 的应用，解决海量日志传输和缓冲的问题。</p><p>应用案例：把日志进行集中收集，用于计算 PV、用户行为分析等等。</p><p><a href="https://s4.51cto.com/oss/201909/11/bba38195714603fa5f80b41a6f1d62b6.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201909/11/bba38195714603fa5f80b41a6f1d62b6.jpg" alt=""></a></p><p><strong>消息通讯</strong></p><p>消息队列一般都内置了高效的通信机制，因此也可以用于单纯的消息通讯，比如实现点对点消息队列或者聊天室等。</p><p><strong>消息队列的推拉模型</strong></p><p><strong>Push 推消息模型</strong></p><p>消息生产者将消息发送给消息队列，消息队列又将消息推给消息消费者。</p><p><strong>Pull 拉消息模型</strong></p><p>消费者请求消息队列接受消息，消息生产者从消息队列中拉该消息。</p><p><strong>两种类型的区别</strong></p><p>两种类型的区别如下图：</p><p><a href="https://s4.51cto.com/oss/201909/11/c249620e35570c4050448802e92b5a37.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201909/11/c249620e35570c4050448802e92b5a37.jpg" alt=""></a></p><p><strong>消息队列技术对比</strong></p><p>本部分主要介绍四种常用的消息队列( ActiveMQ/RabbitMQ/RocketMQ/Kafka)的主要特性、优点、缺点。</p><p><strong>ActiveMQ</strong></p><p>ActiveMQ 是由 Apache 出品， ActiveMQ 是一个完全支持 JMS1.1 和 J2EE1.4 规范的 JMS Provider 实现。</p><p>它非常快速，支持多种语言的客户端和协议，而且可以非常容易的嵌入到企业的应用环境中，并有许多高级功能。</p><p><a href="https://s2.51cto.com/oss/201909/11/b7b74444a26cafaa7fd0de305f022bcc.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201909/11/b7b74444a26cafaa7fd0de305f022bcc.jpg" alt=""></a></p><p>主要特性：</p><ul><li>服从 JMS 规范：JMS 规范提供了良好的标准和保证，包括：同步或异步的消息分发，一次和仅一次消息分发，消息接收和订阅等等。</li><li>遵从 JMS 规范的好处在于，不论使用什么 JMS 实现提供者，这些基础特性都是可用的。</li><li>连接灵活性：ActiveMQ 提供了广泛的连接协议，支持的协议有：HTTP/S，IP 多播，SSL，TCP，UDP 等等。对众多协议的支持让 ActiveMQ 拥有了很好的灵活性。</li><li>支持的协议种类多：OpenWire、STOMP、REST、XMPP、AMQP。</li><li>持久化插件和安全插件：ActiveMQ 提供了多种持久化选择。而且， ActiveMQ 的安全性也可以完全依据用户需求进行自定义鉴权和授权。</li><li>支持的客户端语言种类多：除了 Java 之外，还有 C/C++，.Net，Perl， PHP，Python，Ruby。</li><li>代理集群：多个 ActiveMQ 代理可以组成一个集群来提供服务。</li><li>异常简单的管理：ActiveMQ 是以开发者思维被设计的。所以，它并不需要专门的管理员，因为它提供了简单又实用的管理特性。</li></ul><p>有很多种方法可以监控 ActiveMQ 不同层面的数据，包括使用在 JConsole 或者在 ActiveMQ 的 WebConsole 中使用 JMX。</p><p>通过处理 JMX 的告警消息，通过使用命令行脚本，甚至可以通过监控各种类型的日志。</p><p>部署环境：ActiveMQ 可以运行在 Java 语言所支持的平台之上。</p><p>使用 ActiveMQ 需要：</p><ul><li><strong>JavaJDK</strong></li><li><strong>ActiveMQ 安装包</strong></li></ul><p>优点如下：</p><ul><li>跨平台(Java 编写与平台无关，ActiveMQ 几乎可以运行在任何的 JVM上)。</li><li>可以用 JDBC：可以将数据持久化到数据库。虽然使用 JDBC 会降低 ActiveMQ 的性能，但是数据库一直都是开发人员最熟悉的存储介质。</li><li>支持 JMS 规范：支持 JMS 规范提供的统一接口。</li><li>支持自动重连和错误重试机制。</li><li>有安全机制：支持基于 Shiro，JAAS 等多种安全配置机制，可以对 Queue/Topic 进行认证和授权。</li><li>监控完善：拥有完善的监控，包括 WebConsole，JMX，Shell 命令行， Jolokia 的 RESTfulAPI。</li><li>界面友善：提供的 WebConsole 可以满足大部分情况，还有很多第三方的组件可以使用，比如 Hawtio。</li></ul><p>缺点如下：</p><ul><li>社区活跃度不及 RabbitMQ 高。</li><li>根据其他用户反馈，会出莫名其妙的问题，会丢失消息。</li><li>目前重心放到 ActiveMQ 6.0 产品 Apollo，对 5.x 的维护较少。</li><li>不适合用于上千个队列的应用场景。</li></ul><p><strong>RabbitMQ</strong></p><p>RabbitMQ 于 2007 年发布，是一个在 AMQP(高级消息队列协议)基础上完成的，可复用的企业消息系统，是当前最主流的消息中间件之一。</p><p><a href="https://s5.51cto.com/oss/201909/11/80e37b0974f6680b83cff32b939922ec.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201909/11/80e37b0974f6680b83cff32b939922ec.jpg" alt=""></a></p><p>主要特性如下：</p><ul><li>可靠性：提供了多种技术可以让你在性能和可靠性之间进行权衡。这些技术包括持久性机制、投递确认、发布者证实和高可用性机制。</li><li>灵活的路由：消息在到达队列前是通过交换机进行路由的。RabbitMQ 为典型的路由逻辑提供了多种内置交换机类型。</li><li>如果你有更复杂的路由需求，可以将这些交换机组合起来使用，你甚至可以实现自己的交换机类型，并且当做 RabbitMQ 的插件来使用。</li><li>消息集群：在相同局域网中的多个 RabbitMQ 服务器可以聚合在一起，作为一个独立的逻辑代理来使用。</li><li>队列高可用：队列可以在集群中的机器上进行镜像，以确保在硬件问题下还保证消息安全。</li><li>支持多种协议：支持多种消息队列协议。</li><li>支持多种语言：用 Erlang 语言编写，支持只要是你能想到的所有编程语言。</li><li>管理界面：RabbitMQ 有一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。</li><li>跟踪机制：如果消息异常， RabbitMQ 提供消息跟踪机制，使用者可以找出发生了什么。</li><li>插件机制：提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。</li><li>部署环境：RabbitMQ 可以运行在 Erlang 语言所支持的平台之上，包括 Solaris，BSD，Linux，MacOSX，TRU64，Windows 等。</li></ul><p>使用 RabbitMQ 需要：</p><ul><li><strong>ErLang 语言包</strong></li><li><strong>RabbitMQ 安装包</strong></li></ul><p>优点如下：</p><ul><li>由于 Erlang 语言的特性，消息队列性能较好，支持高并发。</li><li>健壮、稳定、易用、跨平台、支持多种语言、文档齐全。</li><li>有消息确认机制和持久化机制，可靠性高。</li><li>高度可定制的路由。</li><li>管理界面较丰富，在互联网公司也有较大规模的应用，社区活跃度高。</li></ul><p>缺点如下：</p><ul><li>尽管结合 Erlang 语言本身的并发优势，性能较好，但是不利于做二次开发和维护。</li><li>实现了代理架构，意味着消息在发送到客户端之前可以在中央节点上排队。此特性使得 RabbitMQ 易于使用和部署，但是使得其运行速度较慢，因为中央节点增加了延迟，消息封装后也比较大。</li><li>需要学习比较复杂的接口和协议，学习和维护成本较高。</li></ul><p><strong>RocketMQ</strong></p><p>RocketMQ 出自阿里的开源产品，用 Java 语言实现，在设计时参考了 Kafka，并做出了自己的一些改进，消息可靠性上比 Kafka 更好。</p><p>RocketMQ 在阿里内部被广泛应用在订单，交易，充值，流计算，消息推送，日志流式处理，Binglog 分发等场景。</p><p><a href="https://s5.51cto.com/oss/201909/11/9097b16cd50222e3229296019ae30364.jpg-wh_600x-s_845021495.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201909/11/9097b16cd50222e3229296019ae30364.jpg-wh_600x-s_845021495.jpg" alt=""></a></p><p>主要特性如下：</p><ul><li>基于队列模型：具有高性能、高可靠、高实时、分布式等特点。</li><li>Producer、Consumer 队列都支持分布式。</li><li>Producer 向一些队列轮流发送消息，队列集合称为 Topic。Consumer 如果做广播消费，则一个 Consumer 实例消费这个 Topic 对应的所有队列。</li><li>如果做集群消费，则多个 Consumer 实例平均消费这个 Topic 对应的队列集合。</li><li>能够保证严格的消息顺序。</li><li>提供丰富的消息拉取模式。</li><li>高效的订阅者水平扩展能力。</li><li>实时的消息订阅机制。</li><li>亿级消息堆积能力。</li><li>较少的外部依赖。</li></ul><p>部署环境：RocketMQ 可以运行在 Java 语言所支持的平台之上。</p><p>使用 RocketMQ 需要：</p><ul><li>JavaJDK</li><li>安装 Git、Maven</li><li>RocketMQ 安装包</li></ul><p>优点如下：</p><ul><li>单机支持 1 万以上持久化队列。</li><li>RocketMQ 的所有消息都是持久化的，先写入系统 PAGECACHE，然后刷盘，可以保证内存与磁盘都有一份数据，而访问时，直接从内存读取。</li><li>模型简单，接口易用( JMS 的接口很多场合并不太实用)。</li><li>性能非常好，可以允许大量堆积消息在 Broker 中。</li><li>支持多种消费模式，包括集群消费、广播消费等。</li><li>各个环节分布式扩展设计，支持主从和高可用。</li><li>开发度较活跃，版本更新很快。</li></ul><p>缺点如下：</p><ul><li>支持的客户端语言不多，目前是 Java 及 C++，其中 C++ 还不成熟。</li><li>RocketMQ 社区关注度及成熟度也不及前两者。</li><li>没有 Web 管理界面，提供了一个 CLI(命令行界面)管理工具带来查询、管理和诊断各种问题。</li><li>没有在 MQ 核心里实现 JMS 等接口。</li></ul><p><strong>Kafka</strong></p><p>Apache Kafka 是一个分布式消息发布订阅系统。它最初由 LinkedIn 公司基于独特的设计实现为一个分布式的日志提交系统(a distributed commit log)，之后成为 Apache 项目的一部分。</p><p>Kafka 性能高效、可扩展良好并且可持久化。它的分区特性，可复制和可容错都是不错的特性。</p><p><a href="https://s2.51cto.com/oss/201909/11/d7a068ae79bd260d9d8278aabc63b83c.jpg-wh_600x-s_2111695512.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201909/11/d7a068ae79bd260d9d8278aabc63b83c.jpg-wh_600x-s_2111695512.jpg" alt=""></a></p><p>主要特性如下：</p><ul><li>快速持久化：可以在 O(1) 的系统开销下进行消息持久化。</li><li>高吞吐：在一台普通的服务器上即可以达到 10W/S 的吞吐速率。</li><li>完全的分布式系统：Broker、Producer 和 Consumer 都原生自动支持分布式，自动实现负载均衡。</li><li>支持同步和异步复制两种高可用机制。</li><li>支持数据批量发送和拉取。</li><li>零拷贝技术(zero-copy)：减少 IO 操作步骤，提高系统吞吐量。</li><li>数据迁移、扩容对用户透明。</li><li>无需停机即可扩展机器。</li><li>其他特性：丰富的消息拉取模型、高效订阅者水平扩展、实时的消息订阅、亿级的消息堆积能力、定期删除机制。</li></ul><p>部署环境，使用 Kafka 需要：</p><ul><li><strong>JavaJDK</strong></li><li><strong>Kafka 安装包</strong></li></ul><p>优点如下：</p><ul><li>客户端语言丰富：支持 Java、.Net、PHP、Ruby、Python、Go 等多种语言。</li><li>高性能：单机写入 TPS 约在 100 万条/秒，消息大小 10 个字节。</li><li>提供完全分布式架构，并有 Replica 机制，拥有较高的可用性和可靠性，理论上支持消息无限堆积。</li><li>支持批量操作。</li><li>消费者采用 Pull 方式获取消息。消息有序，通过控制能够保证所有消息被消费且仅被消费一次。</li><li>有优秀的第三方 Kafka Web 管理界面 Kafka-Manager。</li><li>在日志领域比较成熟，被多家公司和多个开源项目使用。</li></ul><p>缺点如下：</p><ul><li>Kafka 单机超过 64 个队列/分区时， Load 时会发生明显的飙高现象。队列越多，负载越高，发送消息响应时间变长。</li><li>使用短轮询方式，实时性取决于轮询间隔时间。</li><li>消费失败不支持重试。</li><li>支持消息顺序，但是一台代理宕机后，就会产生消息乱序。</li><li>社区更新较慢。</li></ul><p><strong>几种消息队列对比</strong></p><p>这里列举了上述四种消息队列的差异对比：</p><p><a href="https://s5.51cto.com/oss/201909/11/d13d02d18176c68b0914abdac6ee22b3.jpg-wh_600x-s_2971247064.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201909/11/d13d02d18176c68b0914abdac6ee22b3.jpg-wh_600x-s_2971247064.jpg" alt=""></a></p><p>Kafka 在于分布式架构，RabbitMQ 基于 AMQP 协议来实现，RocketMQ 的思路来源于 Kafka，改成了主从结构，在事务性和可靠性方面做了优化。</p><p>广泛来说，电商、金融等对事务一致性要求很高的，可以考虑 RabbitMQ 和 RocketMQ，对性能要求高的可考虑 Kafka。</p><p><strong>小结</strong></p><p>本文介绍了消息队列的特点，消息队列的传递服务模型，消息的传输方式，消息的推拉模式。</p><p>然后介绍了 ActiveMQ，RabbitMQ，RocketMQ 和 Kafka 几种常见的消息队列，阐述了各种消息队列的主要特点和优缺点。</p><p>通过本文，对于消息队列及相关技术选型，相信你会有更深入的理解和认识。更多细节和原理性的东西，还需在实践中见真知!</p><p>原文地址：<a href="http://developer.51cto.com/art/201909/602791.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201909/602791.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我国信用国家标准现状</title>
      <link href="/2019/09/05/%E6%88%91%E5%9B%BD%E4%BF%A1%E7%94%A8%E5%9B%BD%E5%AE%B6%E6%A0%87%E5%87%86%E7%8E%B0%E7%8A%B6/"/>
      <url>/2019/09/05/%E6%88%91%E5%9B%BD%E4%BF%A1%E7%94%A8%E5%9B%BD%E5%AE%B6%E6%A0%87%E5%87%86%E7%8E%B0%E7%8A%B6/</url>
      
        <content type="html"><![CDATA[<p>我国信用国家标准现状</p><p>　　信用国家标准（GB/T 31950-2015）《企业诚信管理体系》日前正式发布。通过本文的梳理总结，让大家对我国信用国家标准现状有个基本了解，为今后的工作提供帮助和借鉴。<br><a id="more"></a><br>　　<strong>一、信用标准对社会信用体系建设的重要作用</strong></p><p>　　现代市场经济是信用经济。我国正处于深化经济体制改革和完善社会主义市场经济体制的攻坚期，党中央、国务院高度重视社会信用体系建设。如十八大提出“加强政务诚信、商务诚信、社会诚信和司法公信建设”；十八届三中全会提出“建立健全社会征信体系，褒扬诚信，惩戒失信”；《国民经济和社会第十二个五年规划纲要》提出“加快社会信用体系建设”的总体要求；2007年国务院办公厅印发《关于社会信用体系建设的若干意见》；2014年6月14日国务院印发《社会信用体系建设规划纲要（2014-2020年）》等等。</p><p>　　经过多年的努力，我国的社会信用体建设处于一个全面繁荣发展的关键第一步，一系列具有深远意义的政策、法规、标准，正在、也将根本性改变我们社会信用体系的发展。目前已批准发布的国家标准有23项，正在编制的标准有30项，涵盖了基础通用标准，质量信用标准，企业信用标准，电子商务信用标准等，初步搭建了社会信用标准体系，为建设我国社会信用体系做出重要的基础性理论研究准备工作，提供了重要的技术支撑。</p><p>　　  <strong>二、信用国家标准跟踪梳理</strong></p><p>　　一项国家标准的诞生需要经过立项、起草、征求意见、报批、发布等阶段。信息的跟踪周期较长，建议跟踪周期以月为单位，由专人负责跟踪梳理。已正式立项的信用国家标准信息，可以在中国国家标准化管理委员会下达的本年度国家标准制修订计划的通知获取，获取渠道是中国国家标准化管理委员会网站、中国标准化研究院网站。编制过程中的信用国家标准动态，可通过中国国家标准化管理委员会、中国标准化研究院网站或相关新闻媒体报道了解。成稿的信用国家标准，可通过中国国家标准化管理委员会、国家发改委标准网、国家标准查询网等官方渠道进行检索获取。</p><p>　　国家标准分两种：强制性国家标准（GB）和推荐性国家标准（GB/T）。强制性国标是保障人体健康、人身、财产安全的标准和法律及行政法规规定强制执行的国家标准。推荐性国标是指生产、交换、使用等方面，通过经济手段调节而自愿采用的一类标准，又称自愿标准。这类标准任何单位都有权决定是否采用，违反这类标准，不承担经济或法律方面的责任。</p><p>　　<strong>三、已批准发布和正在编制的信用国家标准</strong></p><p>　　目前已批准发布的国家标准有23项，正在编制的标准有30项，标准发布单位均为中华人民共和国国家质量监督检验检疫总局和中国国家标准化管理委员会。具体详见下表一、表二。</p><p>　　表一 已批准发布的国家标准（23项）</p><p>　　序号 标准名称 发布时间 标准编号</p><p>　　1 《企业信用等级表示方法》 2008年 GB/T 22116-2008</p><p>　　2 《信用基本术语》 GB/T 22117-2008</p><p>　　3 《企业信用信息采集、处理和提供规范》 GB/T 22118-2008</p><p>　　4 《信用中介组织评价服务规范信用评级机构》 GB/T 22119-2008</p><p>　　5 《企业信用数据项规范》 GB/T 22120-2008</p><p>　　6 《企业质量信用等级划分通则》 2009年 GB/T 23791-2009</p><p>　　7 《信用标准化工作指南》 GB/T 23792-2009</p><p>　　8 《合格供应商信用评价规范》 GB/T 23793-2009</p><p>　　9 《企业信用评价指标体系分类及代码》 GB/T 23794-2009</p><p>　　10 《基于电子商务活动的交易主体 个人信用档案规范》 2011年 GB/T 28042-2011</p><p>　　11 《基于电子商务活动的交易主体 个人信用评价指标体系及表示规范》 GB/T28041-2011</p><p>　　12 《基于电子商务活动的交易主体 企业信用档案规范》 GB/T26841-2011</p><p>　　13 《基于电子商务活动的交易主体 企业信用评价指标与等级表示规范》 GB/T26842-2011</p><p>　　14 《信用主体识别规范》 GB/T26819-2011</p><p>　　15 《个人信用调查报告格式规范基本信息报告》 GB/T26818-2011</p><p>　　16 《企业信用调查报告格式规范基本信息报告、普通调查报告、深度调查报告》 GB/T26817-2011</p><p>　　17 《企业质量诚信管理实施规范》 2012年 GB/T29467-2012</p><p>　　18 《国际物流企业信用评价指标要素》 GB/T28836-2012</p><p>　　19 《认证机构信用评价准则》 2013年 GB/T 27201-2013</p><p>　　20 《电子商务信用卖方交易信用信息披露规范》 GB/T 29622-2013</p><p>　　21 《认证执业人员信用评价准则》 GB/T 27202-2013</p><p>　　22 《国际物流企业信用管理规范》 GB/T 30345-2013</p><p>　　23 《企业诚信管理体系》 2015年 GB/T 31950-2015</p><p>　　<strong>表二 目前正在编制的信用国家标准（30项）</strong></p><p>　　序号 标准名称 状态</p><p>　　1 企业质量信用评价指标分类 征求意见</p><p>　　2 企业质量信用报告编写指南 征求意见</p><p>　　3 信用基础数据元目录 征求意见</p><p>　　4 信用信息分类与代码 征求意见</p><p>　　5 信用信息分级规范 征求意见</p><p>　　6 企业信用信息公示格式规范 征求意见</p><p>　　7 信用信息共享数据目录 征求意见</p><p>　　8 企业信用档案信息规范 征求意见</p><p>　　9 企业信用评估报告编制指南 征求意见</p><p>　　10 企业信用评价指标体系 征求意见</p><p>　　11 社会组织信用评价指标体系 征求意见</p><p>　　12 职业经理人信用评价指标体系 征求意见</p><p>　　13 企业合同信用指标指南 征求意见</p><p>　　14 电子商务信用 网络交易主体分类规范 征求意见</p><p>　　15 电子商务信用 B2B网络交易卖方信用评价指标 征求意见</p><p>　　16 电子商务信用 可信自营型购物网站要求 征求意见</p><p>　　17 电子商务信用 B2B第三方交易平台信用规范 征求意见</p><p>　　18 法人和其他组织统一社会信用代码编码规则 征求意见</p><p>　　19 法人和其他组织统一社会信用代码基础数据元 征求意见</p><p>　　20 法人和其他组织统一社会信用代码数据交换接口 征求意见</p><p>　　21 法人和其他组织统一社会信用代码数据管理规范 征求意见</p><p>　　22 法人和其他组织统一社会信用代码赋码操作规范 征求意见</p><p>　　23 诚信管理体系建立及实施 实验室要求 起草</p><p>　　24 电子商务信用 网络零售信用评价指标体系 起草</p><p>　　25 商贸物流企业信用评价指标体系 起草</p><p>　　26 电子商务信用 第三方网络零售平台信用管理体系要求 起草</p><p>　　27 电子商务信用 自营型网络零售平台信用管理体系要求 起草</p><p>　　28 信用标准化总体架构 起草</p><p>　　29 信用信息征集规范 起草</p><p>　　30 第三方信用服务机构业务规范 起草</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么会产生微服务架构，原来是这些原因</title>
      <link href="/2019/09/05/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E4%BA%A7%E7%94%9F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%BA%9B%E5%8E%9F%E5%9B%A0/"/>
      <url>/2019/09/05/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E4%BA%A7%E7%94%9F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%BA%9B%E5%8E%9F%E5%9B%A0/</url>
      
        <content type="html"><![CDATA[<p>Web应用架构受系统用户量、开发人员组织方式影响严重。过去二十年互联网迅速发展，Web架构也从单体式演进出微服务，背后还有比如 Martin Fowler 提出的理论支撑。虽然每个人都听说过微服务，但是很多人并不太清楚为什么要这么做，应该怎么做，怎么拆。要回答这个问题我认为需要从Web架构的演化历史的高度去理解这些架构设计中的取舍。<br><a id="more"></a><br><a href="https://s1.51cto.com/oss/201908/30/21230bdbd5609561a1945c8d38de9407.jpg-wh_651x-s_2058415208.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201908/30/21230bdbd5609561a1945c8d38de9407.jpg-wh_651x-s_2058415208.jpg" alt=""></a></p><p>首先我们改进系统架构的目的是为了满足系统可靠性、并发量以及快速开发的需求。所有的改进方案都是为了解决这其中一个或多个问题而产生的。</p><p><strong>单体结构</strong></p><p><img src="https://s4.51cto.com/oss/201908/30/2866c3b93d40e872c7d0b87eae86818c.jpeg" alt="为什么会产生微服务架构，原来是这些原因">  </p><p>单体结构</p><p>最开始Web服务器、数据库全部部署在同一台服务器上，这也是最简单的应用架构，通常公司早期项目都采用这种方式。在很长一段时间里单体结构可以满足系统快速开发与并发量的需求。当用户量越来越大，通常会数据库性能会成为系统瓶颈，此时可以将Web业务与数据库部署在不同服务器上，增强数据库服务器的配置并做读写分离等提高系统的吞吐量与可用性。</p><p>与此同时也可以将业务系统等价部署在多台服务器上来提高系统吞吐量，但整体上这仍然是一个单体应用。</p><p><img src="https://s1.51cto.com/oss/201908/30/6991bb2cfcd4a4f01f608c93946f394d.jpeg" alt="为什么会产生微服务架构，原来是这些原因">  </p><p>单体等价部署</p><p>随着用户、数据量进一步增大，单体应用的缺点会进一步显露出来，比如：</p><ul><li>耦合严重、复杂度高、可靠性差 ：单体应用越来越来很多业务会耦合在一起，一但某些模块出现Bug会影响整个系统正常运行，业务代码的耦合也会形成开发人员的依赖造成新业务难以推进</li><li>增加技术债、部署困难效率差 ：技术债越来越多容易会造成“不坏不修“的囧境，已完成的代码难以被修改以防止系统某个地方意料之外的调用。同于由于代码量大导致应用全量部署困难</li><li>系统吞吐量受限、阻碍技术进步 ：单体应用难以进一步扩展使系统吞吐量受限，同时单体应用要求使用统一技术平台或解决方案，要想引入新语言或框架会非常困难</li></ul><p><strong>拆分</strong></p><p>应用规模越来越大，首先遇到瓶颈的可能就是数据库系统，面对数据库压力通常我们可以对数据库做拆分把负载分担到不同的服务器上来解决，通常数据库拆分有两种方案：</p><ul><li>垂直拆分：对不同的业务系统如账户、搜索、推荐系统使用不同的数据库</li><li>水平拆分：对于大表，比如十亿百亿级别的，进行多表拆分</li></ul><p>数据库水平拆分与业务逻辑耦合紧密，需要具体问题具体分析，通常这是一个非常复杂的问题。后来人们引入 NoSQL、NewSQL 用分布式概念在数据库层屏蔽掉数据库的水平拆分，比如 NoSQL 的 MongoDB Sharding，NewSQL 的 TiDB。</p><p>同样的在业务层上我们也可以通过垂直拆分和水平拆分将单体业务拆成不同的服务，服务之间通过约定好的协议通信，以提高人员开发效率，实现多机部署冗余部署来提高系统可用性与吞吐量。</p><p><strong>微服务</strong></p><p>我们都知道微服务是一种提倡将单一服务拆分成一组小服务、服务之间相互协调、配合，提高开发效率，最终为用户提供价值的思路。说到微服务那么这里面最重要的一个问题就是服务应该怎么拆。微服务作为 SOA(Service Oriented Architecture)思想的一种具体实践我们首先想到的就是按照不同的业务系统做垂直拆分，如下图所示：</p><p><img src="https://s3.51cto.com/oss/201908/30/27eb363289718a19d6bff867ada5165d.jpeg" alt="为什么会产生微服务架构，原来是这些原因">  </p><p>SOA垂直拆分</p><p>按业务系统对单体应用做垂直拆分，不同的业务线完全可以独立配备产品经历与工程师同步开发维护，将不同业务线解耦出来有不同团队维护。但上图是一种理想情况，各系统拆分力度比较大，系统之间不需要更详细的通信。如果是被拆除出了的子系统之间有大量的数据交互与调用，网关模式便不是一种很好的实践，通常会将各业务子系统接入一个数据总线用 ESB(Enterprise Service Bus)模式来进行数据交互，各子系统与数据总线进行数据交换便需要对子系统做统一管理，这遍有了 服务治理 的概念，用一套统一的保准来处理各子系统的注册、权限、监控等，目前有很多 ESB 开源或闭源的解决方案，这里不再赘述。</p><p>垂直拆分将各业务子系统解耦出来，但是每次请求在不同阶段遇到的瓶颈与负载是不一样的，因此我们对可以使用水平拆分的思路对服务进行拆分：</p><p><img src="https://s3.51cto.com/oss/201908/30/da8b106ac753094d0f2e5f2e22ae64d0.jpeg" alt="为什么会产生微服务架构，原来是这些原因">  </p><p>水平拆分</p><p>首先用户请求通过http协议到达网关，网关将json数据格式转为protobuf，通过tcp长链接与服务层、数据层通信获取目标数据然后返回给用户。这样拆分加长了用户请求链路时延，但是如果服务全部部署在同一内网，而且使用protobuf格式通信那么这个时延在几十毫秒内是完全可以接受的。业务层与数据层完全解耦便可以轻松将不同类型的服务进入冗余部署，同时在不动业务层的同时修改它的数据存储方式。</p><p>如果我们对系统即做垂直拆分也做水分拆分，那么就有了微服务的样子,</p><p><img src="https://s1.51cto.com/oss/201908/30/9ae13308538da58e862b784ca047f39b.jpeg" alt="为什么会产生微服务架构，原来是这些原因">  </p><p>水平拆分</p><p>每级服务只能调用比他低级别的服务，如果搜索服务层只能掉账户接口层服务而不能调账户服务层接口，这样可以用来避免服务A调用服务B，而服务B同时又调用了服务A的循环调用问题。但是这样的拆分粒度仍然不够的，比如搜索系统和推荐系统都要调用账户系统的一些基础查询、修改逻辑，那么需要在搜索与推荐的服务层两次实现同样的代码吗，这样显然是不合理了，任何不能复用的设计显然都是有问题的。如果通过编写SDK库提供Jar包的模式去实现这个功能呢?，显然也存在问题比如推荐系统是Python实现，而搜索系统是Java实现的呢?所以这里我们将每个子系统可共用代码部分也单独抽取出来作为一个服务。</p><p><img src="https://s4.51cto.com/oss/201908/30/bfb91f55f8e0dbc2f3d5d491bfe904fa.jpeg" alt="为什么会产生微服务架构，原来是这些原因">  </p><p>水平拆分2</p><p>这样拆分后的系统可以灵活部署，独立开发，并且各模块服务使用的技术栈相对独立不受限制。但是同时拆分也将系统的网络拓扑便的复杂，运维负担加重，服务间的依赖使得服务接口的调整成本非常高。服务增多的同时对服务治理的要求也更高，需要专门做服务的发现、注册、鉴权、监控等系统功能。</p><p>原文地址：<a href="http://developer.51cto.com/art/201908/602206.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201908/602206.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一个牛逼的创业公司后台技术栈搭建方案</title>
      <link href="/2019/05/22/%E4%B8%80%E4%B8%AA%E7%89%9B%E9%80%BC%E7%9A%84%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF%E6%A0%88%E6%90%AD%E5%BB%BA%E6%96%B9%E6%A1%88/"/>
      <url>/2019/05/22/%E4%B8%80%E4%B8%AA%E7%89%9B%E9%80%BC%E7%9A%84%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8%E5%90%8E%E5%8F%B0%E6%8A%80%E6%9C%AF%E6%A0%88%E6%90%AD%E5%BB%BA%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<p>在大多数创业公司，因为没有大公司那些完善的基础设施，需要从开源界的一个个系统和组件做选型，最终形成整个的后台技术栈。<br><a id="more"></a><br><a href="https://s4.51cto.com/oss/201905/15/86da40e42c17fda6e81061a746455896.jpg-wh_651x-s_3593099173.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201905/15/86da40e42c17fda6e81061a746455896.jpg-wh_651x-s_3593099173.jpg" alt=""></a></p><p>说到后台技术栈，脑海中是不是浮现的下面这样一幅图?</p><p><a href="https://s1.51cto.com/oss/201905/15/024d1bab1bfaa10c68bbb9ce7a3d1caf.jpg-wh_600x-s_1515669291.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201905/15/024d1bab1bfaa10c68bbb9ce7a3d1caf.jpg-wh_600x-s_1515669291.jpg" alt=""></a></p><p>图 1</p><p>有点眼晕，以下只是我们会用到的一些语言的合集，而且只是语言层面的一部分，就整个后台技术栈来说，这只是一个开始，从语言开始，还有很多很多的内容。</p><p>今天要说的后台是大后台的概念，放在服务器上的东西都属于后台的东西，比如使用的框架，语言，数据库，服务，操作系统等等。</p><p>整个后台技术栈，我的理解包括四个层面的内容：</p><ul><li>语言：用了哪些开发语言，如：C++/Java/Go/PHP/Python/Ruby 等等。</li><li>组件：用了哪些组件，如：MQ 组件，数据库组件等等。</li><li>流程：怎样的流程和规范，如：开发流程，项目流程，发布流程，监控告警流程，代码规范等等。</li><li>系统：系统化建设，上面的流程需要有系统来保证，如：规范发布流程的发布系统，代码管理系统等等。</li></ul><p>结合以上的的 4 个层面的内容，整个后台技术栈的结构如图 2 所示：</p><p><a href="https://s1.51cto.com/oss/201905/15/6f11de9bf8ddb2037d7c8e8e66e954fe.jpg-wh_600x-s_1257533657.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201905/15/6f11de9bf8ddb2037d7c8e8e66e954fe.jpg-wh_600x-s_1257533657.jpg" alt=""></a></p><p>图 2：后台技术栈结构</p><p>以上的这些内容都需要我们从零开始搭建，在创业公司，没有大公司那些完善的基础设施，需要我们从开源界，从云服务商甚至有些需要自己去组合，去拼装，去开发一个适合自己的组件或系统以达成我们的目标。</p><p>咱们一个个系统和组件的做选型，最终形成我们的后台技术栈。</p><p><strong>各系统组件选型</strong></p><p><strong>项目管理/Bug 管理/问题管理</strong></p><p>项目管理软件是整个业务的需求，问题，流程等等的集中地，大家的跨部门沟通协同大多依赖于项目管理工具。</p><p>有一些 SaaS 的项目管理服务可以使用，但是很多时间不满足需求，此时我们可以选择一些开源的项目，这些项目本身有一定的定制能力，有丰富的插件可以使用。</p><p>一般的创业公司需求基本上都能得到满足，常用的项目如下：</p><ul><li>Redmine：用 Ruby 开发的，有较多的插件可以使用，能自定义字段，集成了项目管理，Bug 问题跟踪，WiKi 等功能，不过好多插件 N 年没有更新了。</li><li>Phabricator：用 PHP 开发的，Facebook 之前的内部工具，开发这工具的哥们离职后自己搞了一个公司专门做这个软件，集成了代码托管， Code Review，任务管理，文档管理，问题跟踪等功能，强烈推荐较敏捷的团队使用。</li><li>Jira：用 Java 开发的，有用户故事，Task 拆分，燃尽图等等，可以做项目管理，也可以应用于跨部门沟通场景，较强大。</li><li>悟空 CRM ：这个不是项目管理，这个是客户管理，之所以在这里提出来，是因为在 To B 的创业公司里面，往往是以客户为核心来做事情的，可以将项目管理和问题跟进的在悟空 CRM 上面来做。</li></ul><p>它的开源版本已经基本实现了 CRM 的核心功能，还带有一个任务管理功能，用于问题跟进，不过用这个的话，还是需要另一个项目管理的软件协助，顺便说一嘴，这个系统的代码写得很难维护，只能适用于客户规模小(1 万以内)时。</p><p><strong>DNS</strong></p><p>DNS 是一个很通用的服务，创业公司基本上选择一个合适的云厂商就行了，国内主要是两家：</p><ul><li>阿里万网：阿里 2014 年收购了万网，整合了其域名服务，最终形成了现在的阿里万网，其中就包含 DNS 这块的服务。</li><li>腾讯 DNSPod：腾讯 2012 年以 4000 万收购 DNSPod 100% 股份，主要提供域名解析和一些防护功能。</li></ul><p>如果你的业务是在国内，主要就是这两家，选 一个就好，像今日头条这样的企业用的也是 DNSPod 的服务，除非一些特殊的原因才需要自建，比如一些 CDN 厂商，或者对区域有特殊限制的。</p><p>要实惠一点用阿里最便宜的基础版就好了，要成功率高一些，还是用 DNSPod 的贵的那种。</p><p>在国外还是选择亚马逊吧，阿里的 DNS 服务只有在日本和美国有节点，东南亚最近才开始部点，DNSPod 也只有美国和日本，像一些出海的企业，其选择的云服务基本都是亚马逊。</p><p>如果是线上产品，DNS 强烈建议用付费版，阿里的那几十块钱的付费版基本可以满足需求。</p><p>如果还需要一些按省份或按区域调试的逻辑，则需要加钱，一年也就几百块，省钱省力。</p><p>如果是国外，优先选择亚马逊，如果需要国内外互通并且有自己的 App 的话，建议还是自己实现一些容灾逻辑或者智能调度。</p><p>因为没有一个现成的 DNS 服务能同时较好的满足国内外场景，或者用多个域名，不同的域名走不同的 DNS 。</p><p><strong>LB(负载均衡)</strong></p><p>LB(负载均衡)是一个通用服务，一般云厂商的 LB 服务基本都有如下功能：</p><ul><li><strong>支持四层协议请求(包括 TCP、UDP 协议)</strong></li><li><strong>支持七层协议请求(包括 HTTP、HTTPS 协议)</strong></li><li><strong>集中化的证书管理系统支持 HTTPS 协议</strong></li><li><strong>健康检查</strong></li></ul><p>如果你线上的服务机器都是用的云服务，并且是在同一个云服务商的话，可以直接使用云服务商提供的 LB 服务，如阿里云的 SLB，腾讯云的 CLB，亚马逊的 ELB 等等。如果是自建机房基本都是 LVS + Nginx。</p><p><strong>CDN</strong></p><p>CDN 现在已经是一个很红很红的市场，基本上只能挣一些辛苦钱，都是贴着成本在卖。</p><p>国内以网宿为龙头，他们家占据整个国内市场份额的 40% 以上，后面就是腾讯，阿里。网宿有很大一部分是因为直播的兴起而崛起。</p><p>国外，Amazon 和 Akamai 合起来占比大概在 50%，曾经的国际市场老大 Akamai 拥有全球超一半的份额，在 Amazon CDN入局后，份额跌去了将近 20%，众多中小企业都转向后者，Akamai 也是无能为力。</p><p>国内出海的 CDN 厂商，更多的是为国内的出海企业服务，三家大一点的 CDN 服务商里面也就网宿的节点多一些，但是也多不了多少。阿里和腾讯还处于前期阶段，仅少部分国家有节点。</p><p>就创业公司来说，CDN 用腾讯云或阿里云即可，其相关系统较完善，能轻松接入，网宿在系统支持层面相对较弱一些，而且还贵一些。</p><p>并且，当流量上来后，CDN 不能只用一家，需要用多家，不同的 CDN 在全国的节点覆盖不一样。</p><p>而且针对不同的客户云厂商内部有些区分客户集群，并不是全节点覆盖(但有些云厂商说自己是全网节点)，除了节点覆盖的问题，多 CDN 也在一定程度上起到容灾的作用。</p><p><strong>RPC 框架</strong></p><p>维基百科对 RPC 的定义是：远程过程调用(Remote Procedure Call，RPC)是一个计算机通信协议。</p><p>该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。</p><p>通俗来讲，一个完整的 RPC 调用过程，就是 Server 端实现了一个函数，客户端使用 RPC 框架提供的接口，调用这个函数的实现，并获取返回值的过程。</p><p>业界 RPC 框架大致分为两大流派，一种侧重跨语言调用，另一种是偏重服务治理。</p><p>跨语言调用型的 RPC 框架有 Thrift、gRPC、Hessian、Hprose 等。这类 RPC 框架侧重于服务的跨语言调用，能够支持大部分的语言进行语言无关的调用，非常适合多语言调用场景。</p><p>但这类框架没有服务发现相关机制，实际使用时需要代理层进行请求转发和负载均衡策略控制。</p><p>其中，gRPC 是 Google 开发的高性能、通用的开源 RPC 框架，其由 Google 主要面向移动应用开发并基于 HTTP/2 协议标准而设计，基于 ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。本身它不是分布式的，所以要实现框架的功能需要进一步的开发。</p><p>Hprose(High Performance Remote Object Service Engine)是一个 MIT 开源许可的新型轻量级跨语言跨平台的面向对象的高性能远程动态通讯中间件。</p><p>服务治理型的 RPC 框架的特点是功能丰富，提供高性能的远程调用、服务发现及服务治理能力，适用于大型服务的服务解耦及服务治理，对于特定语言(Java)的项目可以实现透明化接入。</p><p>缺点是语言耦合度较高，跨语言支持难度较大。国内常见的冶理型 RPC 框架如下：</p><ul><li>Dubbo：Dubbo 是阿里巴巴公司开源的一个 Java 高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和 Spring 框架无缝集成。</li></ul><p>当年在淘宝内部，Dubbo 由于跟淘宝另一个类似的框架 HSF 有竞争关系，导致 Dubbo 团队解散，最近又活过来了，有专职同学投入。</p><ul><li>DubboX：DubboX 是由当当在基于 Dubbo 框架扩展的一个 RPC 框架，支持 REST 风格的远程调用、Kryo/FST 序列化，增加了一些新的 feature。</li></ul><p>Motan：Motan 是新浪微博开源的一个 Java 框架。它诞生的比较晚，起于 2013 年，2016 年 5 月开源。Motan 在微博平台中已经广泛应用，每天为数百个服务完成近千亿次的调用。</p><ul><li>RPCX：RPCX 是一个类似阿里巴巴 Dubbo 和微博 Motan 的分布式的 RPC 服务框架，基于 Golang NET/RPC 实现。</li></ul><p>但是 RPCX 基本只有一个人在维护，没有完善的社区，使用前要慎重，之前做 Golang 的 RPC 选型时也有考虑这个，最终还是放弃了，选择了 gRPC，如果想自己自研一个 RPC 框架，可以参考学习一下。</p><p><strong>名字发现/服务发现</strong></p><p>名字发现和服务发现分为两种模式，一个是客户端发现模式，一种是服务端发现模式。框架中常用的服务发现是客户端发现模式。</p><p>所谓服务端发现模式是指客户端通过一个负载均衡器向服务发送请求，负载均衡器查询服务注册表并把请求路由到一台可用的服务实例上。现在常用的负载均衡器都是此类模式，常用于微服务中。</p><p>所有的名字发现和服务发现都要依赖于一个可用性非常高的服务注册表，业界常用的服务注册表有如下三个：</p><ul><li>Etcd：一个高可用、分布式、一致性、Key-Value 方式的存储，被用在分享配置和服务发现中。两个著名的项目使用了它：Kubernetes 和 Cloud Foundry。</li><li>Consul：一个发现和配置服务的工具，为客户端注册和发现服务提供了API，Consul 还可以通过执行健康检查决定服务的可用性。</li><li>Apache ZooKeeper：一个广泛使用、高性能的针对分布式应用的协调服务。Apache ZooKeeper 本来是 Hadoop 的子工程，现在已经是顶级工程了。</li></ul><p>除此之外也可以自己实现服务实现，或者用 Redis 也行，只是需要自己实现高可用性。</p><p><strong>关系数据库</strong></p><p>关系数据库分为两种，一种是传统关系数据库，如 Oracle，MySQL，Maria，DB2，PostgreSQL 等等。</p><p>另一种是 NewSQL，即至少要满足以下五点的新型关系数据库：</p><ul><li>完整地支持 SQL，支持 JOIN / GROUP BY /子查询等复杂 SQL 查询。</li><li>支持传统数据标配的 ACID 事务，支持强隔离级别。</li><li>具有弹性伸缩的能力，扩容缩容对于业务层完全透明。</li><li>真正的高可用，异地多活、故障恢复的过程不需要人为的接入，系统能够自动地容灾和进行强一致的数据恢复。</li><li>具备一定的大数据分析能力。</li></ul><p>传统关系数据库用得最多的是 MySQL，因为成熟，稳定，一些基本的需求都能满足，在一定数据量级之前基本单机传统数据库都可以搞定。</p><p>而且现在较多的开源系统都是基于 MySQL，开箱即用，再加上主从同步和前端缓存，百万 PV 的应用都可以搞定了。</p><p>不过 CentOS 7 已经放弃了 MySQL，而改使用 MariaDB。MariaDB 数据库管理系统是 MySQL 的一个分支，主要由开源社区在维护，采用 GPL 授权许可。</p><p>开发这个分支的原因之一是：甲骨文公司收购了 MySQL 后，有将 MySQL 闭源的潜在风险，因此社区采用分支的方式来避开这个风险。</p><p>在 Google 发布了 F1: A Distributed SQL Database That Scales 和 Spanner: Google’s Globally-Distributed Databasa 之后，业界开始流行起 NewSQL。</p><p>于是有了 CockroachDB，然后有了奇叔公司的 TiDB。国内已经有比较多的公司使用 TiDB，之前在创业公司时在大数据分析时已经开始应用 TiDB，当时应用的主要原因是 MySQL 要使用分库分表，逻辑开发比较复杂，扩展性不够。</p><p><strong>NoSQL</strong></p><p>NoSQL 顾名思义就是 Not-Only SQL，也有人说是 No–SQL，个人偏向于 Not-Only SQL，它并不是用来替代关系库，而是作为关系型数据库的补充而存在。</p><p>常见 NoSQL 有四个类型：</p><ul><li>键值，适用于内容缓存，适合混合工作负载并发高扩展要求大的数据集，其优点是简单，查询速度快，缺点是缺少结构化数据，常见的有 Redis，Memcache，BerkeleyDB 和 Voldemort 等等。</li><li>列式，以列簇式存储，将同一列数据存在一起，常见于分布式的文件系统，其中以 Hbase，Cassandra 为代表。</li></ul><p>Cassandra 多用于写多读少的场景，国内用得比较多的有 360，大概 1500 台机器的集群，国外大规模使用的公司比较多，如 eBay，Instagram，Apple 和沃尔玛等等。</p><ul><li>文档，数据存储方案非常适用承载大量不相关且结构差别很大的复杂信息。性能介于 KV 和关系数据库之间，它的灵感来自 Lotus Notes，常见的有 MongoDB，CouchDB 等等。</li><li>图形，图形数据库擅长处理任何涉及关系的状况，比如社交网络，推荐系统等。专注于构建关系图谱，需要对整个图做计算才能得出结果，不容易做分布式的集群方案，常见的有 Neo4J，InfoGrid 等。</li></ul><p>除了以上 4 种类型，还有一些特种的数据库，如对象数据库，XML 数据库，这些都有针对性对某些存储类型做了优化的数据库。</p><p>在实际应用场景中，何时使用关系数据库，何时使用 NoSQL，使用哪种类型的数据库，这是我们在做架构选型时一个非常重要的考量，甚至会影响整个架构的方案。</p><p><strong>消息中间件</strong></p><p>消息中间件在后台系统中是必不可少的一个组件，一般我们会在以下场景中使用消息中间件：</p><ul><li>异步处理：异步处理是使用消息中间件的一个主要原因，在工作中最常见的异步场景有用户注册成功后需要发送注册成功邮件、缓存过期时先返回老的数据，然后异步更新缓存、异步写日志等等。<ul><li>通过异步处理，可以减少主流程的等待响应时间，让非主流程或者非重要业务通过消息中间件做集中的异步处理。</li></ul></li><li>系统解耦：比如在电商系统中，当用户成功支付完成订单后，需要将支付结果通知 ERP 系统、发票系统、WMS、推荐系统、搜索系统、风控系统等进行业务处理。<ul><li>这些业务处理不需要实时处理、不需要强一致，只需要最终一致性即可，因此可以通过消息中间件进行系统解耦。通过这种系统解耦还可以应对未来不明确的系统需求。</li></ul></li><li>削峰填谷：当系统遇到大流量时，监控图上会看到一个一个的山峰样的流量图，通过使用消息中间件将大流量的请求放入队列，通过消费者程序将队列中的处理请求慢慢消化，达到削峰填谷的效果。</li></ul><p>最典型的场景是秒杀系统，在电商的秒杀系统中下单服务往往会是系统的瓶颈，因为下单需要对库存等做数据库操作，需要保证强一致性，此时使用消息中间件进行下单排队和流控，让下单服务慢慢把队列中的单处理完，保护下单服务，以达到削峰填谷的作用。</p><p>业界消息中间件是一个非常通用的东西，大家在做选型时有使用开源的，也有自己造轮子的，甚至有直接用 MySQL 或 Redis 做队列的，关键看是否满足你的需求。</p><p>如果是使用开源的项目，以下的表格在选型时可以参考：</p><p><a href="https://s1.51cto.com/oss/201905/15/c731198eb54bfda1a6c8b5cec6a6a431.jpg-wh_600x-s_2269407435.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201905/15/c731198eb54bfda1a6c8b5cec6a6a431.jpg-wh_600x-s_2269407435.jpg" alt=""></a></p><p>图 3</p><p>以上图的纬度为：名字、成熟度、所属社区/公司、文档、授权方式、开发语言、支持的协议、客户端支持的语言、性能、持久化、事务、集群、负载均衡、管理界面、部署方式、评价。</p><p><strong>代码管理</strong></p><p>代码是互联网创业公司的命脉之一，代码管理很重要，常见的考量点包括两块：</p><ul><li>安全和权限管理，将代码放到内网并且对于关系公司命脉的核心代码做严格的代码控制和机器的物理隔离。</li><li>代码管理工具，Git 作为代码管理的不二之选，你值得拥有。GitLab 是当今最火的开源 Git 托管服务端，没有之一，虽然有企业版，但是其社区版基本能满足我们大部分需求，结合 Gerrit 做 Code Review，基本就完美了。</li></ul><p>当然 GitLab 也有代码对比，但没 Gerrit 直观。Gerrit 比 GitLab 提供了更好的代码检查界面与主线管理体验，更适合在对代码质量有高要求的文化下使用。</p><p><strong>持续集成</strong></p><p>持续集成简称 CI(continuous integration)，是一种软件开发实践，即团队开发成员经常集成他们的工作，每天可能会发生多次集成。</p><p>每次集成都通过自动化的构建(包括编译，发布，自动化测试)来验证，从而尽早地发现集成错误。</p><p>持续集成为研发流程提供了代码分支管理/比对、编译、检查、发布物输出等基础工作，为测试的覆盖率版本编译、生成等提供统一支持。</p><p>业界免费的持续集成工具中，系统我们有如下一些选择：</p><ul><li>Jenkins：Java 写的有强大的插件机制，MIT 协议开源 (免费，定制化程度高，它可以在多台机器上进行分布式地构建和负载测试)。</li></ul><p>Jenkins 可以算是无所不能，基本没有 Jenkins 做不了的，无论从小型团队到大型团队 Jenkins 都可以搞定。不过如果要大规模使用，还是需要有人力来学习和维护。</p><ul><li>TeamCity：TeamCity 与 Jenkins 相比使用更加友好，也是一个高度可定制化的平台。但是用的人多了，TeamCity 就要收费了。</li><li>Strider：Strider 是一个开源的持续集成和部署平台，使用 Node.js 实现，存储使用的是 MongoDB，BSD 许可证，概念上类似 Travis 和 Jenkins。</li><li>GitLab CI：从 GitLab 8.0 开始，GitLab CI 就已经集成在 GitLab，我们只要在项目中添加一个 .gitlab-ci.yml 文件，然后添加一个 Runner，即可进行持续集成。</li></ul><p>并且 GitLab 与 Docker 有着非常好的相互协作的能力。免费版与付费版本的不同可以参见：<a href="https://about.gitlab.com/products/feature-comparison/。" target="_blank" rel="noopener">https://about.gitlab.com/products/feature-comparison/。</a></p><ul><li>Travis：Travis 和 GitHub 强关联;闭源代码使用 SaaS 还需考虑安全问题;不可定制;开源项目免费，其他收费。</li><li>Go：Go 是 ThoughtWorks 公司最新的 Cruise Control 的化身。除了 ThoughtWorks 提供的商业支持，Go 是免费的。它适用于 Windows，Mac 和各种 Linux 发行版。</li></ul><p><strong>日志系统</strong></p><p>日志系统一般包括打日志，采集，中转，收集，存储，分析，呈现，搜索还有分发等。</p><p>一些特殊的如染色，全链条跟踪或者监控都可能需要依赖于日志系统实现。</p><p>日志系统的建设不仅仅是工具的建设，还有规范和组件的建设，最好一些基本的日志在框架和组件层面加就行了，比如全链接跟踪之类的。</p><p>对于常规日志系统 ELK 能满足大部分的需求，ELK 包括如下组件：</p><ul><li>ElasticSearch 是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，RESTful 风格接口，多数据源，自动搜索负载等。</li><li>Logstash 是一个完全开源的工具，它可以对你的日志进行收集、分析，并将其存储供以后使用。</li><li>Kibana 是一个开源和免费的工具，它可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。</li><li>Filebeat 已经完全替代了 Logstash-Forwarder 成为新一代的日志采集器，同时鉴于它轻量、安全等特点，越来越多人开始使用它。</li></ul><p>因为免费的 ELK 没有任何安全机制，所以这里使用了 Nginx 作反向代理，避免用户直接访问 Kibana 服务器。</p><p>加上配置 Nginx 实现简单的用户认证，一定程度上提高安全性。另外，Nginx 本身具有负载均衡的作用，能够提高系统访问性能。</p><p>ELK 架构如图 4 所示：</p><p><a href="https://s5.51cto.com/oss/201905/15/c1bb3b9a42d3f8bd954be5c796287352.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201905/15/c1bb3b9a42d3f8bd954be5c796287352.jpg" alt=""></a></p><p>图 4：ELK 流程图</p><p>对于有实时计算的需求，可以使用 Flume + Kafka + Storm + MySQL 方案，一般架构如图 5 所示：</p><p><a href="https://s3.51cto.com/oss/201905/15/654ec8fd4a1b7d3ea5cc46708d49cfea.jpg-wh_600x-s_267537921.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201905/15/654ec8fd4a1b7d3ea5cc46708d49cfea.jpg-wh_600x-s_267537921.jpg" alt=""></a></p><p>图 5：实时分析系统架构图</p><p>其中：</p><ul><li>Flume 是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的日志收集系统，支持在日志系统中定制各类数据发送方，用于收集数据。</li></ul><p>同时，Flume 提供对数据进行简单处理，并写到各种数据接受方(可定制)的能力。</p><ul><li>Kafka 是由 Apache 软件基金会开发的一个开源流处理平台，由 Scala 和 Java 编写。</li></ul><p>其本质上是一个“按照分布式事务日志架构的大规模发布/订阅消息队列”，它以可水平扩展和高吞吐率而被广泛使用。</p><p>Kafka 追求的是高吞吐量、高负载，Flume 追求的是数据的多样性，二者结合起来简直完美。</p><p><strong>监控系统</strong></p><p>监控系统只包含与后台相关的，这里主要是两块，一个是操作系统层的监控，比如机器负载，IO，网络流量，CPU，内存等操作系统指标的监控。</p><p>另一个是服务质量和业务质量的监控，比如服务的可用性，成功率，失败率，容量，QPS 等等。</p><p>常见业务的监控系统先有操作系统层面的监控(这部分较成熟)，然后扩展出其他监控，如 Zabbix，小米的 Open-Falcon，也有一出来就是两者都支持的，如 Prometheus。</p><p>如果对业务监控要求比较高一些，在创业选型中建议可以优先考虑 Prometheus。</p><p>这里有一个有趣的分布，如图 6 所示：</p><p><a href="https://s2.51cto.com/oss/201905/15/21b99b5b0b15308cc64e3c960bb69545.jpg-wh_600x-s_2188355091.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201905/15/21b99b5b0b15308cc64e3c960bb69545.jpg-wh_600x-s_2188355091.jpg" alt=""></a></p><p>图 6：监控系统分布</p><p>亚洲区域使用 Zabbix 较多，而美洲和欧洲，以及澳大利亚使用 Prometheus 居多，换句话说，英文国家地区(发达国家?)使用 Prometheus 较多。</p><p>Prometheus 是由 Sound Cloud 开发的开源监控报警系统和时序列数据库(TSDB)。</p><p>Prometheus 使用 Go 语言开发，是 Google BorgMon 监控系统的开源版本。</p><p>相对于其他监控系统使用的 Push 数据的方式，Prometheus 使用的是 Pull 的方式，其架构如图 7 所示：</p><p><a href="https://s4.51cto.com/oss/201905/15/e3a4c8c13cfc834cdd214eb74cab99b6.jpg-wh_600x-s_2918007288.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201905/15/e3a4c8c13cfc834cdd214eb74cab99b6.jpg-wh_600x-s_2918007288.jpg" alt=""></a></p><p>图 7：Prometheus 架构图</p><p>如上图所示，Prometheus 包含的主要组件如下：</p><ul><li>Prometheus Server：主要负责数据采集和存储，提供 PromQL 查询语言的支持。Server 通过配置文件、文本文件、ZooKeeper、Consul、DNS SRV Lookup 等方式指定抓取目标。</li></ul><p>根据这些目标，Server 会定时去抓取 Metrics 数据，每个抓取目标需要暴露一个 HTTP 服务的接口给它定时抓取。</p><ul><li>客户端 SDK：官方提供的客户端类库有 Go、Java、Scala、Python、Ruby，其他还有很多第三方开发的类库，支持 Nodejs、PHP、Erlang 等。</li><li>Push Gateway：支持临时性 Job 主动推送指标的中间网关。</li><li>Exporter Exporter：是 Prometheus 的一类数据采集组件的总称。它负责从目标处搜集数据，并将其转化为 Prometheus 支持的格式。</li></ul><p>与传统的数据采集组件不同的是，它并不向中央服务器发送数据，而是等待中央服务器主动前来抓取。</p><p>Prometheus 提供多种类型的 Exporter 用于采集各种不同服务的运行状态。目前支持的有数据库、硬件、消息中间件、存储系统、HTTP 服务器、JMX 等。</p><ul><li>Alertmanager：是一个单独的服务，可以支持 Prometheus 的查询语句，提供十分灵活的报警方式。</li><li>Prometheus HTTP API 的查询方式，自定义所需要的输出。</li><li>Grafana：是一套开源的分析监视平台，支持 Graphite，InfluxDB，OpenTSDB，Prometheus，Elasticsearch，CloudWatch 等数据源，其 UI 非常漂亮且高度定制化。</li></ul><p>创业公司选择 Prometheus + Grafana 的方案，再加上统一的服务框架(如 gRPC)，可以满足大部分中小团队的监控需求。</p><p><strong>配置系统</strong></p><p>随着程序功能的日益复杂，程序的配置日益增多：各种功能的开关、降级开关，灰度开关，参数的配置、服务器的地址、数据库配置等等。</p><p>除此之外，对后台程序配置的要求也越来越高：配置修改后实时生效，灰度发布，分环境、分用户，分集群管理配置，完善的权限、审核机制等等。</p><p>在这样的大环境下，传统的通过配置文件、数据库等方式已经越来越无法满足开发人员对配置管理的需求。</p><p>业界有如下两种方案：</p><ul><li>基于 ZK 和 Etcd，支持界面和 API ，用数据库来保存版本历史，预案，走审核流程，最后下发到 ZK 或 Etcd 这种有推送能力的存储里(服务注册本身也是用 ZK 或 Etcd，选型就一块了)。</li></ul><p>客户端都直接和 ZK 或 Etcd 打交道。至于灰度发布，各家不同，有一种实现是同时发布一个需要灰度的 IP 列表，客户端监听到配置节点变化时，对比一下自己是否属于该列表。</p><p>PHP 这种无状态的语言和其他 ZK/Etcd 不支持的语言，只好自己在客户端的机器上起一个 Agent 来监听变化，再写到配置文件或共享内存，如 360 的 Qconf。</p><ul><li>基于运维自动化的配置文件的推送，审核流程，配置数据管理和方案一类似，下发时生成配置文件，基于运维自动化工具如 Puppet，Ansible 推送到每个客户端，而应用则定时重新读取这个外部的配置文件，灰度发布在下发配置时指定 IP 列表。</li></ul><p>创业公司前期不需要这种复杂，直接上 ZK，弄一个界面管理 ZK 的内容，记录一下所有人的操作日志，程序直连 ZK，或者或者用 Qconf 等基于 ZK 优化后的方案。</p><p><strong>发布系统/部署系统</strong></p><p>从软件生产的层面看，代码到最终服务的典型流程如图 8 所示：</p><p><a href="https://s1.51cto.com/oss/201905/15/07d5073966aee22dba6fec15a091b689.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201905/15/07d5073966aee22dba6fec15a091b689.jpg" alt=""></a></p><p>图 8：流程图</p><p>从上图中可以看出，从开发人员写下代码到服务最终用户是一个漫长过程，整体可以分成三个阶段：</p><ul><li>从代码(Code)到成品库(Artifact)这个阶段主要对开发人员的代码做持续构建并把构建产生的制品集中管理，是为部署系统准备输入内容的阶段。</li><li>从制品到可运行服务 这个阶段主要完成制品部署到指定环境，是部署系统的最基本工作内容。</li><li>从开发环境到最终生产环境 这个阶段主要完成一次变更在不同环境的迁移，是部署系统上线最终服务的核心能力。</li></ul><p>发布系统集成了制品管理，发布流程，权限控制，线上环境版本变更，灰度发布，线上服务回滚等几方面的内容，是开发人员工作结晶最终呈现的重要通道。</p><p>开源的项目中没有完全满足的项目，如果只是 Web 类项目，Walle、Piplin 都是可用的，但是功能不太满足，创业初期可以集成 Jenkins + Gitlab + Walle(可以考虑两天时间完善一下)。</p><p>以上方案基本包括制品管理，发布流程，权限控制，线上环境版本变更，灰度发布(需要自己实现)，线上服务回滚等功能。</p><p><strong>跳板机</strong></p><p>跳板机面对的是需求是要有一种能满足角色管理与授权审批、信息资源访问控制、操作记录和审计、系统变更和维护控制要求，并生成一些统计报表配合管理规范来不断提升 IT 内控的合规性。</p><p>它能对运维人员操作行为的进行控制和审计，对误操作、违规操作导致的操作事故，快速定位原因和责任人。其功能模块一般包括：帐户管理、认证管理、授权管理、审计管理等等。</p><p>开源项目中，Jumpserver 能够实现跳板机常见需求，如授权、用户管理、服务器基本信息记录等，同时又可批量执行脚本等功能。</p><p>其中录像回放、命令搜索、实时监控等特点，又能帮助运维人员回溯操作历史，方便查找操作痕迹，便于管理其他人员对服务器的操作控制。</p><p><strong>机器管理</strong></p><p>机器管理的工具选择的考量可以包含以下三个方面：</p><ul><li>是否简单，是否需要每台机器部署 Agent(客户端)。</li><li>语言的选择(Puppet/Chef vs Ansible/SaltStack )开源技术，不看官网不足以熟练，不懂源码不足以精通;Puppet、Chef 基于 Ruby 开发，Ansible、SaltStack 基于 Python 开发的。</li><li>速度的选择(Ansible vs SaltStack)，Ansible 基于 SSH 协议传输数据，SaltStack 使用消息队列 zeroMQ 传输数据。</li></ul><p>大规模并发的能力对于几十台到 200 台规模的兄弟来讲，Ansible 的性能也可接受，如果一次操作上千台，用 SaltStack 好一些。</p><p>如图 9 所示：</p><p><a href="https://s1.51cto.com/oss/201905/15/fa52621b952fe4469f94a65668d5ea43.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201905/15/fa52621b952fe4469f94a65668d5ea43.jpg" alt=""></a></p><p>图 9：机器管理软件对比</p><p>一般创业公司选择 Ansible 能解决大部分问题，其简单，不需要安装额外的客户端，可以从命令行来运行，不需要使用配置文件。</p><p>至于比较复杂的任务，Ansible 配置通过名为 Playbook 的配置文件中的 YAML 语法来加以处理。Playbook 还可以使用模板来扩展其功能。</p><p><strong>创业公司的选择</strong></p><p>选择合适的语言：</p><ul><li>选择团队熟悉的/能掌控的，创业公司人少事多，无太多冗余让研发团队熟悉新的语言，能快速上手，能快速出活，出了问题能快速解决的问题的语言才是好的选择。</li><li>选择更现代一些的，这里的现代是指语言本身已经完成一些之前需要特殊处理的特性，比如内存管理，线程等等。</li><li>选择开源轮子多的或者社区活跃度高的，这个原则是为了保证在开发过程中减少投入，有稳定可靠的轮子可以使用，遇到问题可以在网上快速搜索到答案。</li><li>选择好招人的一门合适的语言会让创业团队减少招聘的成本，快速招到合适的人。</li><li>选择能让人有兴趣的与上面一点相关，让人感兴趣，在后面留人时有用。</li></ul><p>选择合适的组件和云服务商：</p><ul><li>选择靠谱的云服务商。</li><li>选择云服务商的组件。</li><li>选择成熟的开源组件，而不是最新出的组件。</li><li>选择采用在一线互联网公司落地并且开源的，且在社区内形成良好口碑的产品。</li><li>开源社区活跃度。</li></ul><p>选择靠谱的云服务商，其实这是一个伪命题，因为哪个服务商都不靠谱，他们所承诺的那些可用性问题基本上都会在你的身上发生。</p><p>这里我们还是需要自己做一些工作，比如多服务商备份，如用 CDN，你一定不要只选一家，至少选两家，一个是灾备，保持后台切换的能力，另一个是多点覆盖，不同的服务商在 CDN 节点上的资源是不一样的。</p><p>选择了云服务商以后，就会有很多的产品你可以选择了，比较存储，队列这些都会有现成的产品，这个时候就纠结了，是用呢?还是自己在云主机上搭呢?</p><p>在这里我的建议是前期先用云服务商的，大了后再自己搞，这样会少掉很多运维的事情，但是这里要多了解一下云服务商的组件特性以及一些坑。</p><p>比如他们内网会经常断开，他们升级也会闪断，所以在业务侧要做好容错和规避。</p><p>关于开源组件，尽可能选择成熟的，成熟的组件经历了时间的考验，基本不会出大的问题，并且有成套的配套工具，出了问题在网上也可以很快的找到答案，你所遇到的坑基本上都有人踩过了。</p><p>制定流程和规范：</p><ul><li>制定开发的规范，代码及代码分支管理规范，关键性代码仅少数人有权限</li><li>制定发布流程规范，从发布系统落地</li><li>制定运维规范</li><li>制定数据库操作规范，收拢数据库操作权限</li><li>制定告警处理流程，做到告警有人看有人处理</li><li>制定汇报机制，晨会/周报</li></ul><p><strong>自研和选型合适的辅助系统</strong></p><p>所有的流程和规范都需要用系统来固化，否则就是空中楼阁，如何选择这些系统呢?</p><p>参照上个章节咱们那些开源的，对比一下选择的语言，组件之类的，选择一个最合适的即可。</p><p>比如项目管理的，看下自己是什么类型的公司，开发的节奏是怎样的，瀑布，敏捷的按项目划分，还是按客户划分等等，平时是按项目组织还是按任务组织等等。</p><p>比如日志系统，之前是打的文本，那么上一个 ELK，规范化一些日志组件，基本上很长一段时间内不用考虑日志系统的问题，最多拆分一下或者扩容一下。等到组织大了，自己搞一个日志系统。</p><p>比如代码管理，项目管理系统这些都放内网，安全，在互联网公司来说，属于命脉了，命脉的东西还是放在别人拿不到或很难拿到的地方会比较靠谱一些。</p><p><strong>选择过程中需要思考的问题</strong></p><p>技术栈的选择有点像做出了某种承诺，在一定的时间内这种承诺没法改变，于是我们需要在选择的时候有一些思考。</p><p>看前面内容，有一个词出现了三次，合适，选择是合适的，不是最好，也不是最新，是最合适，适合是针对当下，这种选择是最合适的吗?</p><p>比如用 Go 这条线的东西，技术比较新，业界组件储备够吗?组织内的人员储备够吗?学习成本多少?写出来的东西能满足业务性能要求吗?能满足时间要求吗?</p><p>向未来看一眼，在一年到三年内，我们需要做出改变吗?技术栈要做根本性的改变吗?如果组织发展很快，在 200 人，500 人时，现有的技术栈是否需要大动?</p><p>创业过程中需要考虑成本，这里的成本不仅仅是花费多少钱，付出多少工资，有时更重要的是时间成本，很多业务在创业时大家拼的就是时间，就是一个时间窗，过了就没你什么事儿了。</p><p><strong>基于云的创业公司后台技术架构</strong></p><p>结合上面内容的考量，在对一个个系统和组件的做选型之后，以云服务为基础，一个创业公司的后台技术架构如图 10 所示：</p><p><a href="https://s2.51cto.com/oss/201905/15/730cf00aaf73b5911cc8e85fd2dcd9d5.jpg-wh_600x-s_1516492473.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201905/15/730cf00aaf73b5911cc8e85fd2dcd9d5.jpg-wh_600x-s_1516492473.jpg" alt=""></a></p><p>图 10：后台技术架构</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>29岁MIT博士小姐姐努力6年、处理半吨硬盘数据，“洗”出人类第一张黑洞照片</title>
      <link href="/2019/04/17/29%E5%B2%81MIT%E5%8D%9A%E5%A3%AB%E5%B0%8F%E5%A7%90%E5%A7%90%E5%8A%AA%E5%8A%9B6%E5%B9%B4%E3%80%81%E5%A4%84%E7%90%86%E5%8D%8A%E5%90%A8%E7%A1%AC%E7%9B%98%E6%95%B0%E6%8D%AE%EF%BC%8C%E2%80%9C%E6%B4%97%E2%80%9D%E5%87%BA%E4%BA%BA%E7%B1%BB%E7%AC%AC%E4%B8%80%E5%BC%A0%E9%BB%91%E6%B4%9E%E7%85%A7%E7%89%87/"/>
      <url>/2019/04/17/29%E5%B2%81MIT%E5%8D%9A%E5%A3%AB%E5%B0%8F%E5%A7%90%E5%A7%90%E5%8A%AA%E5%8A%9B6%E5%B9%B4%E3%80%81%E5%A4%84%E7%90%86%E5%8D%8A%E5%90%A8%E7%A1%AC%E7%9B%98%E6%95%B0%E6%8D%AE%EF%BC%8C%E2%80%9C%E6%B4%97%E2%80%9D%E5%87%BA%E4%BA%BA%E7%B1%BB%E7%AC%AC%E4%B8%80%E5%BC%A0%E9%BB%91%E6%B4%9E%E7%85%A7%E7%89%87/</url>
      
        <content type="html"><![CDATA[<p>人类第一次看到黑洞照片之后，一名小姐姐的照片也跟着刷了屏。<br><a id="more"></a><br><a href="https://s2.51cto.com/oss/201904/12/c2fc3565cbf52df2615a0f0e27e8ecd9.jpg-wh_651x-s_2775530103.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201904/12/c2fc3565cbf52df2615a0f0e27e8ecd9.jpg-wh_651x-s_2775530103.jpg" alt=""></a></p><p>她激动得十指交叉，身边的电脑屏幕上，黑洞的样子模糊显现出来，一行行代码在旁边滚动。</p><p>在Facebook上发出这张照片时，小姐姐说：</p><p>看着我做的第一张黑洞照片一点点“洗”出来，真是不可思议。</p><p>“我做的第一张黑洞照片”？</p><p>对，全人类看到的第一张黑洞照片，就是她“做”的。</p><p>这位小姐姐名叫Katie Bouman (凯蒂 · 布曼) ，今年29岁。她带领算法团队“洗”出了这张照片，也成了第一批“看到”黑洞的人类之一。</p><p>人类“拍”到的第一张黑洞照片，并不是像我们拿手机拍照那样，点下屏幕就好，而是需要分布在全球各地的许多天文望远镜在同一时间“按下快门”，记录无线电数据。</p><p><a href="https://s4.51cto.com/oss/201904/12/6ed5575ceb0fcb4bf4535dc64a7c7215.jpg-wh_600x-s_3407854538.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201904/12/6ed5575ceb0fcb4bf4535dc64a7c7215.jpg-wh_600x-s_3407854538.jpg" alt=""></a></p><p>然后，再依靠机器学习算法，把数据拼到一起，重建出图像。而这个“洗照片”的任务，就是凯蒂在MIT读博时做的项目。</p><p><strong>搞定半吨硬盘</strong></p><p>六年前，凯蒂开始了她在MIT CSAIL的博士生涯，想要研究“如何看见或者测量肉眼看不见的东西”，黑洞简直是再合适不过的研究对象了。因此，她加入了EHT（事件视界望远镜）团队。</p><p><a href="https://s4.51cto.com/oss/201904/12/18e65fee9f254c35f10dfbacee83bd04.jpg-wh_600x-s_4166319682.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201904/12/18e65fee9f254c35f10dfbacee83bd04.jpg-wh_600x-s_4166319682.jpg" alt=""></a></p><p>凯蒂的本科读的密歇根大学的电气工程，硕士读的是MIT的电气工程和计算机科学专业，可以说，对于天文方面，她当时一窍不通。</p><p>就这样，她开始研究“把多台天文望远镜获得的数据合成一张黑洞照片”的算法。</p><p>一搞就是三年的秘密工作。在2016年之前，这个项目一直是保密的，小姐姐研究这么激动人心的项目，却憋着不能说，连自己的家人都没告诉。</p><p>而且直到2017年6月，凯蒂的算法才终于可以开始实战。她收到了一堆装着黑洞观测数据的硬盘：</p><p><a href="https://s4.51cto.com/oss/201904/12/b5e2ba388a49a54f970de6e86cfb61d3.jpg-wh_600x-s_490588400.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201904/12/b5e2ba388a49a54f970de6e86cfb61d3.jpg-wh_600x-s_490588400.jpg" alt=""></a></p><p>这些硬盘足有半吨，从世界各地用飞机运来。数量之大，甚至让人联想到1969年玛格丽特·汉密尔顿为阿波罗11号登月而准备的一人高的代码。</p><p><a href="https://s5.51cto.com/oss/201904/12/c6bd5828b2f741138a58c95127a1588f.jpg-wh_600x-s_302098233.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201904/12/c6bd5828b2f741138a58c95127a1588f.jpg-wh_600x-s_302098233.jpg" alt=""></a></p><p>这些硬盘中的数据，来自智利、夏威夷、南极洲、亚利桑那、西班牙、墨西哥六个地方的一共八台天文望远镜。</p><p>天文望远镜获取的数据量非常大，一晚上就能收集到2PB（约2000TB）。如此庞大的数据难以用网络传输，必须装到硬盘里，空运到MIT。</p><p>而且，这半吨硬盘里的数据不仅仅是黑洞，还包含天空中的各种复杂、凌乱的数据，凯蒂要靠这些数据，拼出一张完整的黑洞写真。</p><p>本来，根据射电望远镜数据还原天体图像需要人类天文学家参与。他们以自己的专业知识，将成像算法指引到他们认为正确的方向。</p><p>然而面对PB级稀疏、嘈杂的数据，想靠人力从中找出图像太难了。于是，他们使用了机器学习方法。</p><p>虽然这支团队已经花了好几年的时间构建算法，在合成数据上实验，但直到有了这些硬盘，他们才能真正知道他们的算法，是不是真的能捕捉到不可见的黑洞。</p><p>这项任务究竟什么样？</p><p>就好比，你把一颗鹅卵石扔进池塘，却还想看到它的样子。</p><p><strong>一石激起的涟漪</strong></p><p>入水的瞬间，石子会激起一圈一圈的涟漪。</p><p><a href="https://s5.51cto.com/oss/201904/12/a18120b3a4b2f3c95935db6600b81d71.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201904/12/a18120b3a4b2f3c95935db6600b81d71.jpg" alt=""></a></p><p>只要这些涟漪，就算石子沉到水下，也依然可以通过算法重现它的模样。</p><p>黑洞，就像是这颗已经看不见的鹅卵石。</p><p>不同的望远镜收到的两股无线电波相遇，就起了涟漪，学名叫做“干涉”。</p><p><a href="https://s5.51cto.com/oss/201904/12/428dddd0e07f75b5faa8600263712814.jpg-wh_600x-s_2431272709.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201904/12/428dddd0e07f75b5faa8600263712814.jpg-wh_600x-s_2431272709.jpg" alt=""></a></p><p>而凯蒂提出的CHIRP算法，便是依靠干涉来重建黑洞的。</p><p>具体来说，从银河中心传来的无线电信号，到达两台望远镜的时间是不一样的，干涉也是这样发生的。</p><p>所以说，重建黑洞照片，最重要的就是时间差。</p><p><a href="https://s2.51cto.com/oss/201904/12/f838f06a2a299d1dd6d6f163d62e89bf.jpg-wh_600x-s_3783789814.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201904/12/f838f06a2a299d1dd6d6f163d62e89bf.jpg-wh_600x-s_3783789814.jpg" alt=""></a></p><p>可是，地球有厚重的大气层保护着，无线电波穿过大气层的时候，速度会变慢，时间的测定也就不够准确了。</p><p>所以，小姐姐想出了一种机智的方法，来解决这个问题：</p><p>如果每一个测量值，都是三台望远镜 (不是两台) 相乘的结果，大气带来的误差就能相互抵消了。</p><p>这样一来，算法有了，团队便开始“冲洗”黑洞的照片了。</p><p><strong>一洗就是两年</strong></p><p>半吨硬盘的数据量处理起来，工程量还是太大了。</p><p>洗照片的过程中，一度有四个团队同时工作，每个团队负责分析一部分数据。</p><p>原本预计一年洗好的照片，花了两年时间才让世界看到。</p><p>除了耗时之外，小姐姐也说过，团队就是一口大锅，里面有天文学家，物理学家，数学家，工程师……如果不是这样，也不可能完成这个从前看来不可能的任务。</p><p>而她的工作，是在照片终于合成成功并公布之后，凯蒂终于可以告诉全世界，合成第一张黑洞照片的意义：</p><p>这是我们了解黑洞的一个窗口，从这里开始，我们验证了我们的物理规律。虽然我们已经靠理论推断出黑洞的样子，但只有亲眼所见才能验证，因此，看到黑洞图像也是巨大的科学进步。</p><p>现在，凯蒂早已博士毕业，继续在MIT的EHT项目做了一段时间的博士后之后，即将成为加州理工大学的助理教授。</p><p><strong>庞大的团队</strong></p><p>除了凯蒂之外，整个团队还有很多人，他们来自各种各样的领域。</p><p><a href="https://s2.51cto.com/oss/201904/12/c7ac51493001da13b9df1cb59bf6e53b.jpg-wh_600x-s_1658652993.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201904/12/c7ac51493001da13b9df1cb59bf6e53b.jpg-wh_600x-s_1658652993.jpg" alt=""></a></p><p>凯蒂在2017年的TED演讲中分享了团队的核心成员名单，包括：</p><p>Sheperd Doeleman</p><p>哈佛大学黑洞计划观测助理主任</p><p>Andrew Chael</p><p>哈佛大学黑洞计划研究生</p><p>Lindy Blackburn</p><p>哈佛大学黑洞计划射电天文学家</p><p>Michael Johnson</p><p>哈佛 - 史密森尼天体物理中心研究员</p><p>Katherine Rosenfeld</p><p>哈佛 - 史密森尼天体物理中心研究员</p><p>Hotaka Shiokawa</p><p>哈佛 - 史密森尼天体物理中心博士后</p><p>William T. Freeman</p><p>MIT计算机科学与人工智能实验室教授</p><p>Vincent Fish</p><p>MIT Haystack天文台研究科学家</p><p>Kazumori Akiyama</p><p>MIT Haystack天文台博士后</p><p>Daniel Zoran</p><p>DeepMind研究科学家</p><p>传送门</p><p>最后，如果你对他们所用的算法感兴趣，可以读一下这篇论文：</p><p>Computational Imaging for VLBI Image Reconstruction</p><p><a href="https://dspace.mit.edu/handle/1721.1/103077" target="_blank" rel="noopener">https://dspace.mit.edu/handle/1721.1/103077</a></p><p>原文地址：<a href="http://stor.51cto.com/art/201904/594901.htm" target="_blank" rel="noopener">http://stor.51cto.com/art/201904/594901.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么国内程序员996.ICU，而美国程序员却可以轻松养老</title>
      <link href="/2019/04/08/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9B%BD%E5%86%85%E7%A8%8B%E5%BA%8F%E5%91%98996-ICU%EF%BC%8C%E8%80%8C%E7%BE%8E%E5%9B%BD%E7%A8%8B%E5%BA%8F%E5%91%98%E5%8D%B4%E5%8F%AF%E4%BB%A5%E8%BD%BB%E6%9D%BE%E5%85%BB%E8%80%81/"/>
      <url>/2019/04/08/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9B%BD%E5%86%85%E7%A8%8B%E5%BA%8F%E5%91%98996-ICU%EF%BC%8C%E8%80%8C%E7%BE%8E%E5%9B%BD%E7%A8%8B%E5%BA%8F%E5%91%98%E5%8D%B4%E5%8F%AF%E4%BB%A5%E8%BD%BB%E6%9D%BE%E5%85%BB%E8%80%81/</url>
      
        <content type="html"><![CDATA[<p>前几天，一位“疑似阿里码农的小伙子骑车逆行被拦后下跪、爆哭”的视频在网络热传，令人唏嘘。</p><p><a href="https://s1.51cto.com/oss/201904/08/1eabaa7cf2427d761a90bacc302f70e5.jpg-wh_651x-s_1609140146.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201904/08/1eabaa7cf2427d761a90bacc302f70e5.jpg-wh_651x-s_1609140146.jpg" alt=""></a><br><a id="more"></a><br>不只是这位小伙子，最近整个中国程序员群体的加班问题在全球范围内引发了话题。详情可参见文章《无数网友泪奔：码农骑车逆行被拦后当场崩溃!》</p><p><a href="https://s1.51cto.com/oss/201904/08/f1d73bdc53e8a44a73895b187cb74d64.jpg-wh_600x-s_112667361.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201904/08/f1d73bdc53e8a44a73895b187cb74d64.jpg-wh_600x-s_112667361.jpg" alt=""></a></p><p>向来以“沉默的大多数”形象示人的中国程序员，这次吹响了集结号，团结一致地公开反对国内已约定俗成的“996”工作制( 9 AM 到岗， 9 PM下班，每周工作 6 天)。</p><p>一个起源于中国码农的 Github“996.ICU”项目，正以燎原之势席卷全球各地的程序员圈子：</p><p><a href="https://s3.51cto.com/oss/201904/08/a1c2be13c0ca4c8729e92cbd5c8a237e.jpg-wh_600x-s_1866736098.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201904/08/a1c2be13c0ca4c8729e92cbd5c8a237e.jpg-wh_600x-s_1866736098.jpg" alt=""></a></p><p>996ICU 即“工作 996，生病 ICU”，某程序员注册了一个叫做 996.ICU 的域名，并在这个网页上大举控诉部分互联网公司实行 996 工作制。截至今日，996.ICU 网站已被翻译为多国语言版本。</p><p>Python 之父都在 Twitter 上声援他远在中国的同行们：</p><p><a href="https://s1.51cto.com/oss/201904/08/62c1e78ce5e80766445871259bad8fb2.jpg-wh_600x-s_982767212.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201904/08/62c1e78ce5e80766445871259bad8fb2.jpg-wh_600x-s_982767212.jpg" alt=""></a></p><p><strong>Python 之父：996 工作制度非常不人性化</strong></p><p>该条推文下集结了来自印度、日本、欧洲程序员们，他们也送上了同情和慰问：</p><p><a href="https://s1.51cto.com/oss/201904/08/c8cb20c90e92225b15939a8f5df6eda7.jpg-wh_600x-s_4231521695.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201904/08/c8cb20c90e92225b15939a8f5df6eda7.jpg-wh_600x-s_4231521695.jpg" alt=""></a></p><p>日本网友：无言以对并感到抱歉。</p><p><a href="https://s5.51cto.com/oss/201904/08/0410d425f845bdc8621a1f7084363604.jpg-wh_600x-s_646586607.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201904/08/0410d425f845bdc8621a1f7084363604.jpg-wh_600x-s_646586607.jpg" alt=""></a></p><p>中国程序员：今天周六，我在加班;印度程序员：哥们儿，你不是一个人。</p><p><a href="https://s1.51cto.com/oss/201904/08/41a2c9b140908a67d956a9cd0e7f2137.jpg-wh_600x-s_1455903696.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201904/08/41a2c9b140908a67d956a9cd0e7f2137.jpg-wh_600x-s_1455903696.jpg" alt=""></a></p><p>白俄罗斯程序员：这不就是工作效率不重要，但人得呆在办公室的路子吗。</p><p>一些在美国的华人程序员也纷纷晒出自己在美国工作的日常，并支持国内同胞踊跃维权：</p><p><a href="https://s2.51cto.com/oss/201904/08/a4efea3274e52eb39259265a657743bf.jpg-wh_600x-s_1573928232.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201904/08/a4efea3274e52eb39259265a657743bf.jpg-wh_600x-s_1573928232.jpg" alt=""></a></p><p><a href="https://s5.51cto.com/oss/201904/08/6e7598563e971134bbfc7c8dc5efe714.jpg-wh_600x-s_331287721.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201904/08/6e7598563e971134bbfc7c8dc5efe714.jpg-wh_600x-s_331287721.jpg" alt=""></a></p><p>看完美国同行的分享，国内的程序员更不解了：为什么同样是程序员，在中国工作 996 生病 ICU，而美国的程序员工作轻松、加班少?</p><p>其实，996 是一种工作形式，它可能出现在各行各业。在美国，也有很多公司的工作强度非常贴近我们熟知的 996 形式。</p><p>但之所以这次国内程序员对 996“起义”，还得到了国外其他行业的人的声援，其背后原因不是拘泥在 996 本身，而是 996 之后的结果与程序员自己所获得的价值欠缺平衡。</p><p>大部分人对于 996.ICU 的抵制主要在于：</p><p>虽然 996 并且 ICU，可既没有获得与 996 相符的薪酬，也没有拿到在 ICU 之前的补偿 。</p><p>996 成为了业界流行规则，为了迎合这个规则不被淘汰，继而漫无目的地瞎忙，暗无天日。</p><p>看似付出了 996 的劳动，但实际上对公司产品和自己职场发展的价值都不大，没有终点。</p><p>为了进一步讨论这个问题，我们总结了知乎上的高赞回答，探讨了中美工程师加班文化的异同，以及对于 996 现象的看法。</p><p><strong>工程师文化差异</strong></p><p>中美两国科技公司差别非常大的一点就是：美国科技公司注重工程师文化，而国内技术团队话语权低。</p><p>在国内的互联网公司，技术团队一般没什么话语权，基本上是被产品层“赶着走”。</p><p>产品层在还没有想清楚某个功能的价值和收益，就给技术团队提需求、让他们“先做出来看看”，是常有的事儿。</p><p><a href="https://s3.51cto.com/oss/201904/08/716d24c45e10ad5f906ddd7a3e1b40b7.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201904/08/716d24c45e10ad5f906ddd7a3e1b40b7.jpg" alt=""></a></p><p>图片来源：吓脑湿</p><p>再加上国内互联网产品迭代速度很快，为了抢占市场份额，需要不断地更新 Feature。一旦看到竞品出了什么新功能，加班加点，必须赶上!</p><p><a href="https://s2.51cto.com/oss/201904/08/fe0763d26ee72e102c0cf1bf12cde9e0.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201904/08/fe0763d26ee72e102c0cf1bf12cde9e0.jpg" alt=""></a></p><p>但在美国的科技公司，技术团队一般都是中心和主流。像谷歌就是非常典型的 engineering-driven 文化，对于一个没想清楚的需求，技术团队可以直接拒绝。</p><p><a href="https://s5.51cto.com/oss/201904/08/d8125e5da76b8383a28b5405011b0111.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201904/08/d8125e5da76b8383a28b5405011b0111.jpg" alt=""></a></p><p>同时，技术团队会自主地花很多时间在技术优化上面。宁可延后产品上线时间，也要按照最优的技术方案来推进。</p><p>当然，美国也有不少像 Facebook 这样 product-driven、鼓励快节奏的公司，这些公司的程序员日常压力自然也比谷歌的更大一些。</p><p><strong>基础架构差异</strong></p><p>美国的科技公司一般都标配底层架构团队，也就是我们说的 Infra 部门，这也是工程师文化导向的一个结果。</p><p>对于一些中大型的互联网公司来说，Infra 部门非常重要，因为它是产品的基础保障(当业务量非常大的时候，底层系统的一点变动就会引起巨大的业务损失)，同时又能帮助节省成本，提高资源利用率。</p><p>比如在 Google，所有的架构都被封装成 Service 了，使用起来只要关心业务逻辑和资源分配就行，非常省时间。在新的 Server 里面加 Feature 也只需要加一个新的模块就行。</p><p>Google 做 Infra 时间比 Facebook 长，Infra 种类比 Facebook 多，这也是 Google 比 Facebook 轻松一点的一个原因。</p><p>—— 知乎作者：李小白</p><p><a href="https://s3.51cto.com/oss/201904/08/c5f77a0ef5eb0eb1395ecae17ebbf4c9.jpg-wh_600x-s_1897522369.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201904/08/c5f77a0ef5eb0eb1395ecae17ebbf4c9.jpg-wh_600x-s_1897522369.jpg" alt=""></a></p><p>反观国内，能做到这样的公司屈指可数。毕竟，做底层架构并不是能够赚钱、直接为公司带来收益的业务。</p><p>中小公司不必说，在这样高度竞争环境下，哪有财力养一个不能直接带来 KPI 提升的团队，即使有钱，也没时间搞优化。</p><p>而大公司呢，即使腾讯这样的体量，有一个 TEG 事业群做基础设施建设。但是现实情况是，依然没有全公司通用的内部工具，各个团队依然在重复造轮子。</p><p>阿里算是做的很好的，前几年开始推行的大中台战略取得了显著的成果，解放了各个产品部门的生产力。</p><p>—— 知乎作者：L-Jay</p><p><strong>社会形态差异</strong></p><p>美国人非常讲究个人生活和工作的平衡，人权在公司的利益之上。如果企业过分压榨员工，员工反手一下子就能把公司告上劳工部。</p><p>所以我们看到，在美国公司上班，踩着点下班是再正常不过的事情。老板甚至会鼓励员工休年假出去放松身心。</p><p>来 Offer 的一位学员跟我们分享他在谷歌的工作经历：谷歌绝不鼓励加班，而是鼓励大家努力做到 Work-Life-Balance。</p><p>我的同事们也有周末跑到公司来的，但是目的不是加班，而是和其他同事们一起打高尔夫、沙滩排球之类。</p><p><a href="https://s3.51cto.com/oss/201904/08/bec02bad1b090f97f45340099da25770.jpg-wh_600x-s_3563952391.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201904/08/bec02bad1b090f97f45340099da25770.jpg-wh_600x-s_3563952391.jpg" alt=""></a></p><p>工作第一周的周末，就有同组的新同事约我去公司玩桌游。我心里还纳闷，桌游为啥一定要去公司玩呢?</p><p>去了一次才知道：</p><ul><li>在公司的会议室玩桌游，不会吵到邻居，隔音效果很好，还有大屏幕。</li><li>同事一起玩桌游，是 Team Building 的重要方式，痛快淋漓的对弈之后，同事之间也加深了感情和彼此之间的默契。</li></ul><p>而国内职场却流传着这样风气：加班就是努力，不加班就是态度不行。</p><p>甚至一些员工为了让自己看起来很辛苦，上班时间一会儿看看新闻，一会儿又刷刷朋友圈磨磨唧唧，非要把工作时间拖到晚上 9、10 点。这却反而让那些能够高效、准时完成工作的人看起来清闲了。</p><p><strong>美国也有加班的程序员，中国也有不加班的程序员</strong></p><p>其实，美国的程序员并不是不加班，像 Uber、Facebook、以及一些 Startups 的程序员，他们有时候的工作强度甚至比 996 更大。而素来以”工作轻松“闻名的谷歌，也存在着加班现象。</p><p>来 Offer 的创始人孙老师曾跟我们分享过他在谷歌加班的亲身经历：在 Facebook 上市之前的那几年，Google 曾经试图用 Google+ 和 Facebook 在社交上一较高下。于是高层从 Google 的各个组招募了一个精英团队，形成了 Google+ 的部门。</p><p>竞争最激烈的时候，Google+ 和 Facebook 两边同时 Lockdown，上到 Google+ 的大老板，下到每一个程序员，除了婚丧嫁娶生病，其他情况一律不允许请假。</p><p>那个时候，Google+ 几乎每天都要 Push 上一个新的版本，办公室里全都是五颜六色的帐篷，大家吃住都在公司，洗澡用公司的健身房，每个人都在高压下全力以赴。</p><p>当然，这种公司硬性加班的特殊情况，在 Google 确实不是很多。但是，我们为什么很少听到美国的程序员抱怨加班?</p><p>这是因为，只要付出和回报是对等的，做的事情是有意义的，996、加班都不是问题。</p><p>尤其是那些升职加薪快的程序员，总是过的很忙。除了出色地完成自己手头上的任务，他们还会有意识地提高自己的技术，有目的地增加自己的项目经历，以产生更大的 Impact。</p><p>总而言之，美国程序员加不加班，工作辛不辛苦，不仅看所在的公司和组，也看个人的选择。</p><p>而中国的程序员也并不都是活在“压迫”之下，996.ICU 推出之后，GitHub 随即又推出了一个新的项目：955.WLB ，即国内也有朝九晚五，能平衡工作/生活的程序员群体。</p><p><a href="https://s4.51cto.com/oss/201904/08/1c8ce9bf6d1de5240f43af9d2dc32693.jpg-wh_600x-s_793425293.jpg" target="_blank" rel="noopener"><img src="https://s4.51cto.com/oss/201904/08/1c8ce9bf6d1de5240f43af9d2dc32693.jpg-wh_600x-s_793425293.jpg" alt=""></a></p><p>不过不得不承认的是，该项目列出的公司大部分是外企。目前陆陆续续有一些浏览器开始屏蔽了 996.ICU 的 Github 项目。</p><p><a href="https://s3.51cto.com/oss/201904/08/9caebf1662242b8562df34237b37aafc.png" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201904/08/9caebf1662242b8562df34237b37aafc.png" alt=""></a></p><p>综上所述，想从根本上打破国内 996.ICU 制度，不仅需要从公司角度加强对技术团队的尊重和重视，还需要扭转长期积累的社会风气，无疑是个漫长的过程。</p><p>不过想要脱离 996ICU 这趟浑水中，程序员还有一个办法：提升自身实力。说白了，国内知名企业之所以敢推行 996，有很大一部分原因在于，公司不缺程序员，尤其是不缺平庸的程序员。</p><p>而实力超群、无可替代的程序员，公司自然是不敢得罪的。举个真实的例子：腾讯最嚣张的程序员、微信之父张小龙。</p><p><a href="https://s2.51cto.com/oss/201904/08/3ab4da1fdc8e0fe278df918d517009dc.jpg-wh_600x-s_2893700237.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201904/08/3ab4da1fdc8e0fe278df918d517009dc.jpg-wh_600x-s_2893700237.jpg" alt=""></a></p><p>张小龙最大的爱好，除了玩电动，就是睡觉。他经常因为贪睡而上班迟到，因此还被戏称为中国科技界最“懒”的主管。</p><p>腾讯每周马化腾主持的例会，张小龙嫌 2h 的车程太远，起不来床，拒绝参加。</p><p>马化腾却从未因此动怒，只是笑笑，“没事，不怕他上班睡觉，也不怕他打游戏，就怕他被马云挖走，只要他高兴就好”。</p><p><a href="https://s3.51cto.com/oss/201904/08/1b0ef3ae772842d9efe464876e04e2fa.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201904/08/1b0ef3ae772842d9efe464876e04e2fa.jpg" alt=""></a></p><p>美国这边也一样，虽然程序员仍是高需职位，近年来也有越来越多的人试图通过刷题转行高薪的 CS 行业。</p><p>但公司真正想要的是有实力、高水准的程序员。也只有真正有实力的程序员，在哪儿都不会被取代。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支撑百万并发的数据库架构如何设计？</title>
      <link href="/2019/03/05/%E6%94%AF%E6%92%91%E7%99%BE%E4%B8%87%E5%B9%B6%E5%8F%91%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%EF%BC%9F/"/>
      <url>/2019/03/05/%E6%94%AF%E6%92%91%E7%99%BE%E4%B8%87%E5%B9%B6%E5%8F%91%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9E%B6%E6%9E%84%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>看到这个题目，很多人第一反应就是：分库分表啊!但是实际上，数据库层面的分库分表到底是用来干什么的，他的不同的作用如何应对不同的场景，我觉得很多同学可能都没搞清楚。<br><a id="more"></a><br><strong>用一个创业公司的发展作为背景引入</strong></p><p>假如我们现在是一个小创业公司，注册用户就 20 万，每天活跃用户就 1 万，每天单表数据量就 1000，然后高峰期每秒钟并发请求最多就 10。</p><p>天哪!就这种系统，随便找一个有几年工作经验的高级工程师，然后带几个年轻工程师，随便干干都可以做出来。</p><p>因为这样的系统，实际上主要就是在前期快速的进行业务功能的开发，搞一个单块系统部署在一台服务器上，然后连接一个数据库就可以了。</p><p>接着大家就是不停的在一个工程里填充进去各种业务代码，尽快把公司的业务支撑起来。</p><p>如下图所示：</p><p><a href="https://s1.51cto.com/oss/201902/27/ce7023e2622aa57f0c20429342fdfe10.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201902/27/ce7023e2622aa57f0c20429342fdfe10.jpg" alt=""></a></p><p>结果呢，没想到我们运气这么好，碰上个优秀的 CEO 带着我们走上了康庄大道!</p><p>公司业务发展迅猛，过了几个月，注册用户数达到了 2000 万!每天活跃用户数 100 万!每天单表新增数据量达到 50 万条!高峰期每秒请求量达到 1 万!</p><p>同时公司还顺带着融资了两轮，估值达到了惊人的几亿美金!一只朝气蓬勃的幼年独角兽的节奏!</p><p>好吧，现在大家感觉压力已经有点大了，为啥呢?因为每天单表新增 50 万条数据，一个月就多 1500 万条数据，一年下来单表会达到上亿条数据。</p><p>经过一段时间的运行，现在咱们单表已经两三千万条数据了，勉强还能支撑着。</p><p>但是，眼见着系统访问数据库的性能怎么越来越差呢，单表数据量越来越大，拖垮了一些复杂查询 SQL 的性能啊!</p><p>然后高峰期请求现在是每秒 1 万，咱们的系统在线上部署了 20 台机器，平均每台机器每秒支撑 500 请求，这个还能抗住，没啥大问题。但是数据库层面呢?</p><p>如果说此时你还是一台数据库服务器在支撑每秒上万的请求，负责任的告诉你，每次高峰期会出现下述问题：</p><ul><li>你的数据库服务器的磁盘 IO、网络带宽、CPU 负载、内存消耗，都会达到非常高的情况，数据库所在服务器的整体负载会非常重，甚至都快不堪重负了。</li><li>高峰期时，本来你单表数据量就很大，SQL 性能就不太好，这时加上你的数据库服务器负载太高导致性能下降，就会发现你的 SQL 性能更差了。</li><li>最明显的一个感觉，就是你的系统在高峰期各个功能都运行的很慢，用户体验很差，点一个按钮可能要几十秒才出来结果。</li><li>如果你运气不太好，数据库服务器的配置不是特别的高的话，弄不好你还会经历数据库宕机的情况，因为负载太高对数据库压力太大了。</li></ul><p><strong>多台服务器分库支撑高并发读写</strong></p><p>首先我们先考虑第一个问题，数据库每秒上万的并发请求应该如何来支撑呢?</p><p>要搞清楚这个问题，先得明白一般数据库部署在什么配置的服务器上。通常来说，假如你用普通配置的服务器来部署数据库，那也起码是 16 核 32G 的机器配置。</p><p>这种非常普通的机器配置部署的数据库，一般线上的经验是：不要让其每秒请求支撑超过 2000，一般控制在 2000 左右。</p><p>控制在这个程度，一般数据库负载相对合理，不会带来太大的压力，没有太大的宕机风险。</p><p>所以首先第一步，就是在上万并发请求的场景下，部署个 5 台服务器，每台服务器上都部署一个数据库实例。</p><p>然后每个数据库实例里，都创建一个一样的库，比如说订单库。此时在 5 台服务器上都有一个订单库，名字可以类似为：db_order_01，db_order_02，等等。</p><p>然后每个订单库里，都有一个相同的表，比如说订单库里有订单信息表，那么此时 5 个订单库里都有一个订单信息表。</p><p>比如 db_order_01 库里就有一个 tb_order_01 表，db_order_02 库里就有一个 tb_order_02 表。</p><p>这就实现了一个基本的分库分表的思路，原来的一台数据库服务器变成了 5 台数据库服务器，原来的一个库变成了 5 个库，原来的一张表变成了 5 个表。</p><p>然后你在写入数据的时候，需要借助数据库中间件，比如 sharding-jdbc，或者是 mycat，都可以。</p><p>你可以根据比如订单 id 来 hash 后按 5 取模，比如每天订单表新增 50 万数据，此时其中 10 万条数据会落入 db_order_01 库的 tb_order_01 表，另外 10 万条数据会落入 db_order_02 库的 tb_order_02 表，以此类推。</p><p>这样就可以把数据均匀分散在 5 台服务器上了，查询的时候，也可以通过订单 id 来 hash 取模，去对应的服务器上的数据库里，从对应的表里查询那条数据出来即可。</p><p>依据这个思路画出的图如下所示，大家可以看看：</p><p><a href="https://s3.51cto.com/oss/201902/27/5a82c585cf3693decb35754a146f0554.jpg-wh_600x-s_1153234073.jpg" target="_blank" rel="noopener"><img src="https://s3.51cto.com/oss/201902/27/5a82c585cf3693decb35754a146f0554.jpg-wh_600x-s_1153234073.jpg" alt=""></a></p><p>做这一步有什么好处呢?第一个好处，原来比如订单表就一张表，这个时候不就成了 5 张表了么，那么每个表的数据就变成 1/5 了。</p><p>假设订单表一年有 1 亿条数据，此时 5 张表里每张表一年就 2000 万数据了。</p><p>那么假设当前订单表里已经有 2000 万数据了，此时做了上述拆分，每个表里就只有 400 万数据了。</p><p>而且每天新增 50 万数据的话，那么每个表才新增 10 万数据，这样是不是初步缓解了单表数据量过大影响系统性能的问题?</p><p>另外就是每秒 1 万请求到 5 台数据库上，每台数据库就承载每秒 2000 的请求，是不是一下子把每台数据库服务器的并发请求降低到了安全范围内?</p><p>这样，降低了数据库的高峰期负载，同时还保证了高峰期的性能。</p><p><strong>大量分表来保证海量数据下的查询性能</strong></p><p>但是上述的数据库架构还有一个问题，那就是单表数据量还是过大，现在订单表才分为了 5 张表，那么如果订单一年有 1 亿条，每个表就有 2000 万条，这也还是太大了。</p><p>所以还应该继续分表，大量分表。比如可以把订单表一共拆分为 1024 张表，这样 1 亿数据量的话，分散到每个表里也就才 10 万量级的数据量，然后这上千张表分散在 5 台数据库里就可以了。</p><p>在写入数据的时候，需要做两次路由，先对订单 id hash 后对数据库的数量取模，可以路由到一台数据库上，然后再对那台数据库上的表数量取模，就可以路由到数据库上的一个表里了。</p><p>通过这个步骤，就可以让每个表里的数据量非常小，每年 1 亿数据增长，但是到每个表里才 10 万条数据增长，这个系统运行 10 年，每个表里可能才百万级的数据量。</p><p>这样可以一次性为系统未来的运行做好充足的准备，看下面的图，一起来感受一下：</p><p><a href="https://s2.51cto.com/oss/201902/27/82f9be8a6e606a787aed85a16b276271.jpg-wh_600x-s_1360230890.jpg" target="_blank" rel="noopener"><img src="https://s2.51cto.com/oss/201902/27/82f9be8a6e606a787aed85a16b276271.jpg-wh_600x-s_1360230890.jpg" alt=""></a></p><p><strong>全局唯一 id 如何生成</strong></p><p>在分库分表之后你必然要面对的一个问题，就是 id 咋生成?因为要是一个表分成多个表之后，每个表的 id 都是从 1 开始累加自增长，那肯定不对啊。</p><p>举个例子，你的订单表拆分为了 1024 张订单表，每个表的 id 都从 1 开始累加，这个肯定有问题了!</p><p>你的系统就没办法根据表主键来查询订单了，比如 id = 50 这个订单，在每个表里都有!</p><p>所以此时就需要分布式架构下的全局唯一 id 生成的方案了，在分库分表之后，对于插入数据库中的核心 id，不能直接简单使用表自增 id，要全局生成唯一 id，然后插入各个表中，保证每个表内的某个 id，全局唯一。</p><p>比如说订单表虽然拆分为了 1024 张表，但是 id = 50 这个订单，只会存在于一个表里。</p><p>那么如何实现全局唯一 id 呢?有以下几种方案：</p><p><strong>方案一：独立数据库自增 id</strong></p><p>这个方案就是说你的系统每次要生成一个 id，都是往一个独立库的一个独立表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。</p><p>比如说你有一个 auto_id 库，里面就一个表，叫做 auto_id 表，有一个 id 是自增长的。</p><p>那么你每次要获取一个全局唯一 id，直接往这个表里插入一条记录，获取一个全局唯一 id 即可，然后这个全局唯一 id 就可以插入订单的分库分表中。</p><p>这个方案的好处就是方便简单，谁都会用。缺点就是单库生成自增 id，要是高并发的话，就会有瓶颈的，因为 auto_id 库要是承载个每秒几万并发，肯定是不现实的了。</p><p><strong>方案二：UUID</strong></p><p>这个每个人都应该知道吧，就是用 UUID 生成一个全局唯一的 id。</p><p>好处就是每个系统本地生成，不要基于数据库来了。不好之处就是，UUID 太长了，作为主键性能太差了，不适合用于主键。</p><p>如果你是要随机生成个什么文件名了，编号之类的，你可以用 UUID，但是作为主键是不能用 UUID 的。</p><p><strong>方案三：获取系统当前时间</strong></p><p>这个方案的意思就是获取当前时间作为全局唯一的 id。但是问题是，并发很高的时候，比如一秒并发几千，会有重复的情况，这个肯定是不合适的。</p><p>一般如果用这个方案，是将当前时间跟很多其他的业务字段拼接起来，作为一个 id，如果业务上你觉得可以接受，那么也是可以的。</p><p>你可以将别的业务字段值跟当前时间拼接起来，组成一个全局唯一的编号，比如说订单编号：时间戳 + 用户 id + 业务含义编码。</p><p>方案四：SnowFlake 算法的思想分析</p><p>SnowFlake 算法，是 Twitter 开源的分布式 id 生成算法。其核心思想就是：使用一个 64 bit 的 long 型的数字作为全局唯一 id。</p><p>这 64 个 bit 中，其中 1 个 bit 是不用的，然后用其中的 41 bit 作为毫秒数，用 10 bit 作为工作机器 id，12 bit 作为序列号。</p><p><a href="https://s5.51cto.com/oss/201902/27/a23a094ded06d3ef4f7fc16a0ebdd062.jpg-wh_600x-s_1032626538.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201902/27/a23a094ded06d3ef4f7fc16a0ebdd062.jpg-wh_600x-s_1032626538.jpg" alt=""></a></p><p>给大家举个例子吧，比如下面那个 64 bit 的 long 型数字：</p><ul><li>第一个部分，是 1 个 bit：0，这个是无意义的。</li><li>第二个部分是 41 个 bit：表示的是时间戳。</li><li>第三个部分是 5 个 bit：表示的是机房 id，10001。</li><li>第四个部分是 5 个 bit：表示的是机器 id，1 1001。</li><li>第五个部分是 12 个 bit：表示的序号，就是某个机房某台机器上这一毫秒内同时生成的 id 的序号，0000 00000000。</li></ul><p><strong>①1 bit：是不用的，为啥呢?</strong></p><p>因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。</p><p><strong>②41 bit：表示的是时间戳，单位是毫秒。</strong></p><p>41 bit 可以表示的数字多达 2^41 - 1，也就是可以标识 2 ^ 41 - 1 个毫秒值，换算成年就是表示 69 年的时间。</p><p><strong>③10 bit：记录工作机器 id，代表的是这个服务最多可以部署在 2^10 台机器上，也就是 1024 台机器。</strong></p><p>但是 10 bit 里 5 个 bit 代表机房 id，5 个 bit 代表机器 id。意思就是最多代表 2 ^ 5 个机房(32 个机房)，每个机房里可以代表 2 ^ 5 个机器(32 台机器)。</p><p><strong>④12 bit：这个是用来记录同一个毫秒内产生的不同 id。</strong></p><p>12 bit 可以代表的最大正整数是 2 ^ 12 - 1 = 4096，也就是说可以用这个 12 bit 代表的数字来区分同一个毫秒内的 4096 个不同的 id。</p><p>简单来说，你的某个服务假设要生成一个全局唯一 id，那么就可以发送一个请求给部署了 SnowFlake 算法的系统，由这个 SnowFlake 算法系统来生成唯一 id。</p><p>这个 SnowFlake 算法系统首先肯定是知道自己所在的机房和机器的，比如机房 id = 17，机器 id = 12。</p><p>接着 SnowFlake 算法系统接收到这个请求之后，首先就会用二进制位运算的方式生成一个 64 bit 的 long 型 id，64 个 bit 中的第一个 bit 是无意义的。</p><p>接着 41 个 bit，就可以用当前时间戳(单位到毫秒)，然后接着 5 个 bit 设置上这个机房 id，还有 5 个 bit 设置上机器 id。</p><p>最后再判断一下，当前这台机房的这台机器上这一毫秒内，这是第几个请求，给这次生成 id 的请求累加一个序号，作为最后的 12 个 bit。</p><p>最终一个 64 个 bit 的 id 就出来了，类似于：</p><p><a href="https://s5.51cto.com/oss/201902/27/b6b89e0253f950fe19c2ba05a3d55e59.jpg-wh_600x-s_943757648.jpg" target="_blank" rel="noopener"><img src="https://s5.51cto.com/oss/201902/27/b6b89e0253f950fe19c2ba05a3d55e59.jpg-wh_600x-s_943757648.jpg" alt=""></a></p><p>这个算法可以保证说，一个机房的一台机器上，在同一毫秒内，生成了一个唯一的 id。可能一个毫秒内会生成多个 id，但是有最后 12 个 bit 的序号来区分开来。</p><p>下面我们简单看看这个 SnowFlake 算法的一个代码实现，这就是个示例，大家如果理解了这个意思之后，以后可以自己尝试改造这个算法。</p><p>总之就是用一个 64 bit 的数字中各个 bit 位来设置不同的标志位，区分每一个 id。</p><p>SnowFlake 算法的实现代码如下：</p><ol><li>public class IdWorker { </li><li>private long workerId; // 这个就是代表了机器id </li><li>private long datacenterId; // 这个就是代表了机房id </li><li>private long sequence; // 这个就是代表了一毫秒内生成的多个id的最新序号 </li><li>public IdWorker(long workerId, long datacenterId, long sequence) { </li><li>// sanity check  for workerId </li><li>// 这儿不就检查了一下，要求就是你传递进来的机房id和机器id不能超过32，不能小于0 </li><li><p>if (workerId &gt; maxWorkerId || workerId &lt; 0) { </p></li><li><p>throw new IllegalArgumentException( </p></li><li>String.format(“worker Id can’t be greater than %d or less than 0”,maxWorkerId)); </li><li><p>} </p></li><li><p>if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) { </p></li><li><p>throw new IllegalArgumentException( </p></li><li>String.format(“datacenter Id can’t be greater than %d or less than 0”,maxDatacenterId)); </li><li>} </li><li>this.workerId = workerId; </li><li>this.datacenterId = datacenterId; </li><li>this.sequence = sequence; </li><li>} </li><li>private long twepoch = 1288834974657L; </li><li>private long workerIdBits = 5L; </li><li><p>private long datacenterIdBits = 5L; </p></li><li><p>// 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内 </p></li><li>private long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); </li><li>// 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内 </li><li>private long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); </li><li>private long sequenceBits = 12L; </li><li>private long workerIdShift = sequenceBits; </li><li>private long datacenterIdShift = sequenceBits + workerIdBits; </li><li>private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; </li><li>private long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); </li><li>private long lastTimestamp = -1L; </li><li>public long getWorkerId(){ </li><li>return workerId; </li><li>} </li><li>public long getDatacenterId() { </li><li>return datacenterId; </li><li>} </li><li>public long getTimestamp() { </li><li>return System.currentTimeMillis(); </li><li>} </li><li>// 这个是核心方法，通过调用nextId()方法，让当前这台机器上的snowflake算法程序生成一个全局唯一的id </li><li>public synchronized long nextId() { </li><li>// 这儿就是获取当前时间戳，单位是毫秒 </li><li>long timestamp = timeGen(); </li><li>if (timestamp &lt; lastTimestamp) { </li><li>System.err.printf( </li><li>“clock is moving backwards. Rejecting requests until %d.”, lastTimestamp); </li><li>throw new RuntimeException( </li><li>String.format(“Clock moved backwards. Refusing to generate id for %d milliseconds”, </li><li>lastTimestamp - timestamp)); </li><li><p>} </p></li><li><p>// 下面是说假设在同一个毫秒内，又发送了一个请求生成一个id </p></li><li>// 这个时候就得把seqence序号给递增1，最多就是4096 </li><li><p>if (lastTimestamp == timestamp) { </p></li><li><p>// 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来， </p></li><li>//这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围 </li><li>sequence = (sequence + 1) &amp; sequenceMask; </li><li>if (sequence == 0) { </li><li>timestamp = tilNextMillis(lastTimestamp); </li><li><p>} </p></li><li><p>} else { </p></li><li>sequence = 0; </li><li>} </li><li>// 这儿记录一下最近一次生成id的时间戳，单位是毫秒 </li><li>lastTimestamp = timestamp; </li><li>// 这儿就是最核心的二进制位运算操作，生成一个64bit的id </li><li>// 先将当前时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后12 bit </li><li>// 最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型 </li><li>return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) | </li><li>(datacenterId &lt;&lt; datacenterIdShift) | </li><li>(workerId &lt;&lt; workerIdShift) | sequence; </li><li>} </li><li><p>private long tilNextMillis(long lastTimestamp) { </p></li><li><p>long timestamp = timeGen(); </p></li><li><p>while (timestamp &lt;= lastTimestamp) { </p></li><li>timestamp = timeGen(); </li><li>} </li><li>return  timestamp; </li><li>} </li><li>private long timeGen(){ </li><li>return System.currentTimeMillis(); </li><li>} </li><li>//—————测试————— </li><li><p>public  static void main(String[] args) { </p></li><li><p>IdWorker worker = new IdWorker(1,1,1); </p></li><li><p>for (int i = 0; i &lt; 30; i++) { </p></li><li>System.out.println(worker.nextId()); </li><li>} </li><li>} </li><li>} </li></ol><p>SnowFlake 算法一个小小的改进思路：其实在实际的开发中，这个SnowFlake算法可以做一点点改进。</p><p>因为大家可以考虑一下，我们在生成唯一 id 的时候，一般都需要指定一个表名，比如说订单表的唯一 id。</p><p>所以上面那 64 个 bit 中，代表机房的那 5 个 bit，可以使用业务表名称来替代，比如用 00001 代表的是订单表。</p><p>因为其实很多时候，机房并没有那么多，所以那 5 个 bit 用做机房 id 可能意义不是太大。</p><p>这样就可以做到，SnowFlake 算法系统的每一台机器，对一个业务表，在某一毫秒内，可以生成一个唯一的 id，一毫秒内生成很多 id，用最后 12 个 bit 来区分序号对待。</p><p><strong>读写分离来支撑按需扩容以及性能提升</strong></p><p>这个时候整体效果已经挺不错了，大量分表的策略保证可能未来 10 年，每个表的数据量都不会太大，这可以保证单表内的 SQL 执行效率和性能。</p><p>然后多台数据库的拆分方式，可以保证每台数据库服务器承载一部分的读写请求，降低每台服务器的负载。</p><p>但是此时还有一个问题，假如说每台数据库服务器承载每秒 2000 的请求，然后其中 400 请求是写入，1600 请求是查询。</p><p>也就是说，增删改的 SQL 才占到了 20% 的比例，80% 的请求是查询。此时假如说随着用户量越来越大，又变成每台服务器承载 4000 请求了。</p><p>那么其中 800 请求是写入，3200 请求是查询，如果说你按照目前的情况来扩容，就需要增加一台数据库服务器。</p><p>但是此时可能就会涉及到表的迁移，因为需要迁移一部分表到新的数据库服务器上去，是不是很麻烦?</p><p>其实完全没必要，数据库一般都支持读写分离，也就是做主从架构。</p><p>写入的时候写入主数据库服务器，查询的时候读取从数据库服务器，就可以让一个表的读写请求分开落地到不同的数据库上去执行。</p><p>这样的话，假如写入主库的请求是每秒 400，查询从库的请求是每秒 1600。</p><p>那么图大概如下所示：</p><p><a href="https://s1.51cto.com/oss/201902/27/25a528a9481d132c774be17ac2143833.jpg-wh_600x-s_561997955.jpg" target="_blank" rel="noopener"><img src="https://s1.51cto.com/oss/201902/27/25a528a9481d132c774be17ac2143833.jpg-wh_600x-s_561997955.jpg" alt=""></a></p><p>写入主库的时候，会自动同步数据到从库上去，保证主库和从库数据一致。</p><p>然后查询的时候都是走从库去查询的，这就通过数据库的主从架构实现了读写分离的效果了。</p><p>现在的好处就是，假如说现在主库写请求增加到 800，这个无所谓，不需要扩容。然后从库的读请求增加到了 3200，需要扩容了。</p><p>这时，你直接给主库再挂载一个新的从库就可以了，两个从库，每个从库支撑 1600 的读请求，不需要因为读请求增长来扩容主库。</p><p>实际上线上生产你会发现，读请求的增长速度远远高于写请求，所以读写分离之后，大部分时候就是扩容从库支撑更高的读请求就可以了。</p><p>而且另外一点，对同一个表，如果你既写入数据(涉及加锁)，还从该表查询数据，可能会牵扯到锁冲突等问题，无论是写性能还是读性能，都会有影响。</p><p>所以一旦读写分离之后，对主库的表就仅仅是写入，没任何查询会影响他，对从库的表就仅仅是查询。</p><p><strong>高并发下的数据库架构设计总结</strong></p><p>从大的一个简化的角度来说，高并发的场景下，数据库层面的架构肯定是需要经过精心的设计的。</p><p>尤其是涉及到分库来支撑高并发的请求，大量分表保证每个表的数据量别太大，读写分离实现主库和从库按需扩容以及性能保证。</p><p>这篇文章就是从一个大的角度来梳理了一下思路，各位同学可以结合自己公司的业务和项目来考虑自己的系统如何做分库分表。</p><p>另外就是，具体的分库分表落地的时候，需要借助数据库中间件来实现分库分表和读写分离，大家可以自己参考 Sharding-JDBC 或者 MyCAT 的官网即可，里面的文档都有详细的使用描述。</p><p>中华石杉：十余年 BAT 架构经验，一线互联网公司技术总监。带领上百人团队开发过多个亿级流量高并发系统。现将多年工作中积累下的研究手稿、经验总结整理成文，倾囊相授。微信公众号：石杉的架构笔记(ID：shishan100)。</p><p>原文地址：<a href="http://developer.51cto.com/art/201902/592573.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201902/592573.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>边缘计算急需解决的难题</title>
      <link href="/2019/01/28/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%80%A5%E9%9C%80%E8%A7%A3%E5%86%B3%E7%9A%84%E9%9A%BE%E9%A2%98/"/>
      <url>/2019/01/28/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%80%A5%E9%9C%80%E8%A7%A3%E5%86%B3%E7%9A%84%E9%9A%BE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>目前边缘计算已经得到了各行各业的广泛重视，并且在很多应用场景下开花结果。根据边缘计算领域特定的特点，本文认为6个方向是未来几年迫切需要解决的问题：编程模型、软硬件选型、基准程序与标准、动态调度、与垂直行业的紧密结合以及边缘节点的落地。<br><a id="more"></a><br><a href="http://s2.51cto.com/oss/201901/27/00e2959e3ecbc30d2201615b2a714d7a.jpg-wh_651x-s_3479503204.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/27/00e2959e3ecbc30d2201615b2a714d7a.jpg-wh_651x-s_3479503204.jpg" alt="边缘计算" title="边缘计算"></a></p><p><strong>1. 编程模型</strong></p><p>编程模型可以使开发者快速上手开发应用产品，从而快速推动领域的发展.在云计算场景中，用户程序在目标平台上编写和编译，然后运行到云服务器，基础设施对于用户是透明的，例如亚马逊基于此编程模型推出的 Lambda 计算服务，可使用户无需预配置或者管理服务器即可运行代码，极大地方便了用户的使用。然而，边缘计算模型与云计算模型存在较大的区别，从功能角度讲，边缘计算是一种分布式的计算系统，具有弹性管理、协同执行和环境异构的特点，如图4所示：</p><p><a href="http://s1.51cto.com/oss/201901/27/a74bb2cf628578cf0efb846b85f97984.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/27/a74bb2cf628578cf0efb846b85f97984.jpg" alt="边缘计算" title="边缘计算"></a></p><p>从图4可知，边缘计算包含3个关键内容：</p><ul><li>应用程序/服务功能可分割。边缘计算中的一个任务可以迁移到不同的边缘设备去执行，任务可分割包括仅能分割其自身或将一个任务分割成子任务，任务的执行需要满足可迁移性，即任务可迁移是实现在边 缘设备上进行数据处理的必要条件。</li><li>数据可分布。数据可分布既是边缘计算的特征也是边缘计算模型对待处理数据集合的要求。边缘数据的可分布性是针对不同数据源而言的，不同数据源来源于数据生产者所产生的大量数据。</li><li>资源可分布。边缘计算模型中的数据具有一定的可分布性，从而要求处理数据所需要的计算、存储和通信资源也具有可分布性.只有当边缘计算系统具备数据处理和计算所需要的资源，边缘设备才可以对数据进行处理。</li></ul><p>因此，传统的编程模型并不适合边缘计算。边缘计算中的设备大多是异构计算平台，每个设备上的运行时环境、数据也不相同，且边缘设备的资源相对受限，在边缘计算场景下部署用户应用程序会有较大的困难。Li等人针对边缘设备资源受限的特性设计了一种轻量级的编程语言EveryLite，该工作将计算迁移任务中主体为接口调用的、时间和空间复杂度受限的计算任务称为微任务(micro task)， EveryLite能够在物端设备上处理边缘计算场景中微任务，经过实验对比可以发现EveryLite的执行时间分别比JerryScript和Lua低77%和74%，编译后内存占用量分别是JerryScript和Lua的18. 9% 和1. 4%。因此，针对边缘计算场景下的编程模型的研究具有非常大的空间，也十分紧迫。</p><p><strong>2. 软硬件选型</strong></p><p>边缘计算系统具有碎片化和异构性的特点。在硬件层面上，有CPU，GPU，FPGA，ASIC等各类计算单元，即便是基于同一类计算单元，也有不同的整机产品，例如基于英伟达GPU的边缘硬件产品，既有计算能力较强的DRIVEPX2，又有计算能力较弱 的Jetson TX2；在软件系统上，针对深度学习应用， 有 TensorFlow, Caffe, PyTorch 等各类框架.不同的软硬件及其组合有各自擅长的应用场景，这带来了一个问题：开发者不知道如何选用合适的软硬件产品以满足自身应用的需求。</p><p>在软硬件选型时，既要对自身应用的计算特性做深人了解，从而找到计算能力满足应用需求的硬件产品，又要找到合适的软件框架进行开发，同时还要考虑到硬件的功耗和成本在可接受范围内。因此，设计并实现一套能够帮助用户对边缘计算平台进行性能、功耗分析并提供软硬件选型参考的工具十分重要。</p><p><strong>3. 基准程序和标准</strong></p><p>随着边缘计算的发展，学术界和工业界开始推出越来越多的针对不同边缘计算场景设计的硬件或软件系统平台，那么我们会面临一个紧迫的问题，即如何对这些系统平台进行全面并公平的评测。传统的计算场景都有经典基准测试集(benchmark)，例如并行计算场景中的PARSEC、高性能计算场景中的 HPCC、大数据计算场景中的BigDataBench。</p><p>由于边缘计算仍然是较新的计算场景，业界仍然没有一个比较权威的用于评测系统性能的Benchmark出现，但是学术界已经开始有了一些探索工作SD-VBS和MEVBench均是针对移动端设备评测基于机器视觉负载的基准测试集。SD-VBS选取了28个机器视觉核心负载，并提供了C和Matlab的实现；MEVBench则提供了一些列特征提取、特征分类、物体检测和物体追踪相关的视觉算法负责，并提供单线程核多线程的C++实现。SLAMBench是一个针对移动端机器人计算系统设计的基准测试集，其使用RG&amp;D SLAM作为评测负载，并且针对不同异构硬件提供C++，OpenMP， OpenCL 和 CUDA 版本的实现。CAVBench是第1个针对智能网联车边缘计算系统设计的基准测试集，其选择6个智能网联车上的典型应用作为评测负责，并提供标准的输人数据集和应用-系统匹配指标。</p><p>由于边缘计算场景覆盖面广，短期来看不会出现一个统一的基准测试集可以适应所有场景下的边缘计算平台，而是针对每一类计算场景会出现一个经典的基准测试集，之后各个基准测试集互相融合借鉴，找出边缘计算场景下的若干类核心负载，最终形成边缘计算场景中的经典基准测试集。</p><p><strong>4. 动态调度</strong></p><p>在云计算场景下，任务调度的一般策略是将计算密集型任务迁移到资源充足的计算节点上执行。但是在边缘计算场景下，边缘设备产生的海量数据无法通过现有的带宽资源传输到云计算中心进行集中式计算，且不同边缘设备的计算、存储能力均不 相同，因此，边缘计算系统需要根据任务类型和边缘设备的计算能力进行动态调度。调度包括2个层面：</p><ul><li>云计算中心和边缘设备之前的调度;</li><li>边缘设备之间的调度。</li></ul><p>云计算中心与边缘设备间的调度分为2种方式：自下而上和自上而下。自下而上是在网络边缘处将边缘设备采集或者产生的数据进行部分或者全部的预处理，过滤无用数据，以此降低传输带宽;自上而下是指将云计算中心所执行的复杂计算任务进行分割，然后分配给边缘设备执行，以此充分利用边缘设备的计算资源，减少整个计算系统的延迟和能耗。2017年，Kang等人设计了一个轻量级的调度器 Neurosurgeon,它可以将深度神经网络不同层的计算任务在移动设备和数据中心间自动分配，使得移动设备功耗最多降低了 94.7%，系统延迟最多加快了40.7倍，并且数据中心的吞吐量最多增加了6. 7倍。边缘设备间也需要动态调度。边缘设备的计算、存储能力本身是不同的，并且会随着时间的变化而变化，而它们承担的任务类型也是不一样的，因此需要动态调度边缘设备上的任务，提高整体系统性能，防止出现计算任务调度到一个系统任务过载情况下的设备.Zhang等人针对延迟敏感性的社会感知任务设计了一个边缘任务调度框架C〇GTA，实验证明该框架可以满足应用和边缘设备的需求。</p><p>综上所述，动态调度的目标是为应用程序调度边缘设备上的计算资源，以实现数据传输开销最小化和应用程序执行性能的最大化。设计调度程序时应该考虑：任务是否可拆分可调度、调度应该采取什么策略、哪些任务需要调度等.动态调度需要在边缘设备能耗、计算延时、传输数据量、带宽等指标之间寻找最优平衡.根据目前的工作，如何设计和实现一种有效降低边缘设备任务执行延迟的动态调度策略是一个急需解决的问题。</p><p><strong>5. 和垂直行业紧密合作</strong></p><p>在云计算场景下，不同行业的用户都可将数据传送至云计算中心，然后交由计算机从业人员进行数据的存储、管理和分析。云计算中心将数据抽象并提供访问接口给用户，这种模式下计算机从业人员与用户行业解耦和，他们更专注数据本身，不需对用户行业领域内知识做太多了解。</p><p>但是在边缘计算的场景下，边缘设备更贴近数据生产者，与垂直行业的关系更为密切，设计与实现边缘计算系统需要大量的领域专业知识。另一方面，垂直行业迫切需要利用边缘计算技术提高自身的竞争力，却面临计算机专业技术不足的问题.因此计算 机从业人员必须与垂直行业紧密合作，才能更好地完成任务，设计出下沉可用的计算系统.在与垂直行业进行合作时，需要着重解决3个问题：</p><ul><li>减少与行业标准间的隔阂。在不同行业内部有经过多年积累的经验与标准，在边缘计算系统的设计中，需要与行业标准靠近，减少隔阂。例如，在针对自动驾驶汽车的研究中，自动驾驶任务的完成需要使用到智能算法、嵌人式操作系统、车载计算硬件等各类计算机领域知识，这对于计算机从业人员而言是一个机遇，因此许多互联网公司投人资源进行研究。然而，若想研制符合行业标准的汽车，仅应用计算机领域知识是完全不够的，还需要对汽车领域专业知识有较好的理解，例如汽车动力系统、控制系统等，这就需要与传统汽车厂商进行紧密合作。同样，在智能制造、工业物联网等领域，同样需要设计下沉到领域内、符合行业标准的边缘计算系统。</li><li>完善数据保护和访问机制。在边缘计算中，需要与行业结合，在实现数据隐私保护的前提下设计统一、易用的数据共享和访问机制.由于不同行业具有的特殊性，许多行业不希望将数据上传至公有云，例如医院、公安机构等。而边缘计算的一大优势是数据存放在靠近数据生产者的边缘设备上，从而保证了数据隐私.但是这也导致了数据存储空间的多样性，不利于数据共享和访问.在传统云计算中，数据传输到云端 ，然后通过统一接口来访问，极大地方便了用户的使用.边缘计算需要借助这种优势来设计数据防护和访问机制。</li><li>提高互操作性。边缘计算系统的设计需要易于结合行业内现有的系统，考虑到行业现状并进行利用，不要与现实脱节。例如在视频监控系统中，除了近些年出现的智能计算功能的摄像头，现实中仍然有大量的非智能摄像头，其每天仍然在采集大量的视频数据，并将数据传输至数据中心。学术界设计了A3系统，它利用了商店或者加油站中已有的计算设备。然而实际情况下，摄像头周边并不存在计算设备。因此，在边缘计算的研究中需要首先考虑如何部署在非智能的摄像头附近部署边缘计算设备. 在目前的解决方案中，多是采用建立更多的数据中心或AI—体机来进行处理，或者采用一些移动的设备，如各种单兵作战设备，来进行数据的采集.前者耗费巨大，且从本质来说，仍然是云计算的模式;后者通常使用于移动情况下，仅作为临时的计算中心，无法和云端进行交互。在视频监控领域，Luo等人提出了一个尚属于前期探讨的EdgeBox方案，其同时具备计算能力和通信能力，可以作为中间件插人到摄像头和数据中心之间，完成数据的预处理. 因此，如何与垂直行业紧密合作，设计出下沉可用的边缘计算系统，实现计算机与不同行业间的双赢是边缘计算面临的一个紧迫问题。</li></ul><p><strong>6. 边缘节点落地问题</strong></p><p>边缘计算的发展引起了工业界的广泛关注，但是在实际边缘节点的落地部署过程中，也涌现出一些急需解决的问题，例如应该如何建立适用于边缘计算的商业模式、如何选择参与计算的边缘节点和边缘计算数据、如何保证边缘节点的可靠性等。</p><p>1)新型商业模式.在云计算场景下，云计算公司是计算服务的提供者，它们收集、存储、管理数据并且负责软硬件、基础设施的建设和维护，用户付费购买服务，不需要关注计算节点本身的成本，也无需关注服务质量的升级换代过程.这种商业模式为用户使用云服务带来了便利，也让云计算公司具备盈利能力，从而更好地提高服务质量。</p><p>而在边缘计算场景下，边缘节点分布在靠近数据生产者的位置，在地理位置上具有较强的离散性，这使得边缘节点的统一性维护变得困难，同时也给软硬件升级带来了难度。例如提供安全服务的摄像头，在使用过程中需要进行软硬件的升级，软件的升级可以通过网络统一进行，而硬件的升级需要亲临现场。依赖于服务提供者去为每一个边缘节点(摄像头)进行硬件的升级和维护会带来巨大的成本开销，而服务的使用者一般不关注也不熟悉硬件设备的维护工作。又如，在CDN服务的应用中，需要考虑 CDN服务器是以家庭为单位还是以园区为单位配置，不同的配置方式会带来成本的变化，也为服务质量的稳定性增加了不确定因素，而维护CDN所需的开销，需要考虑支付者是服务提供者还是使用者。</p><p>因此工业界需要寻求一种或多种新的商业模式来明确边缘计算服务的提供者和使用者各自应该承担什么责任，例如谁来支付边缘节点建立和维护所需的费用、谁来主导软硬件升级的过程等。</p><p>2) 边缘节点的选择。边缘计算是一个连续统，边缘指从数据源到云计算中心路径之间的任意计算和网络资源。(在实际应用中，用户可以选择云到端整个链路上任意的边缘节点来降低延迟和带宽.由于边缘节点的计算能力、网络带宽的差异性，不同边缘节点的选择会导致计算延迟差异很大.现有的基础设施可以用作边缘节点，例如使用手持设备访问进行通信时，首先连接运营商基站，然后访问主干网络。这种以现有基础设施当做边缘节点的方式会加大延迟，如果手持设备能够绕过基站，直接访问主干网络的边缘节点，将会降低延迟.因此，如何选择合适的边缘节点以降低通信延迟和计算开销是一个重要的问题.在此过程中，需要考虑现有的基础设施如何与边缘节点融合，边缘计算技术会不会构建一个新兴的生态环境，给现有的基础设施发生革命性的变化?</p><p>3)边缘数据选择。边缘节点众多，产生的数据数量和类型也众多，这些数据间互有交集，针对一个问题往往有多个可供选择的解决方案。例如在路况实时监控应用中，既可以利用车上摄像头获得数据，也可以利用交通信号灯的实时数据统计，还可以利用路边计算单元进行车速计算。因此如何为特定应用合理地选择不同数据源的数据，以最大程度地降低延迟和带宽，提高服务的可用性是一个重要问题。</p><p>4)边缘节点的可靠性。边缘计算中的数据存储 和计算任务大多数依赖于边缘节点，不像云计算中心有稳定的基础设施保护，许多边缘节点暴露于自 然环境下，保证边缘节点的可靠性非常重要.例如， 基于计算机视觉的公共安全解决方案需要依赖智能摄像头进行存储和计算，然而在极端天气条件下，摄像头容易在物理上收到损害，例如暴风天气会改变摄像头的角度，暴雪天气会影响摄像头的视觉范围， 在此类场景中，需要借助基础设施的配合来保证边缘节点的物理可靠性。同时，边缘数据有时空特性，从而导致数据有较强的唯一性和不可恢复性，需要设计合理的多重备份机制来保证边缘节点的数据可靠性.因此，如何借助基础设施来保障边缘计算节点的物理可靠性和数据可靠性是一个重要的研究课题。</p><p>在边缘节点落地过程中，已经有了不少尝试，例如联通提出了建设边缘云，其规划至2020年建设6000~7000个边缘节点，将高带宽、低时延、本地化业务下沉到网络边缘，进一步提高网络效率、增强服务能力。因此针对如何选择边缘节点，处理好边缘节点与现有基础设施的关系，保证边缘节点的可靠性的研究非常紧迫。</p><p>原文地址：<a href="http://network.51cto.com/art/201901/591164.htm" target="_blank" rel="noopener">http://network.51cto.com/art/201901/591164.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 物联网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据工程师必备的学习资源（附链接）</title>
      <link href="/2019/01/28/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E5%A4%87%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%EF%BC%88%E9%99%84%E9%93%BE%E6%8E%A5%EF%BC%89/"/>
      <url>/2019/01/28/%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E5%A4%87%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%EF%BC%88%E9%99%84%E9%93%BE%E6%8E%A5%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>在建立模型之前，在数据经过清洗用于探索分析之前，甚至在数据科学家工作开始之前，数据工程师就已经闪亮登场了。每一个数据驱动的业务都需要一个适用于数据科学管道的框架，否则就是失败的配置。</p><p>大多数人怀揣着成为数据科学家的梦想进入数据科学世界，但却没有意识到数据工程师是做什么的，或者这个角色需要具备什么能力。数据工程师是数据科学项目的重要组成部分，以至于在当今数据丰富的环境里，产业对他们的需求正在指数式地上涨。<br><a id="more"></a><br><a href="http://s1.51cto.com/oss/201901/27/64137ddbb08fa2f300192510fc1a23af.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/27/64137ddbb08fa2f300192510fc1a23af.jpg" alt="数据工程师" title="数据工程师"></a></p><p>目前，没有统一的或者正式的学习路线可供数据工程师使用。大多数担任这个角色的人是通过在工作中学习的，而不是遵循一个详细的学习路线。我写这篇文章的目的是帮助那些想成为数据工程师，但却不知道从哪里开始以及从哪里找到学习资源的人。</p><p>本文中，我列出了所有有抱负的数据工程师需要知道的事情。首先，我们将了解什么是数据工程师，以及该角色和数据科学家的区别，然后将继续讨论你的技能宝箱中应该有的核心技能，以便完全胜任这个工作，最后我还提到了一些应该考虑的行业认可证书。</p><p>好了，让我们直接开始吧!</p><p><strong>一、什么是数据工程师</strong></p><p>数据工程师负责构建和维护数据科学项目的数据架构，他们必须确保服务器和应用程序之间的数据流是连续的。改进数据基础应用程序，将新的数据管理技术和软件集成到现有系统中，构建数据收集管道及其他各种各样的事情，都属于数据工程师的职责。</p><p>数据工程中最受欢迎的技能之一是设计和构建数据仓库的能力。数据仓库是收集、存储和检索所有原始数据的地方，如果没有数据仓库，一个数据科学家做的所有任务就会变得要么太昂贵，要么太大，以至于无法拓展。</p><p>ETL(提取、转换和载入)是数据工程师构建数据管道所遵循的步骤，它实际上是一份关于如何处理、转换收集来的原始数据以备分析的蓝图。</p><p>数据工程师通常有着工程背景，与数据科学家不同的是，这个角色不需要太多的学术和科学知识。因此，对构建大规模结构和体系结构的开发人员或工程师非常适合这个角色。</p><p><strong>二、数据科学家和数据工程师之间的区别</strong></p><p><a href="http://s4.51cto.com/oss/201901/27/2ef22aa8627281c47a9efd25c610d5ce.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201901/27/2ef22aa8627281c47a9efd25c610d5ce.jpg" alt="数据工程师" title="数据工程师"></a></p><p>了解这两种角色之间的区别非常重要。从广义上讲，数据科学家综合使用统计学、数学、机器学习和行业知识来构建模型。他/她必须使用组织支持的相同工具/语言和框架来编码和构建这些模型。而数据工程师必须构建并维护适用于数据收集、处理和部署数据密集型应用的数据结构和体系架构。构建数据收集和存储管道，将数据汇总给数据科学家，从而将模型投入生产-这些只是数据工程师必须执行的任务中的一部分。</p><p>要使任何大规模数据科学项目取得成功，数据科学家和数据工程师需要携手合作，否则事情很快就会出错。</p><p>要了解有关这两个角色之间差异的更多信息，请访问我们的详细信息图。</p><p>详细信息图：<a href="https://www.analyticsvidhya.com/blog/2015/10/job-comparison-data-scientist-data-engineer-statistician/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2015/10/job-comparison-data-scientist-data-engineer-statistician/</a></p><p><strong>三、与数据工程相关的不同角色</strong></p><ul><li>数据架构师：数据架构师为数据管理系统收集、整合和维护所有的数据源奠定基础，这个角色需要了解SQL、XML、Hive、Pig、Spark等工具。</li><li>数据库管理员：顾名思义，担任此角色的人需要对数据库有着广泛的了解。职责包括确保数据库对所有需要的用户可用，适当地维护数据库，并且保证在添加新特性时没有任何中断。</li><li>数据工程师：精通以上众多技巧的人。正如我们所见，数据工程师需要掌握数据库工具、Python和Java语言、分布式系统(如Hadoop)等知识，这个角色负责多种组合任务。</li></ul><p><strong>四、数据工程认证</strong></p><p><strong>1. 谷歌认证专家</strong></p><p><a href="http://s5.51cto.com/oss/201901/27/33eb02d212d42967a8d374ec1df6da49.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/27/33eb02d212d42967a8d374ec1df6da49.jpg" alt="谷歌认证专家" title="谷歌认证专家"></a></p><p>这是目前最重要的数据工程认证之一。要获得此证书，你需要成功地通过一个具有挑战性的、2个小时多的考试，题型是多项选择题。你可以在这个网页上找到考试内容的大体范围，此外，这个网页提供给了一些实际操作谷歌云技术的实践指南。请一定要看一下!</p><p>谷歌认证专家：<a href="https://cloud.google.com/certification/data-engineer" target="_blank" rel="noopener">https://cloud.google.com/certification/data-engineer</a></p><p><strong>2. IBM认证数据工程师</strong></p><p><a href="http://s1.51cto.com/oss/201901/27/0b2ade395d2be00cd328666ce24aca31.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/27/0b2ade395d2be00cd328666ce24aca31.jpg" alt=" IBM认证数据工程师" title=" IBM认证数据工程师"></a></p><p>要获得证书，你需要通过这个考试。考试包含54个问题，你必须正确回答44个。我建议在考试前，先了解IBM希望你了解的内容。“考试”链接中还提供了学习资料的进一步链接，你可以参考这些资料进行准备。</p><ul><li>IBM认证数据工程师：<a href="https://www.ibm.com/certify/cert?id=50001501" target="_blank" rel="noopener">https://www.ibm.com/certify/cert?id=50001501</a></li><li>考试：<a href="https://www.ibm.com/certify/exam?id=C2090-101" target="_blank" rel="noopener">https://www.ibm.com/certify/exam?id=C2090-101</a></li></ul><p><strong>3. Cloudera的CCP数据工程师</strong></p><p><a href="http://s1.51cto.com/oss/201901/27/f5f01089fbc3e177f873ef7ee6b132fe.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/27/f5f01089fbc3e177f873ef7ee6b132fe.jpg" alt="数据工程师" title="数据工程师"></a></p><p>这是另一个全球公认的认证，对新手来说是一个相当具有挑战性的认证。你的概念需要更新和深入，你应该有一些使用数据工程工具的实践经验，如Hadoop，Oozie，AWS Sandbox等。但是，如果你通过这次考试，对于你获得开启数据工程领域工作来说，会是一个充满希望的开始!</p><p>Cloudera曾提到，如果你参加他们的Apache Spark和Hadoop培训课程，这将有助于你通过考试，原因是考试主要基于这两个工具。</p><ul><li>Cloudera的CCP数据工程师：<a href="https://www.cloudera.com/more/training/certification/ccp-data-engineer.html" target="_blank" rel="noopener">https://www.cloudera.com/more/training/certification/ccp-data-engineer.html</a></li><li>Apache Spark和Hadoop培训课程：<a href="https://www.cloudera.com/more/training/courses/developer-training-for-spark-and-hadoop.html" target="_blank" rel="noopener">https://www.cloudera.com/more/training/courses/developer-training-for-spark-and-hadoop.html</a></li></ul><p><strong>五、数据工程核心技能及其学习资源</strong></p><p><strong>1. 数据工程简介</strong></p><p><a href="http://s2.51cto.com/oss/201901/27/62379a197113a3b017ed50c4a684b717.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/27/62379a197113a3b017ed50c4a684b717.jpg" alt=""></a></p><p>在深入了解角色之间的不同方面之前，首先得了解数据工程的实质是什么。数据工程每天执行的不同工作是什么?顶尖技术公司想要怎样的数据工程师?你是应该了解可见的所有一切，还是仅仅了解与某一特定角色相关的东西?我的目的是提供以下参考资料，以助你找到这些问题或者其余更多问题的答案。</p><ul><li>《数据工程入门指南》(第1部分)：这是一篇非常受欢迎的、有关数据工程的文章，出自爱彼迎(Airbnb)的一位数据科学家之手。作者首先解释了为什么数据工程是所有机器学习项目中如此关键的一方面，然后深入探讨了本主题的每个部分。我认为这是所有想要成为数据工程师、数据科学家的新手们必读的一篇文章。</li><li>《数据工程入门指南》(第1部分)：<a href="https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7" target="_blank" rel="noopener">https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7</a></li><li>《数据工程入门指南》(第2部分)：接着上面的文章，第2部分将介绍数据建模、数据分区、Airflow和ETL的最佳实践。</li><li>《数据工程入门指南》(第2部分)：<a href="https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-ii-47c4e7cbda71" target="_blank" rel="noopener">https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-ii-47c4e7cbda71</a></li><li>《数据工程入门指南》(第3部分)：这是入门指南系列中的最后一部分，本部分将介绍数据工程框架的概念。在整个系列中，作者不断将理论与Airbnb的实践相结合，从而写了一篇篇精妙绝伦的文章，而且还在持续更新中。</li><li>《数据工程入门指南》(第3部分)：<a href="https://medium.com/@rchang/a-beginners-guide-to-data-engineering-the-series-finale-2cc92ff14b0" target="_blank" rel="noopener">https://medium.com/@rchang/a-beginners-guide-to-data-engineering-the-series-finale-2cc92ff14b0</a></li><li>O’Reilly的免费数据工程电子书套件：O’Reilly以其优秀的图书而出名，这一系列也不例外。不过，这些书是免费的!向下滚动到“大数据架构”部分，查看那里的书籍。有些书籍需要有大数据基础设施的基本知识，但这些书将有助于你熟悉复杂的数据工程任务。</li><li>O’Reilly的免费数据工程电子书套件：<a href="https://www.oreilly.com/data/free/" target="_blank" rel="noopener">https://www.oreilly.com/data/free/</a></li></ul><p><strong>2. 基本语言要求：Python</strong></p><p><a href="http://s1.51cto.com/oss/201901/27/b186b3b62a9e15b8913118b0194d06a2.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/27/b186b3b62a9e15b8913118b0194d06a2.jpg" alt="Python" title="Python"></a></p><p>虽然还有其他的数据工程专用编程语言(如JAVA和Scala)，但我们本文将只关注Python。我们看到业界已经明显转向使用Python，而且使用率正在快速上升。它已经成为数据工程师(和数据科学家)技能的重要组成部分。</p><p>网络上有大量的学习Python资源，我在下面提到了其中的一些。</p><ul><li>在Scratch平台上使用Python学习数据科学的完整教程：KunalJain的这篇文章涵盖了一系列可以用来开始学习和提升Python的资源，这是必读的资源。</li><li>在Scratch平台上使用Python学习数据科学的完整教程：<a href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/</a></li><li>使用Python的数据科学导论：这是Analytics Vidhya上最受欢迎的课程，涵盖了Python的基本知识。我们还额外介绍了核心统计概念和预测建模方法，以巩固你对python和数据科学基础的理解。</li><li>使用Python的数据科学导论：<a href="https://trainings.analyticsvidhya.com/courses/coursev1:AnalyticsVidhya+DS101+2018T2/about" target="_blank" rel="noopener">https://trainings.analyticsvidhya.com/courses/coursev1:AnalyticsVidhya+DS101+2018T2/about</a></li><li>Codeacademy上学习Python课程：本课程不需要编程基础，绝对是从python的最基础开始，这是一个很好的起点。</li><li>Codeacademy上学习Python课程：<a href="https://www.codecademy.com/learn/learn-python" target="_blank" rel="noopener">https://www.codecademy.com/learn/learn-python</a></li></ul><p>如果你喜欢通过书本来学习，下面是一些免费的电子书，便于你开始学习：</p><ul><li>Allen Downey的《思考Python》：全面深入地介绍了Python语言，非常适合新手，甚至非程序员。</li><li>Allen Downey的《思考Python》：<a href="http://www.greenteapress.com/thinkpython/thinkpython.pdf" target="_blank" rel="noopener">http://www.greenteapress.com/thinkpython/thinkpython.pdf</a></li><li>Python 3的非程序员教程：顾名思义，它是非IT背景和非技术背景新手们的完美起点，每章都有大量的示例来测试你的知识。</li><li>Python 3的非程序员教程：<a href="https://upload.wikimedia.org/wikipedia/commons/1/1d/Non-Programmer%27s_Tutorial_for_Python_3.pdf" target="_blank" rel="noopener">https://upload.wikimedia.org/wikipedia/commons/1/1d/Non-Programmer%27s_Tutorial_for_Python_3.pdf</a></li></ul><p><strong>3. 扎实的操作系统知识</strong></p><p><a href="http://s2.51cto.com/oss/201901/27/efe9ba47ff7e715bbe56e12427ef9ae4.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/27/efe9ba47ff7e715bbe56e12427ef9ae4.jpg" alt="操作系统知识" title="操作系统知识"></a></p><p>在整个数据科学世界的“机器”中，操作系统是使管道运转起来的重要“齿轮”。数据工程师应该了解基础设施组件(如虚拟机、网络、应用程序服务等)的输入和输出。你对服务器管理有多精通?你对Linux是否有足够的了解，可以浏览不同的配置吗?你对访问控制方法有多熟悉?作为一名数据工程师，这些只是你将面临的一些问题。</p><ul><li>Linux服务器管理和安全：本课程是为那些想了解Linux如何在公司应用的人而设计的，课程内容分为4周(最后还有一个项目)，详细介绍了这个主题中的所有基本内容。</li><li>Linux服务器管理和安全：<a href="https://www.coursera.org/learn/linux-server-management-security" target="_blank" rel="noopener">https://www.coursera.org/learn/linux-server-management-security</a></li><li>CS401-操作系统：和其他操作系统课程一样全面，这个课程包含9个部分，专门介绍操作系统的不同方面。主要介绍基于Unix的系统，尽管Windows也包括在内。</li><li>CS401-操作系统：<a href="https://learn.saylor.org/course/cs401" target="_blank" rel="noopener">https://learn.saylor.org/course/cs401</a></li><li>Raspberry Pi平台和Raspberry Pi的python编程：这是一个炙手可热的编程方式，现在对这种编程人员的需求空前高涨。本课程旨在让你熟悉Raspberry Pi环境，并让你开始学习Raspberry PI上的python基本代码。</li><li>Raspberry Pi平台和Raspberry Pi的python编程：<a href="https://www.coursera.org/learn/raspberry-pi-platform" target="_blank" rel="noopener">https://www.coursera.org/learn/raspberry-pi-platform</a></li></ul><p><strong>4. 丰富、深入的数据库知识-SQL和NoSQL</strong></p><p><a href="http://s2.51cto.com/oss/201901/27/ec4285d801f48d7c5041dd3042448cfb.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/27/ec4285d801f48d7c5041dd3042448cfb.jpg" alt=""></a></p><p>为了成为一名数据工程师，你需要熟练掌握数据库语言和工具。这是另一个非常基本的要求，你需要具备实时从数据库收集、存储和查询信息的能力。现今有很多可用的数据库，我已经列出了目前在业界广泛使用的数据库的相关资源，分为SQL和NoSQL两部分。</p><p><strong>(1) SQL数据库</strong></p><p><a href="http://s5.51cto.com/oss/201901/27/e71a0d667d63d9af17e7b2bd947bc88d.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/27/e71a0d667d63d9af17e7b2bd947bc88d.jpg" alt="SQL数据库" title="SQL数据库"></a></p><ul><li>免费学习SQL：这是codecademy另一个课程，你可以在这里学到SQL很基本的知识，像操作、查询、聚合函数这些主题从一开始就涵盖了。如果你是这个领域的新手，没有比这更好的起点了。</li><li>免费学习SQL：<a href="https://www.codecademy.com/learn/learn-sql" target="_blank" rel="noopener">https://www.codecademy.com/learn/learn-sql</a></li><li>快速查找SQL命令的备忘录：一个非常有用的Github存储库，包含定期更新的SQL查询和示例。为了保证你在任何时候都可以快速查找SQL相关命令，请将为这个存储库加入收藏，作为日常参考。</li><li>快速查找SQL命令的备忘录：<a href="https://github.com/enochtangg/quick-SQL-cheatsheet" target="_blank" rel="noopener">https://github.com/enochtangg/quick-SQL-cheatsheet</a></li><li>MYSQL教程：MySQL创建于20多年前，至今仍是业界的热门选择。这个资源是一个基于文本的教程，易于理解。这个站点最酷的是，每个主题都附带实用示例的SQL脚本和屏幕截图。</li><li>MYSQL教程：<a href="http://www.mysqltutorial.org/" target="_blank" rel="noopener">http://www.mysqltutorial.org/</a></li><li>学习Microsoft SQL Server：本教程从基础知识到更高的主题探讨SQL Sever的概念，并以代码和详细的屏幕截图的方式解释了概念。</li><li>学习Microsoft SQL Server：<a href="https://www.tutorialspoint.com/ms_sql_server/" target="_blank" rel="noopener">https://www.tutorialspoint.com/ms_sql_server/</a></li><li>PostgreSQL教程：这是一个让人惊叫的详细指南，让你开始和熟悉PostgreSQL。本教程分为16个部分，因此你完全可以想象出该课程的覆盖面有多广。</li><li>PostgreSQL教程：<a href="http://www.postgresqltutorial.com/" target="_blank" rel="noopener">http://www.postgresqltutorial.com/</a></li><li>Oracle Live SQL：谁能比创建者更好地学习Oracle SQL数据库?这个平台设计得非常好提供了良好的终端用户体验。你可以在这个平台上查看脚本和教程，然后还可以在这里编码。哇，这太棒啦!</li><li>Oracle Live SQL：<a href="https://livesql.oracle.com/apex/f?p=590:1000" target="_blank" rel="noopener">https://livesql.oracle.com/apex/f?p=590:1000</a></li></ul><p><strong>(2) NoSQL数据库</strong></p><p><a href="http://s4.51cto.com/oss/201901/27/ce989cca5c7ed06308156ce4f7055998.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201901/27/ce989cca5c7ed06308156ce4f7055998.jpg" alt="NoSQL数据库" title="NoSQL数据库"></a></p><ul><li>MongoDB来自MongoDB：这是目前最流行的NoSQL数据库，和上面提及的Oracle培训课程一样，学习MongoDB最好的方式是从创建它的大师们那里学习。我在这里链接了他们的整个课程目录，你可以选择你想参加的培训课程。</li><li>MongoDB来自MongoDB：<a href="https://university.mongodb.com/courses/catalog" target="_blank" rel="noopener">https://university.mongodb.com/courses/catalog</a></li><li>MongoDB简介：本课程将帮助你快速启动和运行MongoDB，并教你如何利用它进行数据分析。这是一个为期3周的短课程，但有大量的练习。当你完成的时候，会觉得自己就是一名专家了!</li><li>MongoDB简介：<a href="https://www.coursera.org/learn/introduction-mongodb" target="_blank" rel="noopener">https://www.coursera.org/learn/introduction-mongodb</a></li><li>学习Cassandra：如果你正在寻找一个优秀的、基于文本的、新手易于理解的Cassandra简介，这会是一个完美的资源。像Cassandra的架构、安装、关键操作等主题都会在这里有所介绍，本教程还提供了专门的章节来讲解CQL种可用的数据类型和集合、以及如何使用用户自定义的数据类型。</li><li>学习Cassandra：<a href="https://www.tutorialspoint.com/cassandra/index.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/cassandra/index.htm</a></li><li>Redis Enterprise：了解Redis的资源不多，但这一个站点就足够了。有多个课程和精心设计的视频，使人沉浸其中，乐趣无穷，而且它是免费的!</li><li>Redis Enterprise：<a href="https://university.redislabs.com/" target="_blank" rel="noopener">https://university.redislabs.com/</a></li><li>Google Bigtable：作为Google的产品，学习BigTable工作原理的资源稀缺得让人惊讶，我链接了一个包含大量谷歌云主题的课程，你可以向下滚动，选择BigTable(或BigQuery)。不过，我建议你仔细阅读整个课程，因为它提供了有关谷歌整个云产品如何工作的宝贵见解。</li><li>Google Bigtable：<a href="https://www.coursera.org/learn/gcp-fundamentals" target="_blank" rel="noopener">https://www.coursera.org/learn/gcp-fundamentals</a></li><li>Couchbase：这里提供多种培训课程(向下滚动查看免费培训课程)，从初学者到高级都有。如果Couchbase是你们所用的数据库，那么你将在这里了解有关它的所有信息。</li><li>Couchbase：<a href="http://training.couchbase.com/store" target="_blank" rel="noopener">http://training.couchbase.com/store</a></li></ul><p><strong>5. 数据仓库-Hadoop、MapReduce、Hive、Pig、Apache Spark、Kafka</strong></p><p><a href="http://s3.51cto.com/oss/201901/27/61d619a17b11117f84b327c692d57e18.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/27/61d619a17b11117f84b327c692d57e18.jpg" alt=""></a></p><p>现在，在每一个数据工程师的工作描述中都会看到像Hadoop(HDFS)这样的分布式文件系统。它是所有角色都需要掌握的，你应该非常熟悉。除此之外，你还需要了解ApacheSpark、Hive、Pig、Kafka等平台和框架，我在本节列出了所有这些主题的资源。</p><p><strong>(1) Hadoop和MapReduce</strong></p><p><a href="http://s2.51cto.com/oss/201901/27/fa9c03c285838a943effd09edac657ef.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/27/fa9c03c285838a943effd09edac657ef.jpg" alt=""></a></p><ul><li>Hadoop基础知识：这本质上是Hadoop的学习路径，它包括5门课程，可以让你深入地了解hadoop是什么、定义它的体系结构和组件是什么、如何使用它、它的应用怎么样以及其他更多的内容。</li><li>Hadoop基础知识：<a href="https://cognitiveclass.ai/learn/hadoop/" target="_blank" rel="noopener">https://cognitiveclass.ai/learn/hadoop/</a></li><li>Hadoop入门包：对于想要着手开始学Hadoop的人来说，这是一个非常全面的、优秀的免费课程。它包括HDFS、MapReduce、Pig和Hive之类的主题，可以通过免费访问集群来练习所学的内容。</li><li>Hadoop入门包：<a href="https://www.udemy.com/hadoopstarterkit/" target="_blank" rel="noopener">https://www.udemy.com/hadoopstarterkit/</a></li><li>HortonWorks教程：作为Hadoop的创建者，HortonWorks拥有一套令人万分期待的课程，可以学习与Hadoop相关的各种知识。从低级到高级，本页有着非常全面的教程列表，一定要看一下这个!</li><li>HortonWorks教程：<a href="https://hortonworks.com/tutorials/" target="_blank" rel="noopener">https://hortonworks.com/tutorials/</a></li><li>MapReduce简介：在阅读本文之前，你需要了解Hadoop的基本工作原理。请完成后，再回来深入了解MapReduce的世界。</li><li>MapReduce简介：<a href="https://www.analyticsvidhya.com/blog/2014/05/introduction-mapreduce/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2014/05/introduction-mapreduce/</a></li><li>Hadoop超越了传统的MapReduce-简版：本文介绍了Hadoop生态系统的概述，它超越了简单的MapReduce。</li><li>Hadoop超越了传统的MapReduce-简版：<a href="https://www.analyticsvidhya.com/blog/2014/11/hadoop-mapreduce/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2014/11/hadoop-mapreduce/</a></li></ul><p>更喜欢书吗?别担心，我已经帮你选好了!下面是一些免费电子书，涵盖hadoop和它的组件。</p><ul><li>《Hadoop详解》：简要介绍Hadoop的复杂体系，对Hadoop的工作原理、优势、现实场景中的应用程序等进行了高层次的概述。</li><li>《Hadoop详解》：<a href="https://www.packtpub.com/packt/free-ebook/hadoop-explained" target="_blank" rel="noopener">https://www.packtpub.com/packt/free-ebook/hadoop-explained</a></li><li>《Hadoop-你应该了解的》：这本书和上面的书有相似的内容。正如描述所说，这些书所涵盖的内容足够让你了解Hadoop的方方面面，从而做出明智的决策。</li><li>《Hadoop-你应该了解的》：<a href="https://www.oreilly.com/data/free/hadoop-what-you-need-to-know.csp?intcmp=il-data-free-lp-lgen_free_reports_page" target="_blank" rel="noopener">https://www.oreilly.com/data/free/hadoop-what-you-need-to-know.csp?intcmp=il-data-free-lp-lgen_free_reports_page</a></li><li>《使用MapReduce进行数据密集型文本处理》：这本免费电子书涵盖了MapReduce的基本知识及其算法的设计，然后深入探讨了你应该了解的示例和应用程序。建议你在阅读这本书之前先上上述课程。</li><li>《使用MapReduce进行数据密集型文本处理》：<a href="https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf" target="_blank" rel="noopener">https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf</a></li></ul><p>你应该加入Hadoop LinkedIn小组，以保证自己获取最新的消息，并询问你的任何问题。</p><ul><li>Hadoop LinkedIn小组：<a href="https://www.linkedin.com/groups/988957/profile" target="_blank" rel="noopener">https://www.linkedin.com/groups/988957/profile</a></li></ul><p><strong>(2)</strong> <strong>Apache Spark</strong></p><p><a href="http://s5.51cto.com/oss/201901/27/7fc90552655fa8cef88872fa3fd732d6.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/27/7fc90552655fa8cef88872fa3fd732d6.jpg" alt=""></a></p><p>Apache Spark、RDD和Dataframes(使用PySpark)的综合指南：这是一篇让你开始学习Apache Spark的终极文章，属于必读指南。它介绍了Apache Spark的历史以及如何使用Python、RDD/Dataframes/Datasets安装它，然后通过解决机器学习问题，对自己的知识点进行查漏补缺。</p><p>Apache Spark、RDD和Dataframes(使用PySpark)的综合指南：<a href="https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/</a></p><p>初学者学习Spark R的详细指南：如果你是R的用户，这个就是为你准备的!当然，你可以使用Spark和R，本文可以作为你的指南。</p><p>初学者学习Spark R的详细指南：<a href="https://www.analyticsvidhya.com/blog/2016/06/learning-path-step-step-guide-beginners-learn-sparkr/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2016/06/learning-path-step-step-guide-beginners-learn-sparkr/</a></p><p>Spark的基础知识：本课程涵盖Spark的基础知识、组件、使用方法、使用它的交互式示例和各种Spark库，最后了解Spark集群。你还能从这门课程中要求更多的内容吗?</p><p>Spark的基础知识：<a href="https://cognitiveclass.ai/courses/what-is-spark/" target="_blank" rel="noopener">https://cognitiveclass.ai/courses/what-is-spark/</a></p><p>ApacheSpark和AWS简介：这是一门以实践为中心的课程。你将处理古登堡项目数据，它是世界上最大的电子书开放数据集。你还需要了解Python和Unix命令行，以便从本课程中学到更多。</p><p>ApacheSpark和AWS简介：<a href="https://www.coursera.org/learn/bigdata-cluster-apache-spark-and-aws" target="_blank" rel="noopener">https://www.coursera.org/learn/bigdata-cluster-apache-spark-and-aws</a></p><p><strong>(3)</strong> <strong>涵盖Hadoop、Spark、Hive和Spark SQL的综合教程</strong></p><ul><li>大数据基础知识-HDF、MapReduce和Spark RDD：本课程采用真实的数据来教你基本的大数据技术-HDFS、MapReduce和Spark。这门课程非常详细，示例丰富，数据集实用，而且教师很优秀，属于经典课程。</li><li>大数据基础知识-HDF、MapReduce和Spark RDD：<a href="https://www.coursera.org/learn/big-data-essentials" target="_blank" rel="noopener">https://www.coursera.org/learn/big-data-essentials</a></li><li>大数据分析-Hive、Spark SQL、DataFrames 和GraphFrames：MapReduce和Spark解决了处理大数据的部分问题，通过这门直观的课程你可以掌握这些高级工具，从而掌握有关Hive和Spark SQL等方面的知识。</li><li>大数据分析-Hive、Spark SQL、DataFrames 和GraphFrames：<a href="https://www.coursera.org/learn/big-data-analysis" target="_blank" rel="noopener">https://www.coursera.org/learn/big-data-analysis</a></li><li>大数据应用-实时流：处理大数据的挑战除了要具备处理数据的计算能力，还要具备尽可能快的处理速度。像推荐引擎这样的应用程序需要实时地进行大量数据的处理、存储和查询，这就要求你掌握本课程中所提供的诸如Kafka、Cassandra和Redis等系统的知识。但要学习这门课程，你需要了解Hadoop、Hive、Python、Spark和Spark SQL的应用。</li><li>大数据应用-实时流：<a href="https://www.coursera.org/learn/real-time-streaming-big-data" target="_blank" rel="noopener">https://www.coursera.org/learn/real-time-streaming-big-data</a></li></ul><p><strong>Kafka</strong></p><p><a href="http://s3.51cto.com/oss/201901/27/15bf82b71278d5077adff631dd45d380.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/27/15bf82b71278d5077adff631dd45d380.jpg" alt="Kafka" title="Kafka"></a></p><ul><li>使用Apache Kafka简化数据管道：了解Apache Kafka及其体系架构和使用方法，你需要对Hadoop、Spark和Python有基本的了解，才能真正从本课程中获得最大的收获。</li><li>使用Apache Kafka简化数据管道：<a href="https://cognitiveclass.ai/courses/simplifyingdatapipelines/" target="_blank" rel="noopener">https://cognitiveclass.ai/courses/simplifyingdatapipelines/</a></li><li>Kafka官方文档：这是一个非常直观地介绍Kafka的工作原理及其组件的网页，它还提供了一个关于分布式流媒体平台的解释说明，非常棒!</li><li>Kafka官方文档：<a href="https://kafka.apache.org/intro" target="_blank" rel="noopener">https://kafka.apache.org/intro</a></li><li>用Kafka给数据科学家赋能：这本身不是一个很好的学习资源，而是一篇介绍Stitch Fix的数据工程师如何根据数据科学家的要求构建一个平台的文章，非常有趣，而且十分详细。</li><li>用Kafka给数据科学家赋能：<a href="https://multithreaded.stitchfix.com/blog/2018/09/05/datahighway/" target="_blank" rel="noopener">https://multithreaded.stitchfix.com/blog/2018/09/05/datahighway/</a></li></ul><p><strong>6. 基本的机器学习知识</strong></p><p><a href="http://s3.51cto.com/oss/201901/27/f2e036628d1af3ac1430033b2d23b950.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/27/f2e036628d1af3ac1430033b2d23b950.jpg" alt="机器学习知识" title="机器学习知识"></a></p><p>虽然人们普遍认为机器学习是数据科学家的领域，但数据工程师也需要精通其中的某些技术，原因在于你需要简化将模型投入生产的过程和用于数据收集、生成的管道。因此，你需要对机器学习算法有一个基本的了解。</p><ul><li>学习机器学习基础知识的新手指南：作者Kunal Jain精彩地介绍了机器学习世界，旨在消除你听到或读到地所有行话。指南直截了当地切入问题的核心，最终你会爱上这种写作风格。</li><li>学习机器学习基础知识的新手指南：<a href="https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics/</a></li><li>机器学习算法基本知识：这是一篇优秀的文章，提供了各种对机器学习算法的高层次理解，还提供了在R和python实现这些算法的指南，这是开启你学习旅程的绝佳地点!</li><li>机器学习算法基本知识：<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/</a></li><li>新手必读的机器学习和人工智能书籍：如果你更喜欢看书，那么请阅读本文!这里收藏了最优秀的书，即使你只读了其中的几本，这也会助你朝着梦想中的事业迈进一大步!</li><li>新手必读的机器学习和人工智能书籍：<a href="https://www.analyticsvidhya.com/blog/2018/10/read-books-for-beginners-machine-learning-artificial-intelligence/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2018/10/read-books-for-beginners-machine-learning-artificial-intelligence/</a></li><li>提升你知识和技能的24个终极数据科学项目：一旦你获得了一定量的知识和技能，请一定要把你的理论知识付诸实践。查看这些数据集，按照易到难的顺序，开始处理吧!</li><li>提升你知识和技能的24个终极数据科学项目：<a href="https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/</a></li></ul><p><strong>六、总结</strong></p><p>成为一名数据工程师并不容易，因为你需要从以上所有的资源中获取信息，而且你还要有着将工具、技术和职业道德融为一体的深入理解。由于现在是数据时代，数据工程师在业内需求巨大，对于任何愿意从事这一工作的人来说，这依旧是一个收入可观的职业选择!</p><p>一旦你走上这条路，就力争成为数据工程师吧!请在下面的评论区，告诉我你对这组资源的反馈和建议。</p><p>原文地址：<a href="http://zhuanlan.51cto.com/art/201901/591177.htm" target="_blank" rel="noopener">http://zhuanlan.51cto.com/art/201901/591177.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神奇女侠「下海」拍片？别兴奋，下一个可能就是你「老婆」</title>
      <link href="/2019/01/23/%E7%A5%9E%E5%A5%87%E5%A5%B3%E4%BE%A0%E3%80%8C%E4%B8%8B%E6%B5%B7%E3%80%8D%E6%8B%8D%E7%89%87%EF%BC%9F%E5%88%AB%E5%85%B4%E5%A5%8B%EF%BC%8C%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%83%BD%E5%B0%B1%E6%98%AF%E4%BD%A0%E3%80%8C%E8%80%81%E5%A9%86%E3%80%8D/"/>
      <url>/2019/01/23/%E7%A5%9E%E5%A5%87%E5%A5%B3%E4%BE%A0%E3%80%8C%E4%B8%8B%E6%B5%B7%E3%80%8D%E6%8B%8D%E7%89%87%EF%BC%9F%E5%88%AB%E5%85%B4%E5%A5%8B%EF%BC%8C%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%8F%AF%E8%83%BD%E5%B0%B1%E6%98%AF%E4%BD%A0%E3%80%8C%E8%80%81%E5%A9%86%E3%80%8D/</url>
      
        <content type="html"><![CDATA[<p>《神奇女侠》主演盖尔·加朵「无码三级片」流出，《哈利·波特》赫敏扮演者艾玛·沃特森「床照」曝光，女神寡姐斯嘉丽·约翰逊「下海」拍片……</p><p>乍一看还以为好莱坞「艳照门」曝光了，正当宅男们搬好小板凳准备好好清洁一下屏幕时，可能就会发现有什么不妥。<br><a id="more"></a><br>因为这都是视频通过 AI 技术合成的，国外论坛 Reddit 上一位叫「<a href="https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn" target="_blank" rel="noopener">deepfakes</a>」的用户利用 AI 工具将这些好莱坞女星的脸「换」到了色情影片的演员身上。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/1513018103056-Screen-Shot-2017-12-11-at-120730-PM-1.jpeg!720" alt=""></p><p>（盖尔·加朵的脸被换到一个色情影片的演员上）</p><p>一开始这只是 deepfakes 的「自娱自乐」，可如今事件逐渐开始失控。</p><p>这些 AI 工具被做成了每个人都可以下载安装的应用，于是一大批「名人色情电影」开始在互联网上流传，而一些用户甚至将身边的同学朋友变成了色情电影中的主角。</p><p>还记得前段时间让不少家长瑟瑟发抖是儿童邪典视频吗？这些针对儿童制作的动画或真人短片，充斥着血腥暴力和软色情的内容。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/cr5a077225.jpg!720" alt=""></p><p>Reddit 上的一篇文章曝光了这些儿童邪典视频的幕后制作过程，将此事在互联网上彻底引爆。而这次 Reddit 成了「AI 换脸色情电影」滋长的温床。</p><p>儿童邪典视频偃旗息鼓了，全民 DIY 色情影片的热潮却开始了。</p><p>近几年人工智能大潮方兴未艾，霍金和埃隆·马斯克预言的那个 AI 毁灭人类的世界或许不会到来，但 AI 引起的伦理道德危机和法律问题，已经成了这个互联网世界不得不面对的问题。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/20131111121911748.jpg!720" alt=""></p><h3 id="女神怎么成了色情影片的女主角？"><a href="#女神怎么成了色情影片的女主角？" class="headerlink" title="女神怎么成了色情影片的女主角？"></a>女神怎么成了色情影片的女主角？</h3><p>去年 12 月，一段神奇女侠主演盖尔·加朵的「成人短片」在 Reddit 上出现，引来众多网友围观。</p><p>这段影片的制作者是一个网名为 <a href="https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn" target="_blank" rel="noopener">deepfakes</a>  的程序员，他在业余时间用家里的电脑和开源的 AI 工具，通过机器学习算法，将盖尔·加朵的脸「移植」到一位色情电影演员身上。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-19-02-35.gif" alt=""></p><p>沉迷于此的 deepfakes 无法自拔，更多女明星纷纷中招。除了盖尔·加朵，泰勒·斯威夫特、艾玛·沃特森、斯嘉丽·约翰逊和麦茜·威廉姆斯（《权利的游戏》的 二丫）都是 deepfakes 的得意之作。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/i5a33805f80753612362347.jpg!720" alt=""></p><p>（「中招」的明星们）</p><p>又是程序员，又是机器学习，看起来很有技术含量。实际上并非如此，deepfakes 在接受科技媒体 Motherboard 采访时<a href="https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn" target="_blank" rel="noopener">表示</a>，自己不是专业的 AI 研究人员，只是一个对机器学习感兴趣的程序员。</p><p>而 deepfakes 制作这些视频所用到的工具和素材，都是在互联网免费开放的。他使用的软件技术是基于 TensorFlow 和 Keras 等开源软件，而视频素材则是通过 Google 图片搜索、公开图库和 Youtube 等平台获取。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2017/11/Tensorflow_DevFest2017.jpg!720" alt=""></p><p>deepfakes 利用这些工具和素材进行深度学习网络的训练，经过反复训练后，系统就可以正确识别出盖尔·加朵的面部信息，自动将色情影片中的人脸替换成盖尔·加朵的脸。</p><p>一位人工智能研究人员<a href="https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn" target="_blank" rel="noopener">表示</a>，像 deepfakes 制作的这类视频的所需的硬件配置并不高，只需要一个消费级显卡，花费几小时就能达到这样的效果。</p><p>据 deepfakes 在 Reddit 上的介绍，他使用的算法与英伟达的一种 AI 算法类似，这种算法是生成式对抗网络（Generative Adversarial Network）。</p><p>顾名思义，这是由「敌对」的两个机器学习模型组成。一个负责生成视觉数据，另一个负责鉴别，以此提高生成图像视频的真实性。英伟达利用这种算法让计算机根据夏天的场景视频生成同一地点的冬季风光。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/20171206101538792.jpg!720" alt=""></p><p>不过如果仔细观看 deepfakes 制作的视频，还是会发现很多破绽。比如在模拟盖尔·加朵的这段视频中，可以看到人物脸部轮廓有一条分界线，两边的肤色也不一致。在脸部运动时尤为明显，感更像是戴上了一个逼真的人皮面具。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/0-45.jpeg!720" alt=""></p><p>（左：P 得看不出破绽，右：破绽明显，图自：<a href="https://mp.weixin.qq.com/s/xxoiA9IOo_nZc1xGsuH5dQ" target="_blank" rel="noopener">果壳网</a>）</p><p>可作为一个程序员在业余时间的作品，这样的效果也算不错了。实际上，类似的「换脸」技术也不新鲜，而且在电影业等商用领域，可以实现的效果还要逼真得多。</p><p>你可能还记得《指环王》里的咕噜、《猩球崛起》里表情丰富的凯撒，甚至是郭敬明那部《爵迹》里众多的面瘫角色，这些形象都是通过对肢体动作和面部表情的<a href="https://www.ifanr.com/909844" target="_blank" rel="noopener">动作捕捉</a>实现的。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2017/09/0-2.gif" alt=""></p><p>（《指环王》中咕噜扮演者，当时的造型是这样的，图自：naturevideo）</p><p>这样的技术在影视工业领域的应用已经十分成熟，但一般成本较高，耗费时间长。比如面部捕捉需要在演员脸上贴满追踪点，用头戴式高速摄影机捕捉面部肌肉动作，最后在后期转为 CG 画面。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2017/09/0-3.gif" alt=""></p><p>此外通过 CGI 技术甚至可以在大屏幕「复活」已经去世的演员，比如在拍摄《速度与激情 7》中意外去世的保罗·沃克就是通过这种技术重现在大屏幕中。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/AD-Paul-Walker-CGI-FF7.jpg!720" alt=""></p><p>近几年随着人工智能技术的发展，这样的面部捕捉技术也不再是电影行业的专利。去年 7 月份，美国华盛顿大学的研究者们就利用人工智能以及数字图像合成技术，生产了一段真假难辨的<a href="https://www.ifanr.com/890236" target="_blank" rel="noopener">奥巴马演讲视频</a>。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2017/08/F2.gif" alt=""></p><p>（你能分辨哪个才是真的奥巴马吗？图片来源：<a href="https://www.youtube.com/" target="_blank" rel="noopener">YouTube</a>）</p><p>而一款基于人工智能技术的实时视频仿真软件 Face2Face 也能做到类似的效果，使用者只需在软件中输入一个人脸说话的录像，就可以通过算法分析生成对应的人脸模型，之后套用该模型就可以生成以假乱真的人物形象。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2017/08/F1.gif" alt=""></p><p>（Face2Face 工作原理，图自：<a href="https://www.youtube.com/" target="_blank" rel="noopener">YouTube</a>）</p><p>Face2Face 开发者 Justus Thies 在当时已经意识到了这项技术的危险性，他在一次接受采访的时候<a href="http://www.businessinsider.com/cgi-ai-fake-news-videos-real-2017-7?utm_source=feedly&amp;utm_medium=referral" target="_blank" rel="noopener">表示</a>：</p><blockquote><p>如果这类视频软件得到广泛应用的话，将会对社会造成剧烈的影响。这也是为什么我们不把软件代码开源的原因之一。如果让不成熟的人接触到这类软件，将会大大提升网络霸凌的等级。</p></blockquote><p>遗憾的是，这个潘多拉魔盒已经被打开了。</p><h3 id="超低门槛-，全民-DIY-色情影片"><a href="#超低门槛-，全民-DIY-色情影片" class="headerlink" title="超低门槛 ，全民 DIY 色情影片"></a>超低门槛 ，全民 DIY 色情影片</h3><p>自从一些媒体报道了 deepfakes 制作的「名人色情影片」后， deepfakes 继续制作着这些视频，并获得了给你更多人的关注，其在 Reddit 上的专栏在不到两个月内就聚集了超过 15000 名用户。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/meitu_1-4-1024x791.jpg!720" alt=""></p><p>其中一名位「deepfakeapp」用户不满足于只是观看 deepfakes 制作的视频，基于 deepfakes 所用的算法，制作了一款简单易用的应用程序 <a href="https://www.reddit.com/r/deepfakes/comments/7ox5vn/fakeapp_a_desktop_tool_for_creating_deepfakes/" target="_blank" rel="noopener">FakeApp</a>，让每个用户都可以利用手中的数据伪造色情影片。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/fakeapp.1250x709.jpg!720" alt=""></p><p>在维基百科上，FakeApp 的词条是这样描述的：</p><blockquote><p>FakeApp 是一款社区开发型的桌面应用程序，无需安装 Python 和 Tensorflow 等编程语言和开源软件库就能运行，不过需要 CUDA 支持的一个高性能的 GPU 才能顺利运行。</p><p>如果用户没有合适的 GPU 也没关系，通过 Google 云服务租用这种 GPU 即可。</p></blockquote><p>deepfakeapp  <a href="https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley" target="_blank" rel="noopener">表示</a>只要只要一两个目标人物的高清视频， 8 到 12 小时可以制作一条自动换脸的色情影片，超低的门槛让一大批用户加入到这种视频的制作和分享中。</p><p>Justus Thies 的预言一语成谶，超低门槛的工具让这项技术走向不可控。</p><p>很快在 Reddit 上就涌现了一大批「名人色情影片」，爱范儿梳理了一下 Reddit 上被用于伪造色情影片的女明星，其中艾玛·沃特森（《哈利波特》）、娜塔莉·波特曼（《这个杀手不太冷》）和斯嘉丽·约翰逊（黑寡妇）是被用到较多的明星。</p><p>而部分视频的仿真度甚至比起 deepfakes 一开始制作的视频还要高，大家可以简单感受一下：</p><p><strong>盖尔·加朵</strong></p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-18-28-51.gif" alt=""></p><p>（伪造视频）</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/56e313ce91058428008b5d85.jpg!720" alt=""></p><p><strong>（</strong>  真人）</p><p><strong>艾玛·沃特森</strong></p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-18-34-37.gif" alt=""></p><p>（伪造视频）</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/celebrity-feminists-16-1024x687.jpg!720" alt=""></p><p>（真人）</p><p><strong>斯嘉丽·约翰逊</strong></p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-18-40-17.gif" alt=""></p><p>（伪造视频）</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Scarlett-Johansson-Widescreen-5-1024x768.jpg!720" alt=""></p><p>（真人）</p><p><strong>娜塔莉·波特曼</strong></p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-18-44-43.gif" alt=""></p><p>（伪造视频）</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/natalie-portman-hd-wallpaper-1024x640.jpg!720" alt=""></p><p>（真人）</p><p>不少「名人色情影片」的制作者还会在 Reddit 上交流制作心得，一位「<a href="https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley" target="_blank" rel="noopener">UnobtrusiveBot</a>」利用 FakeApp 将女星 Jessica Alba（《神奇四侠》）的脸放到了一位色情影片演员上：</p><blockquote><p>超级快！只需要学习如何重新训练模型，大约 5 小时就搞定了。</p></blockquote><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-19-31-53.gif" alt=""></p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/jessica-alba-inc-magazine_42479.jpg!720" alt=""></p><p>（Jessica Alba）</p><p>而这正是 FakeApp 开发者 deepfakeapp 的初衷，他在 Reddit 上  <a href="https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley" target="_blank" rel="noopener">表示</a>：</p><blockquote><p>我之所以开发 FakeApp，就是为了让没有技术背景和编程经验的人都可以用上这种技术。</p></blockquote><p>当然也不是所有用户都那么顺利，比如这位叫「MrDrPresidentNotSure」的用户就翻车了。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Jan-30-2018-19-37-52.gif" alt=""></p><p>这些视频被当成是真的在色情网站上传播，在一个专门泄露名人隐私照和视频的网站 CelebJihad，有人上传了一段艾玛·沃特森洗澡的视频，并声称这是其收藏的独家视频。实际上这段视频就是 deepfakes 发布在 Reddit 上的视频。</p><p>更要命的是，就像直播答题的火爆让答题助手应运而生，一些辅助伪造色情视频制作的 AI 工具也诞生了。</p><p>这些工具可以用于帮助用户找到与目标人物匹配的色情影片演员，比如这款叫「<a href="http://pornstarbyface.com/" target="_blank" rel="noopener">Porn Star By Face</a>」的工具，声称是首个基于深度神经网络的面部识别搜索引擎，只要你上传想要用于伪造色情影片的女明星照片，系统就会自动匹配到相似度最高的色情影片演员。</p><blockquote><p>微信搜索<strong>爱范儿（微信号 ：ifanr）</strong>，后台回复「<strong>色</strong>」，获取这款面部识别在线测试工具，温馨提示：自娱自乐就好了哟。</p></blockquote><p>爱范儿（微信号 ：ifanr）用两位女神的照片测试了一下，发现结果其实也并不理想。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/a-5-985x1024.jpeg!720" alt=""></p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/b-3-1024x1021.jpeg!720" alt=""></p><p>类似这样的平台还有很多，其中一个叫「Face By Porn」的平台表示其数据库中共有 2347 名色情电影演员，是从 Pornhub、YouPorn 等色情网站上抓取的。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/1516987008907-laracroft3.jpeg!720" alt=""></p><p>（左为《古墓丽影》的 Lara Croft，右为成人影片演员）</p><p>视频分析平台 Pulsar 的数字媒体总监 Jay Owens 在接受 Motherboard 采访<a href="https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley" target="_blank" rel="noopener">表示</a>：</p><blockquote><p>这样的视频在社交和文化层面都有比较强的渗透性，基于社交平台的娱乐属性，那些「名人色情影片」拥有了更好的传播效果。</p></blockquote><h3 id="下一个被「不可描述」的可能就是你"><a href="#下一个被「不可描述」的可能就是你" class="headerlink" title="下一个被「不可描述」的可能就是你"></a>下一个被「不可描述」的可能就是你</h3><p>事情发展到这还远没有结束，此前一些视频用的都是公众人物的素材，一些吃瓜群众还看得挺过瘾。</p><p>可当这样的事情发生你自己、亲人和朋友身上时，你还会这样想吗？</p><p>在社交软件 Discord 的一个名为「deepfakes」的聊天小组，一些用户称他们正在利用身边的熟人和朋友的素材制作这种伪色情视频。其中一位用户表示，他利用的素材来自高中女同学的 Instagram 和 Facebook 账号中的公开照片。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-26a.jpg!720" alt=""></p><p>而在 Reddit 上，每天都能看到不少制作这种视频的教材和求教程的用户。而决定将这项技术用在什么人身上，全凭使用者的意愿。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-26.jpg!720" alt=""></p><p>在如今的社交网络，要找一个人的照片资料十分简单。这几乎注定了普通人也会成为这项技术的受害者，甚至会出现一种更为严重的性骚扰行为。</p><p>此前在 Facebook 上已经出现了非常严重的「复仇色情」现象，所谓「复仇色情」，就是某些人在分手后为了报复将前女友裸照上传到网络平台上的行为。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/ChrissyChambers_En_3343155k.jpg!720" alt=""></p><p>目前 Discord 已经<a href="https://www.theverge.com/2018/1/28/16943056/discord-chat-group-fake-celebrity-ai-porn" target="_blank" rel="noopener">关闭</a>「deepfakes」这个聊天小组，可是在只要这种技术还是如此容易获取，这样的视频和对普通人造成的伤害还会互联网的某个角落再次发生。</p><h3 id="「AI-色情」泛滥，难道就不能避免吗？"><a href="#「AI-色情」泛滥，难道就不能避免吗？" class="headerlink" title="「AI 色情」泛滥，难道就不能避免吗？"></a>「AI 色情」泛滥，难道就不能避免吗？</h3><p>看了上面的介绍，你可能会有这样一个疑问：</p><blockquote><p>这样一个侵犯公民人身权利、传播淫秽色情影像的平台和技术，难道不能对其采取法律手段打击吗？</p></blockquote><p>很遗憾，还真未必可以。</p><p>一方面，虽然美国大部分州都将卖淫定位非法行为，但对于成人电影书刊等色情作品的判断标准比较复杂，就算是红果果的爱情动作片也未必会被法院定为色情作品，这与美国宪法第一修正案的言论自由权有关，在这里就先不展开叙述了。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/FBI.jpeg!720" alt=""></p><p>（在一些成人影片开头，还会有来自 FBI 的版权声明）</p><p>另一方面，这样未经同意使用他人照片制作这种视频确实涉嫌侵犯了公民肖像权等权利，但是在美国的法律中还存在很大的灰色空间。</p><p>迈阿密大学法学院教授 Mary Anne Franks 在接受《连线》杂志采访时<a href="https://www.wired.com/story/face-swap-porn-legal-limbo/" target="_blank" rel="noopener">表示</a>，那些被用来制作色情视频的名人或许有机会维权，但对于普通人来说希望渺茫，因为就算是针对「色情报复」的法律也不适用于这种「AI 色情视频」。</p><blockquote><p>这些视频所涉及的技术，反而为他们的制作者提供了巨大的法律保障。</p></blockquote><p>马里兰大学的法学教授 Danielle Citron 也对此略显无奈。打击这种视频传播的责任只能落在了众多互联网平台身上。</p><p><img src="https://s3.ifanr.com/wp-content/uploads/2018/01/porn-1024x674.jpeg!720" alt=""></p><p>（Reddit 中 deepfakes 帖子的 18 岁准入提醒，可是形同虚设）</p><p>更糟糕的是，随着人工智能技术的发展，这项技术将无可逆转地日趋完善，甚至到达机器也无法区分的程度。</p><p>所幸人们已经开始意识到这项技术的危害性，一些平台开始开发对应的屏蔽机制。正如弗吉尼亚大学工程学院应用伦理学名誉教授 Deborah Johnson <a href="https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley" target="_blank" rel="noopener">所说</a>：</p><blockquote><p>这项技术的创新之处在于将 AI 生成的脸做到难辨真假的效果，而这项技术的可怕之处在于社会中每个人都能轻易使用这种技术。</p><p>这样的技术正在让世界变得不稳定，甚至这个社会商业以及信用体系也随之崩塌。</p></blockquote><p>虽然这样的视频在中国还没出现，但如何更好地与新科技相处，是每个时代每个社会都需要思考的问题。</p><p>原文地址：<a href="https://www.ifanr.com/977830" target="_blank" rel="noopener">https://www.ifanr.com/977830</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>别“狂吹”5G了！</title>
      <link href="/2019/01/21/%E5%88%AB%E2%80%9C%E7%8B%82%E5%90%B9%E2%80%9D5G%E4%BA%86%EF%BC%81/"/>
      <url>/2019/01/21/%E5%88%AB%E2%80%9C%E7%8B%82%E5%90%B9%E2%80%9D5G%E4%BA%86%EF%BC%81/</url>
      
        <content type="html"><![CDATA[<p>1月17日，华为创始人兼CEO任正非在接受媒体采访时说：</p><p>“5G实际上被夸大了它的作用，也被更多人夸大了华为公司的成就……实际上现在人类社会对5G还没有这么迫切的需要……不要把5G想象成海浪一样，浪潮来了，财富来了，赶快捞，捞不到就错过了。5G的发展一定是缓慢的。”<br><a id="more"></a><br>此前一段时间内，“5G”是舆论场的“宠儿”，俨然被塑造成了万能的“救世主”形象，似乎5G引领的未来已经触手可及;更有很多人对中国的5G发展高度乐观，“中国是5G领跑者”、“中国主导5G时代”等观点传播甚广。</p><p>而任正非的话让人不得不反思：5G是否只是看上去十分美好?</p><p>同时我们也看到，美国运营商对5G并不感冒，美国总统特朗普的团队甚至考虑由政府主导5G网络建设;而中国方面的运营商似乎也并没有表现出人们想象中那样高的兴趣。</p><p>抽丝剥茧，对于5G，看法不能流于表面。</p><p>5G技术发育到底到了什么水平?实际商用的效果怎样?人们对5G究竟有无迫切的需求?中国在5G领域的实力和地位，是不是真的可言“主导”?热议之下，库叔来冷静地讲一讲。</p><p><strong>1</strong></p><p><strong>现有5G技术没有那么神奇</strong></p><p>当年的一场“联想投票门”，引起舆论的巨大关注，但也造成了很多人的误解。比如把“编码”炒得过热，甚至把LDPC和Polar(5G编码技术)等同于5G标准。实际上，编码只是5G关键技术之一，而且在提升传输效率上还称不上最关键的技术。</p><p>那么，5G关键技术有哪些呢?请看下图。</p><p><a href="http://s1.51cto.com/oss/201901/21/8b67a536aa21edb79384ee4f5b6886f2.jpg-wh_651x-s_2398217532.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/21/8b67a536aa21edb79384ee4f5b6886f2.jpg-wh_651x-s_2398217532.jpg" alt=""></a></p><p>(图为5G关键技术)</p><p>就调制、编码、多址、组网、多天线等方面看，很多是沿用老技术，而一些新技术不成熟、不可用，或者传输增益有限。</p><p>先来看编码，被热炒的LDPC和Polar，对系统效率的提升其实并不明显。据业内人士分析，其主要意义在于实现了一个理论极限，代表了人类对自然界探索的里程碑，而对实用效率的提升意义有限。</p><p>再看多址接入方面，多址是移动通信的核心技术领域，第一代到第四代移动通信(即1G到4G)分别采用了FDMA、TDMA、CDMA和OFDM技术。现在5G多址技术的主流看法是NOMA(国内外设备商的各种五花八门命名的技术都是NOMA的修改版本)。</p><p><strong>新的技术意义何在?</strong></p><p>据业内人士介绍：</p><p>NOMA(非正交多址接入)是NTT Docomo于2014年9月首先倡导的。其思想是发射端不同的用户分配非正交的通信资源。在正交方案当中，如果一块资源平均分配给N个用户，那么受正交性的约束，每个用户只能够分配到1/N的资源。NOMA摆脱了正交的限制，因此每个用户分配到的资源可以大于1/N。在极限情况下，每个用户都可以分配到所有的资源，实现多个用户的资源共享。</p><p>虽然理想很丰满，但经数学证明，NOMA路线的频谱效率增益严格为零。</p><p>类似情况在4G时代也发生过。</p><p>在4G标准制定中，爱立信主推的SC-FDMA作为LTE的上行多址方案，是OFDM的一种变体。爱立信宣称该方案能够降低峰均比，降低对终端功放的要求。然而，之后的研究和实践表明，SC-FDMA所带来的对导频设计的负面影响，甚至超过它的带来的好处，其综合性能还不如OFDMA简单的削波方案。但即便如此，爱立信通过自身影响力，将SC-FDMA纳入了4G通信标准专利。这种做法虽然增加了爱立信公司在4G标准的话语权，能够收到更多专利费，但却拉低了整个系统的运行效率。</p><p>再看多天线技术。5G宣传的是超多天线技术(mass MIMO)。MIMO(多天线技术)是最近20多年的热门议题，确实是有潜力的，是提高通信能力的一个方向。但通过多年研究发现，仍然难以实现从实验室到市场的实用转化，未能实现商业应用。</p><p>至于原因，其实不难理解。据业内人士分析，多天线技术最初从军事雷达领域而来，但转入民用则面临与天空完全不同的复杂地形等环境，难以控制成本就成了其商业转化的命门。</p><p>就组网而言，CoMP相对于4G时代的SFR/MLSFR也几乎是零增益，甚至可说是负增益。</p><p>再说一下前段时间被热炒的时分双工(TDD)，前段时间，有文章称“今天全世界的5G技术都是TDD技术”，然而，这种说法容易引起误会，需要说明。</p><p>5G的蓝图中，用的并不是时分双工，而是全双工。</p><p>4G时代，TDD-LTE采用时分双工，FDD-LTE采用频分双工。全双工简单的说就是集成了时分双工和频分双工的优点，实现鱼和熊掌可以兼得。</p><p>【注：TDD指上下行传输采用同一个信道，主要优势在于收发间不会产生干扰、上下行信道切换灵活等;FDD采用两个独立的信道可以同时进行数据传输，主要优势在通信速度高、抗外部干扰性能更好等。】</p><p>只是，全双工同样停留在实验室，无法商用。因此，仍然只能拿老技术——时分或频分双工凑合着用，而无法取得实质性突破。</p><p>所以，在这些关键技术上，现在所谓的5G的技术升级没有传说中的那么神奇，很多新技术增益有限、或尚不成熟难以实用，部分专利甚至在技术上有“开倒车”之嫌。</p><p>正是因此，有人将现在的5G称之为商用概念，而不是技术迭代，将之称为4.9G。我们当前与真正的“5G”，尚有距离。</p><p><strong>2</strong></p><p><strong>为何运营商不甚热心</strong></p><p>说了这些提升有限、不成熟，但5G的网速是实实在在“肉眼可见”的提高，这是怎么做到的呢?</p><p>方法其实颇为“简单粗暴”，就是扩大占用的频段、加大投资基站的密度、提升芯片数据处理速度等手段。</p><p>以频谱资源来说，5G准备用1个G左右的带宽。要知道，GSM(中国移动的2G网络)整个移动才5M带宽，3G是20M带宽，4G是60M带宽。</p><p>以此前爱立信的极限测试，网速的确惊人，测出高达20Gbps数据传输速率，但用了800M带宽。</p><p>因此，不谈细节，光看网速快慢，意义并不大。当前5G条件下的高网速，很大程度上要拜大带宽频谱所赐。</p><p>这样，问题就来了。低频点频谱非常珍贵，直接划拨800M带宽实在太奢侈。在国外，这样的黄金频率堪称天价，运营商对此必须三思而后行。</p><p>那么，如果现在要部署5G，就必须用高频。</p><p>但高频的覆盖能力差。低频率(2G使用的频率段)的频谱资源衍射能力能够覆盖数平方公里，而高频率(比如Wifi使用的频率段)的频谱资源衍射性通常不会超过一个20平米的房间。也就是说，用黄金频率建1个基站，其覆盖范围可以媲美用高频建N个基站。</p><p>实际上，美国的5G频谱选的就是高频。</p><p>2018年11月15日，美国联邦通讯委员会(FCC)召开美国首次5G频谱拍卖会，开启28GHz毫米波5G频谱(27.5-28.35GHz频段，共计850MHz)拍卖。此前爱立信的测试也是在15Ghz这个点上前后开辟800M的宽度，即14.6到15.4Ghz之间的宽度。这些频段资源超过了过去无线通信已经使用过的频段的总和——当然，过去是在中低频率频段，而5G只能动用高频率频段。</p><p>现在5G使用高频、采用毫米波小基站的发展路线，问题就在覆盖很差，这将使最终覆盖结构非常“感人”。按照现在宣传的传输速率的标准，5G要覆盖目前全球4G覆盖的区域，基站数量至少是4G的5倍，也就是1500万至2000万个5G基站。</p><p><a href="http://s2.51cto.com/oss/201901/21/7207303095242fa034baf62d06504f0b.jpg-wh_600x-s_2256350380.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/21/7207303095242fa034baf62d06504f0b.jpg-wh_600x-s_2256350380.jpg" alt=""></a></p><p>(图为中国电信的5G基站)</p><p>这个建设成本可想而知。这也解释了为什么运营商明知道5G是个“好东西”，却不像大家想象中那样热心。</p><p><strong>3</strong></p><p><strong>缺乏“杀手级”应用</strong></p><p>通信领域的发展，必须充分考虑其用户对技术应用的需求。</p><p>有种说法，声称5G主要不是给人用的，而是给“物”用的，也就是说5G将大大超出过去20年无线通信发展史中以大众公共通信网络为主业的范畴。</p><p>的确，目前全球宣传5G的主线，即“万物相连”或称为“物物连接”。</p><p>不过这里却有一个问题，目前所谓物物连接的很多场景，要么不需要5G，要么不敢信赖公众性的无线通信。</p><p>虽然很多媒体在报道中将物联网和5G“捆绑”在一起。但其实物联网并非必须配合5G应用。因为现有物联网主要是追求长寿命，设置一个物联网节点，肯定不希望1-2天内就去更换一次，且大量应用都是低速、小数据量的通信连接，用2G、3G、4G就行了。</p><p>以目前全世界最大规模的物联网应用案例——ofo自行车智能锁为例。其使用的NB-IOT其实就是华为和中国电信北京公司合作出的简化版、删减版4G，且只需要用很小的通信容量。</p><p>无独有偶，不久前，国内运营商进行了物联网芯片招标，中标斩获大单的物联网芯片用的就是2G。</p><p>所以，物联网，至少当前的物联网，和所谓万物互联，和5G没有必然联系。</p><p>而另一个被热捧的应用方向——无人驾驶，先不提无人驾驶技术本身是否成熟，即便是谷歌搞的无人车，也是“胖终端”的无人驾驶，接受信号而做出反应的过程是放在车上。而把5G和无人驾驶联系在一起的则是“瘦终端”模式，做出决策是在“遥远”的服务器，这种做法是存在极大隐患的。现有条件下，用5G网络来搞无人驾驶，可以说是自找麻烦，人为创造了黑客顷刻间把现代社会彻底打乱的空间，这后果看看《速度与激情8》可知一二。</p><p><a href="http://s5.51cto.com/oss/201901/21/dca8f513d550e119833b0f1981ae007a.png" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/21/dca8f513d550e119833b0f1981ae007a.png" alt=""></a></p><p>(图片来自电影《速度与激情8》片段)</p><p>目前，人流超高密度区的通信需求，是5G可行的潜在应用方向。此前曾出现十一黄金周期间，由于游客过于密集，直接导致杭州西湖附近的4G网络瘫痪的状况。为了应对类似情况，中国移动开始力推4G基站密集组网。如果5G组网的性价比能够优于4G基站密集组网(当然，现在5G在这方面控制成本的难度相当高)，这有可能成为当下5G一个不错的应用方向。</p><p>因此，目前5G的“杀手级”应用仍然在水面之下，需要交给时间去发掘。</p><p><strong>4</strong></p><p><strong>4G+5G</strong></p><p>Wifi传输速度这么高(远远高于5G宣称的速度)，但无线电信运营商仍然能够生存下来，这是为什么呢?</p><p>因为用户对于普遍覆盖(任何地点任何时间都能接入)的网络是刚需，比较而言超高网速反而并非必需。</p><p>用户在离开WIFI的情况下，必须也要有通信网络。也就是说，在任何生活角落，都能保持信息连接，因为在最紧要的时候这个信息连接是能救命的。</p><p>而Wifi或者此前存在的WiMax做不到这一点，其只能覆盖很小的领域，或者试图覆盖整个城市但效果却千疮百孔。WiMax也因此被LTE击败，连带导致押宝WiMax的加拿大北电和摩托罗拉破产。</p><p><a href="http://s2.51cto.com/oss/201901/21/b3c5420bf79ae00e5b345f31b14b059a.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/21/b3c5420bf79ae00e5b345f31b14b059a.jpg" alt=""></a></p><p>(图为2009年1月14日加拿大多伦多北电网络公司办公楼前拍摄的北电网络标识，两天后公司宣告破产)</p><p>然而，按照现在5G毫米波小基站的发展方向，追求的主要是传输速率。原因如我们前面所说，5G需要建设数倍于4G的基站，才能以其现在宣传的标准，要覆盖目前全球4G覆盖的区域，成本极其高昂，使得运营商缺乏足够兴趣。</p><p>因此，如果没有行之有效的商业模式，运营商不具备建设一张普遍覆盖的5G网络的能力，那么这就偏离了用户的刚需。</p><p>综上，从基建成本和实际需求来看，建设覆盖全国的5G网络并不现实，也不必要。那么，5G组网就不能实现了吗?</p><p>并非如此。</p><p>从技术上来说，4G和2G是非常成功的两代技术，而5G处境和3G有些类似。正如过去2G+3G的组网模式，未来5G完全可以采用类似的组网模式。</p><p>由于5G不太可能形成一张普遍覆盖的通信网络，就必须采用4G来完成广域覆盖。事实上，3GPP就搞出了“4G基站为主+5G基站为辅”的组网方式，也就是用4G完成广域覆盖，5G基站建在大城市人流密集区域。这其实也是国外运营商选择的主流组网方式。</p><p>这样一来，既可以大幅减少5G基站高昂的建设成本，又能提升人口密集区域的网速。</p><p><strong>5</strong></p><p><strong>理论创新突破至关重要</strong></p><p>从1G到4G传输能力的提升，既有系统效率的提升因素，也有暴力堆砌的因素——依靠消耗频谱资源、用性能更强的芯片和高额投资建设基站实现。</p><p>暴力提升的层面，前文已经说明。</p><p>系统效率的层面，1G到2G提升较大，但3G的提升就小了很多很多。</p><p>当年高通的宣传，CDMA将提升增益18倍，之后发现没那么高，但大家还认为CDMA比TDMA可以高5-6倍，结果最终发现这个增益只高了10%。</p><p><a href="http://s4.51cto.com/oss/201901/21/8a750e84e5e2ad0f8f04528fc41c07e8.jpg-wh_600x-s_4110713028.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201901/21/8a750e84e5e2ad0f8f04528fc41c07e8.jpg-wh_600x-s_4110713028.jpg" alt=""></a></p><p>(图为美国高通公司，通过CDMA高通可以在全世界“收税”)</p><p>而事实上5G的情况更加极端，对比以往历次通信技术升级，其系统效率的提升不显著，主要是靠暴力堆砌提升性能。</p><p>即便3G的提升有“吹牛”的成分，但CDMA相比TDMA还是有10%的提升。而在4G时代，也有SFR、OFDM、Alamouti等重大改进，其中SFR还是中国工程师原创性技术。</p><p>SFR——软频率复用(SFR，Soft Frequency Reuse)，被外国人用后改了个名字叫ICIC(Inter-cell Interference Coordination)，翻译过来是蜂窝(基站)间互相作用的协调。软频率复用在近十年已经成为无线通信一个新增的大领域，现在运营商都有ICIC接口功能。SFR在实验室条件下性能提升高达100%，即便是商用条件下实战，最初的性能提升也有10%，相当于从TDMA到CDMA的效率提升，进一步的深化的MLSFR可以提升30%。</p><p>相比之下，由于4G充分运用过去几十年的技术储备，在很多方面已经接近现有理论的天花板了，目前的5G在系统效率提升方面空间实在有限。</p><p>要想打破如今的局面，需要的是少一些商业的喧嚣，静下心来，搞理论创新突破。</p><p>因为人类科技的根本性进步，都需要倚赖理论突破，否则便永远只能是量的累积，无法实现技术迭代。</p><p>5G，也是如此，基础理论的突破是必需。</p><p><strong>6</strong></p><p><strong>中国是重要参与者</strong></p><p>目前，5G在媒体宣传和商业运作中，存在一定“神化”的风险，往往会有“5G秒天秒地秒空气”之感;另有一些媒体极力宣称5G是中国主导标准，或5G是中国伟大创新。其实，这并不客观。</p><p>在3G时代之后，全球通信厂商对高通一家独大心有余悸，因而在之后的标准制定中，大家都在玩平衡，实现决不让任何一家公司、任何一个国家一家独大。就5G标准制定而言，中国当然是一个重要参与者，但远远谈不上“主导”。</p><p>有人会用中国的专利数量说事，但实际上，通信标准专利含金量高低相差很大。至少，我们在5G时代，也没能实现高通在3G时代凭借核心专利全球征收“高通税”的能力，甚至仍然需要承担“高通税”。这也从侧面说明，专利数量尚没能给中国带来5G的主导地位。</p><p>中国在通信产业上的优势，是强大的国家决策执行力，能够集中力量，拥有三大运营商这样能够实现“村村通”的国有企业，以及一批优秀的通信设备整机厂和终端整机厂。</p><p><a href="http://s1.51cto.com/oss/201901/21/0d7bdeb73426d3d1e69029396fdbc9d2.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/21/0d7bdeb73426d3d1e69029396fdbc9d2.jpg" alt=""></a></p><p>(图为2018年4月2日印度士兵在中印边境收到的中国联通信号短信，这条被称为“让印军崩溃的中国短信”，展示了中国通信基建的覆盖能力)</p><p>但短板也很明显，那就是核心器件尚受制于人，一旦核心器件被卡脖子，整个通信产业将遭遇重大的挫折和困难。</p><p>因此，当下，动辄“中国主导5G”，“中国5G综合实力最强”，未免有些过于乐观。我们走在前进的道路上，但道阻且长，脚踏实地不懈努力才可能让我们真正实现心中的愿景。</p><p>(本文图片均来自网络)</p><p>原文地址：<a href="http://network.51cto.com/art/201901/590855.htm" target="_blank" rel="noopener">http://network.51cto.com/art/201901/590855.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DBA的大救星：数据库智能运维探索与实践</title>
      <link href="/2019/01/15/DBA%E7%9A%84%E5%A4%A7%E6%95%91%E6%98%9F%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/01/15/DBA%E7%9A%84%E5%A4%A7%E6%95%91%E6%98%9F%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<p>近些年，传统的数据库运维方式已经越来越难于满足业务方对数据库的稳定性、可用性、灵活性的要求。</p><p>随着数据库规模急速扩大，各种 NewSQL 系统上线使用，运维逐渐跟不上业务发展，各种矛盾暴露的更加明显。</p><p>在业务的驱动下，美团 DBA 团队经历了从“人肉”运维到工具化、产品化、自助化、自动化的转型之旅，也开始了智能运维在数据库领域的思考和实践。</p><p>本文介绍了美团整个数据库平台的演进历史，以及当前现状和面临的一些挑战，最后分享从自动化到智能化运维过渡时，所进行的思考、探索与实践。<br><a id="more"></a><br><strong>数据库平台的演变</strong></p><p><a href="http://s1.51cto.com/oss/201901/13/fb4e6cae0e7b0d4c73b981d986f8af5d.jpg-wh_600x-s_1727977242.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/13/fb4e6cae0e7b0d4c73b981d986f8af5d.jpg-wh_600x-s_1727977242.jpg" alt=""></a></p><p>我们数据库平台的演进大概经历了五个大的阶段：</p><ul><li>脚本化</li><li>工具化</li><li>产品化</li><li>自助化</li><li>自动化</li></ul><p><strong>脚本化阶段</strong></p><p>这个阶段，我们人少，集群少，服务流量也比较小，脚本化的模式足以支撑整个服务。</p><p><strong>工具化阶段</strong></p><p>我们把一些脚本包装成工具，围绕 CMDB 管理资产和服务，并完善了监控系统。</p><p>这时，我们的工具箱也逐渐丰富起来，包括 DDL 变更工具、SQL Review 工具、慢查询采集分析工具和备份闪回工具等等。</p><p><strong>产品化阶段</strong></p><p>工具化阶段可能还是单个的工具，但是在完成一些复杂操作时，就需要把这些工具组装起来形成一个产品。</p><p>当然，并不是说这个产品一定要做成 Web 系统的形式，而是工具组装起来形成一套流程之后，就可以保证所有 DBA 的操作行为，对流程的理解以及对线上的影响都是一致的。</p><p>我们会在易用性和安全性层面不断进行打磨。而工具产品化的主要受益者是 DBA，其定位是提升运维服务的效率，减少事故的发生，并方便进行快速统一的迭代。</p><p><strong>自助化阶段（打造私有云平台）</strong></p><p>随着美团业务的高速发展，仅靠十几、二十个 DBA 越来越难以满足业务发展的需要。</p><p>所以我们就把某些日常操作开放授权，让开发人员自助去做，将 DBA 从繁琐的操作中解放出来：</p><ul><li>当时整个平台每天执行 300 多次改表操作。</li><li>自助查询超过 1 万次。</li><li>自助申请账号、授权并调整监控。</li><li>自助定义敏感数据并授权给业务方管理员自助审批和管理。</li><li>自定义业务的高峰和低峰时间段等等。</li><li>自助下载、查询日志等等。</li></ul><p><strong>自动化阶段</strong></p><p>对这个阶段的理解，其实是“仁者见仁，智者见智”。大多数人理解的自动化，只是通过 Web 平台来执行某些操作，但我们认为这只是半自动化，所谓的自动化应该是完全不需要人参与。</p><p>目前，我们很多操作都还处于半自动化阶段，下一个阶段我们需要从半自动过渡到全自动。</p><p>以 MySQL 系统为例，从运维角度看包括主从的高可用、服务过载的自我保护、容量自动诊断与评估以及集群的自动扩缩容等等。</p><p><strong>现状和面临的挑战</strong></p><p>下图是我们平台的现状，以关系数据库 RDS 平台为例，其中集成了很多管理的功能。</p><p>例如主从的高可用、MGW 的管理、DNS 的变更、备份系统、升级流程、流量分配和切换系统、账号管理、数据归档、服务与资产的流转系统等等。</p><p><a href="http://s1.51cto.com/oss/201901/13/7b0df9884080a40c9c011c270862c2b8.jpg-wh_600x-s_2488477583.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/13/7b0df9884080a40c9c011c270862c2b8.jpg-wh_600x-s_2488477583.jpg" alt=""></a></p><p>而且我们按照逻辑对平台设计进行了划分，例如：</p><ul><li>以用户维度划分的 RDS 自助平台，DBA 管理平台和测试环境管理平台。</li><li>以功能维度划分的运维、运营和监控。</li><li>以存储类型为维度划分的关系型数据库 MySQL、分布式 KV 缓存、分布式 KV 存储，以及正在建设中的 NewSQL 数据库平台等等。</li></ul><p>未来，我们希望打造成“MySQL+NoSQL+NewSQL，存储+缓存的一站式服务平台”。</p><p><strong>挑战一：RootCause 定位难</strong></p><p>即便我们打造了一个很强大的平台，但还是发现有很多问题难以搞定。第一个就是故障定位，如果是简单的故障，我们有类似天网、雷达这样的系统去发现和定位。</p><p>但是如果故障发生在数据库内部，那就需要专业的数据库知识，去定位和查明到底是什么原因导致了故障。</p><p><a href="http://s3.51cto.com/oss/201901/13/00ec9f1e3586076d298186206c2d9051.jpg-wh_600x-s_2681802086.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/13/00ec9f1e3586076d298186206c2d9051.jpg-wh_600x-s_2681802086.jpg" alt=""></a></p><p>通常来讲，故障的轨迹是一个链，但也可能是一个“多米诺骨牌”的连环。</p><p>可能因为一些原因导致 SQL 执行变慢，引起连接数的增长，进而导致业务超时，而业务超时又会引发业务不断重试，结果会产生更多的问题。</p><p>当我们收到一个报警时，可能已经过了 30 秒甚至更长时间，DBA 再去查看时，已经错过了最佳的事故处理时机。</p><p>所以，我们要在故障发生之后，制定一些应对策略，例如快速切换主库、自动屏蔽下线问题从库等等。</p><p>除此之外，还有一个比较难的问题，就是如何避免相似的故障再次出现。</p><p><strong>挑战二：人力和发展困境</strong></p><p>第二个挑战是人力和发展的困境，当服务流量成倍增长时，其成本并不是以相同的速度对应增长的。</p><p>当业务逻辑越来越复杂时，每增加一块钱的营收，其后面对应的数据库 QPS 可能是 2 倍甚至 5 倍，业务逻辑越复杂，服务支撑的难度越大。</p><p>另外，传统的关系型数据库在容量、延时、响应时间以及数据量等方面很容易达到瓶颈。</p><p>这就需要我们不断拆分集群，同时开发诉求也多种多样，当我们尝试使用平台化的思想去解决问题时，还要充分思考如何满足研发人员多样化的需求。</p><p><a href="http://s3.51cto.com/oss/201901/13/d878e10091de81caaeb0f3a0d2fdf4d9.jpg-wh_600x-s_3671441702.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/13/d878e10091de81caaeb0f3a0d2fdf4d9.jpg-wh_600x-s_3671441702.jpg" alt=""></a></p><p>人力困境这一问题，从 DBA 的角度来说，时间被严重的碎片化，自身的成长就会遇到瓶颈，比如经常会做一些枯燥的重复操作。</p><p>另外，业务咨询量暴增，尽管我们已经在尝试平台化的方法，但是还是跟不上业务发展的速度。</p><p>还有一个就是专业的 DBA 越来越匮乏，越来越贵，关键是根本招聘不到人手。</p><p><a href="http://s1.51cto.com/oss/201901/13/587e0fffa3d6317650070a105f5dedef.jpg-wh_600x-s_3782184412.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/13/587e0fffa3d6317650070a105f5dedef.jpg-wh_600x-s_3782184412.jpg" alt=""></a></p><p>在这种背景下，我们必须去思考：如何突破困局？如何朝着智能化转型？传统运维苦在哪里？智能化运维又能解决哪些问题？</p><p>总结有如下五点：</p><ul><li>从故障产生的原因来说，传统运维是故障触发，而智能运维是隐患驱动。换句话来说，智能运维不用报警，通过看报表就能知道可能要出事了，能够把故障消灭在“萌芽”阶段。</li><li>传统运维是被动接受，而智能运维是主动出击。但主动出击不一定是通过 DBA 去做，可能是系统或者机器人操作。</li><li>传统运维是由 DBA 发起和解决的，而智能运维是系统发起、RD 自助。</li><li>传统运维属于“人肉救火”，而智能运维属于“智能决策执行”。</li><li>传统运维需要 DBA 亲临事故现场，而智能运维 DBA 只需要“隐身幕后”。</li></ul><p><strong>从自动化到智能化</strong></p><p>那么，如何从半自动化过渡到自动化，进而发展到智能化运维呢？在这个过程中，我们会面临哪些痛点呢?</p><p><a href="http://s2.51cto.com/oss/201901/13/d6a27b37f4dffb3420595f084ad1d9a1.jpg-wh_600x-s_102674583.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/13/d6a27b37f4dffb3420595f084ad1d9a1.jpg-wh_600x-s_102674583.jpg" alt=""></a></p><p>我们的目标是为整个公司的业务系统提供高效、稳定、快速的存储服务，这也是 DBA 存在的价值。</p><p>业务并不关心后面是 MySQL 还是 NoSQL，只关心数据是否没丢，服务是否可用，出了问题之后多长时间能够恢复等等。</p><p>所以我们尽可能做到把这些东西对开发人员透明化，提供稳定高效快速的服务。</p><p>而站在公司的角度，就是在有限的资源下，提升效率，降低成本，尽可能长远地解决问题。</p><p><a href="http://s3.51cto.com/oss/201901/13/455a2f40f9ffecba963812758f484af3.jpg-wh_600x-s_3439408580.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/13/455a2f40f9ffecba963812758f484af3.jpg-wh_600x-s_3439408580.jpg" alt=""></a></p><p>上图是传统运维和智能运维的特点分析，左边属于传统运维，右边属于智能运维。</p><p>传统运维在采集这一块做的不够，所以它没有太多的数据可供参考，其分析和预警能力是比较弱的。</p><p>而智能运维刚好是反过来，重采集，很多功夫都在平时做了，包括分析、预警和执行，智能分析并推送关键报表。</p><p>而我们的目标，是让智能运维中的“报警+分析+执行”的比重占据的越来越少。</p><p><a href="http://s5.51cto.com/oss/201901/13/f399350186f1ca1d44267ae65cadc64a.jpg-wh_600x-s_1229695610.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/13/f399350186f1ca1d44267ae65cadc64a.jpg-wh_600x-s_1229695610.jpg" alt=""></a></p><p>决策执行如何去做呢？我们都知道，预警重要但不紧急，但报警是紧急且重要的，如果你不能够及时去处理的话，事态可能会扩大，甚至会给公司带来直接的经济损失。</p><p>预警通常代表我们已经定位了一个问题，它的决策思路是非常清晰的，可以使用基于规则或 AI 的方式去解决，相对难度更小一些。</p><p>而报警依赖于现场的链路分析，变量多、路径长，所以决策难，间接导致任何决策的风险可能都变大。</p><p>所以说我们的策略就是全面的采集数据，然后增多预警，率先实现预警发现和处理的智能化。</p><p>就像我们既有步枪，也有手枪和刺刀，能远距离解决敌人的，就尽量不要短兵相接、肉搏上阵。</p><p><a href="http://s1.51cto.com/oss/201901/13/343548e379a12b44916dc4faf0d36d46.jpg-wh_600x-s_1131617444.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/13/343548e379a12b44916dc4faf0d36d46.jpg-wh_600x-s_1131617444.jpg" alt=""></a></p><p>数据采集，从数据库角度来说，我们产生的数据分成四块：</p><ul><li>Global Status、Variable</li><li>Processlist、InnoDB Status</li><li>Slow、Error、General Log</li><li>Binlog</li></ul><p>从应用侧来说，包含端到端成功率、响应时间 95 线、99 线、错误日志和吞吐量；从系统层面，支持秒级采样、操作系统各项指标。</p><p>从变更侧来看，包含集群拓扑调整、在线 DDL、DML 变更、DB 平台操作日志和应用端发布记录等等。</p><p><a href="http://s3.51cto.com/oss/201901/13/a69ef03709901fd4d3045e51cc97f5e9.jpg-wh_600x-s_654795094.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/13/a69ef03709901fd4d3045e51cc97f5e9.jpg-wh_600x-s_654795094.jpg" alt=""></a></p><p>数据分析，首先是围绕集群分析，接着是实例、库，最后是表，其中每个对象都可以在多项指标上同比和环比，具体对比项可参考上图。</p><p><a href="http://s1.51cto.com/oss/201901/13/0169e6b971ac6a2407f86994a133d55e.jpg-wh_600x-s_1715975186.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201901/13/0169e6b971ac6a2407f86994a133d55e.jpg-wh_600x-s_1715975186.jpg" alt=""></a></p><p>通过上面的步骤，我们基本可以获得数据库的画像，并且帮助我们从整体上做资源规划和服务治理。</p><p>例如，有些集群实例数特别多且有继续增加的趋势，那么服务器需要 scale up；读增加迅猛，读写比变大，那么应考虑存储 KV 化。</p><p>利用率和分布情况会影响到服务器采购和预算制定；哪几类报警最多，就专项治理，各个击破。</p><p><a href="http://s2.51cto.com/oss/201901/13/6887f6fc307317d4b52697f73c93927a.jpg-wh_600x-s_3520482560.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201901/13/6887f6fc307317d4b52697f73c93927a.jpg-wh_600x-s_3520482560.jpg" alt=""></a></p><p>从局部来说，我们根据分析到的一些数据，可以做一个集群的健康体检，例如数据库的某些指标是否超标、如何做调整等等。</p><p><a href="http://s5.51cto.com/oss/201901/13/82b8b0411983ab5cb233da8cd18abd44.jpg-wh_600x-s_2247135494.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/13/82b8b0411983ab5cb233da8cd18abd44.jpg-wh_600x-s_2247135494.jpg" alt=""></a></p><p>数据库预警，通过分析去发现隐患，把报警转化为预警。上图是我们实际情况下的报警统计分析结果，其中主从延迟占比最大。</p><p>假设 load.1minPerCPU 比较高，我们怎么去解决？那么，可能需要采购 CPU 单核性能更高的机器，而不是采用更多的核心。</p><p>再比如说磁盘空间，当我们发现 3T 的磁盘空间普遍不够时，我们下次可以采购 6T 或更大空间的磁盘。</p><p><a href="http://s5.51cto.com/oss/201901/13/fc2216e9468cd2c63d739f5bb33f48ef.jpg-wh_600x-s_3833552750.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/13/fc2216e9468cd2c63d739f5bb33f48ef.jpg-wh_600x-s_3833552750.jpg" alt=""></a></p><p>针对空间预警问题，什么时候需要拆分集群？MySQL 数据库里，拆分或迁移数据库，花费的时间可能会很久。</p><p>所以需要评估当前集群，按目前的增长速度还能支撑多长时间，进而反推何时要开始拆分、扩容等操作。</p><p><a href="http://s3.51cto.com/oss/201901/13/bff84bc94a2adae04ed438af0092b26b.jpg-wh_600x-s_1000446050.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/13/bff84bc94a2adae04ed438af0092b26b.jpg-wh_600x-s_1000446050.jpg" alt=""></a></p><p>针对慢查询的预警问题，我们会统计红黑榜，上图是统计数据，也有利用率和出轨率的数据。</p><p>假设这是一个金融事业群的数据库，假设有业务需要访问且是直连，那么这时就会产生几个问题：</p><ul><li>有没有数据所有者的授权？</li><li>如果不通过服务化方式或者接口，发生故障时，它可能会导致整个金融的数据库挂掉，如何进行降级？</li></ul><p>所以，我们会去统计出轨率跟慢查询，如果某数据库正被以一种非法的方式访问，那么我们就会扫描出来，再去进行服务治理。</p><p><a href="http://s5.51cto.com/oss/201901/13/14a563f1dd1ec909f8265d758526f016.jpg-wh_600x-s_1561177605.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/13/14a563f1dd1ec909f8265d758526f016.jpg-wh_600x-s_1561177605.jpg" alt=""></a></p><p>从运维的层面来说，我们做了故障快速转移，包括自动生成配置文件，自动判断是否启用监控，切换后自动重写配置，以及从库可自动恢复上线等等。</p><p><a href="http://s4.51cto.com/oss/201901/13/807c3377a9f8b4e4c7fbe1d24a47c85f.jpg-wh_600x-s_1530734197.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201901/13/807c3377a9f8b4e4c7fbe1d24a47c85f.jpg-wh_600x-s_1530734197.jpg" alt=""></a></p><p>报警自动处理，目前来说大部分的处理工作还是基于规则，在大背景下拟定规则。</p><p>触发之后，按照满足的前提条件触发动作，随着库的规则定义的逐渐完善和丰富，可以逐步解决很多简单的问题，这部分就不再需要人的参与。</p><p><a href="http://s5.51cto.com/oss/201901/13/c052c1a375341a08f37e91f2f28f43f1.jpg-wh_600x-s_3778992075.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/13/c052c1a375341a08f37e91f2f28f43f1.jpg-wh_600x-s_3778992075.jpg" alt=""></a></p><p><strong>展望</strong></p><p>未来我们还会做一个故障诊断平台，类似于“扁鹊”，实现日志的采集、入库和分析，同时提供接口，供全链路的故障定位和分析、服务化治理。</p><p>展望智能运维，应该是在自动化和智能化上交叠演进，在 ABC（AI、Big Data、Cloud Computing）三个方向上深入融合。</p><p>在数据库领域，NoSQL 和 SQL 界限正变得模糊，软硬结合、存储计算分离架构也被越来越多的应用，智能运维正当其时，我们也面临更多新的挑战。</p><p>我们的目标是，希望通过 DB 平台的不断建设加固，平台能自己发现问题，自动定位问题，并智能的解决问题。</p><p>作者：赵应钢</p><p>简介：美团研究员，数据库专家。曾就职于百度、新浪、去哪儿网等，10 年数据库自动化运维开发、数据库性能优化、大规模数据库集群技术保障和架构优化经验。精通主流的 SQL 与 NoSQL 系统，现专注于公司业务在 NewSQL 领域的创新和落地。</p><p>原文地址：<a href="http://database.51cto.com/art/201901/590485.htm" target="_blank" rel="noopener">http://database.51cto.com/art/201901/590485.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一份超详细的MySQL高性能优化实战总结</title>
      <link href="/2019/01/15/%E4%B8%80%E4%BB%BD%E8%B6%85%E8%AF%A6%E7%BB%86%E7%9A%84MySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93/"/>
      <url>/2019/01/15/%E4%B8%80%E4%BB%BD%E8%B6%85%E8%AF%A6%E7%BB%86%E7%9A%84MySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>在进行 MySQL 的优化之前必须要了解的就是 MySQL 的查询过程，很多的查询优化工作实际上就是遵循一些原则让 MySQL 的优化器能够按照预想的合理方式运行而已。</p><p><a href="http://s5.51cto.com/oss/201901/15/cb129fc0ac43fb40e0e1b05072673115.jpg-wh_600x-s_3458444518.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/15/cb129fc0ac43fb40e0e1b05072673115.jpg-wh_600x-s_3458444518.jpg" alt=""></a><br><a id="more"></a><br>MySQL 查询过程</p><p><strong>优化的哲学</strong></p><p>注：优化有风险，修改需谨慎。</p><ul><li>优化可能带来的问题：</li><li>优化不总是对一个单纯的环境进行，还很可能是一个复杂的已投产的系统。</li><li>优化手段本来就有很大的风险，只不过你没能力意识到和预见到。</li><li>任何的技术可以解决一个问题，但必然存在带来一个问题的风险。</li><li>对于优化来说解决问题而带来的问题，控制在可接受的范围内才是有成果。</li><li>保持现状或出现更差的情况都是失败。</li></ul><p>优化的需求：</p><ul><li>稳定性和业务可持续性，通常比性能更重要。</li><li>优化不可避免涉及到变更，变更就有风险。</li><li>优化使性能变好，维持和变差是等概率事件。</li><li>切记优化，应该是各部门协同，共同参与的工作，任何单一部门都不能对数据库进行优化。</li></ul><p>所以优化工作，是由业务需求驱使的!</p><p>优化由谁参与?在进行数据库优化时，应由数据库管理员、业务部门代表、应用程序架构师、应用程序设计人员、应用程序开发人员、硬件及系统管理员、存储管理员等，业务相关人员共同参与。</p><p><strong>优化思路</strong></p><p><strong>优化什么</strong></p><p>在数据库优化上有两个主要方面：</p><ul><li>安全：数据可持续性。</li><li>性能：数据的高性能访问。</li></ul><p><strong>优化的范围有哪些</strong></p><p>存储、主机和操作系统方面：</p><ul><li><strong>主机架构稳定性  </strong></li><li><strong>I/O 规划及配置  </strong></li><li><strong>Swap 交换分区  </strong></li><li><strong>OS 内核参数和网络问题</strong></li></ul><p>应用程序方面：</p><ul><li><strong>应用程序稳定性</strong></li><li><strong>SQL 语句性能</strong></li><li><strong>串行访问资源</strong></li><li><strong>性能欠佳会话管理</strong></li><li><strong>这个应用适不适合用 MySQL</strong></li></ul><p>数据库优化方面：</p><ul><li><strong>内存</strong></li><li><strong>数据库结构(物理&amp;逻辑)</strong></li><li><strong>实例配置</strong></li></ul><p>说明：不管是设计系统、定位问题还是优化，都可以按照这个顺序执行。</p><p><strong>优化维度</strong></p><p><a href="http://s5.51cto.com/oss/201901/15/03f19a43a7ec164b325cbcb7154fb1fd.jpg-wh_600x-s_3464880710.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/15/03f19a43a7ec164b325cbcb7154fb1fd.jpg-wh_600x-s_3464880710.jpg" alt=""></a></p><p>数据库优化维度有如下四个：</p><ul><li><strong>硬件</strong></li><li><strong>系统配置</strong></li><li><strong>数据库表结构</strong></li><li><strong>SQL 及索引</strong></li></ul><p>优化选择：</p><ul><li>优化成本：硬件&gt;系统配置&gt;数据库表结构&gt;SQL 及索引。</li><li>优化效果：硬件&lt;系统配置&lt;数据库表结构</li></ul><p><strong>优化工具有啥</strong></p><p><strong>数据库层面</strong></p><p>检查问题常用的 12 个工具：</p><ul><li>MySQL</li><li>mysqladmin：MySQL 客户端，可进行管理操作</li><li>mysqlshow：功能强大的查看 shell 命令</li><li>SHOW [SESSION | GLOBAL] variables：查看数据库参数信息</li><li>SHOW [SESSION | GLOBAL] STATUS：查看数据库的状态信息</li><li>information_schema：获取元数据的方法</li><li>SHOW ENGINE INNODB STATUS：Innodb 引擎的所有状态</li><li>SHOW PROCESSLIST：查看当前所有连接的 session 状态</li><li>explain：获取查询语句的执行计划</li><li>show index：查看表的索引信息</li><li>slow-log：记录慢查询语句</li><li>mysqldumpslow：分析 slowlog 文件的工具</li></ul><p>不常用但好用的 7 个工具：</p><ul><li>Zabbix：监控主机、系统、数据库(部署 Zabbix 监控平台)</li><li>pt-query-digest：分析慢日志</li><li>MySQL slap：分析慢日志</li><li>sysbench：压力测试工具</li><li>MySQL profiling：统计数据库整体状态工具</li><li>Performance Schema：MySQL 性能状态统计的数据</li><li>workbench：管理、备份、监控、分析、优化工具(比较费资源)</li></ul><p>关于 Zabbix 参考：<a href="http://www.cnblogs.com/clsn/p/7885990.html" target="_blank" rel="noopener">http://www.cnblogs.com/clsn/p/7885990.html</a></p><p><strong>数据库层面问题解决思路</strong></p><p>一般应急调优的思路：针对突然的业务办理卡顿，无法进行正常的业务处理，需要马上解决的场景。</p><ol><li>show processlist </li><li>explain  select id ,name  from stu where  name=’clsn’; # ALL id name age  sex </li><li>select id,name  from stu where id=2-1 函数 结果集&gt;30; </li><li>show index  from  table; </li><li>通过执行计划判断，索引问题（有没有、合不合理）或者语句本身问题 </li><li>show status  like  ‘%lock%’;    # 查询锁状态 </li><li>kill SESSION_ID;   # 杀掉有问题的session </li></ol><p>常规调优思路：针对业务周期性的卡顿，例如在每天 10-11 点业务特别慢，但是还能够使用，过了这段时间就好了。</p><ol><li>查看slowlog，分析slowlog，分析出查询慢的语句； </li><li>按照一定优先级，一个一个排查所有慢语句； </li><li>分析top SQL，进行explain调试，查看语句执行时间； </li><li>调整索引或语句本身。 </li></ol><p><strong>系统层面</strong></p><p>CPU方面：vmstat、sar top、htop、nmon、mpstat。</p><p>内存：free、ps-aux。</p><p>IO 设备(磁盘、网络)：iostat、ss、netstat、iptraf、iftop、lsof。</p><p>vmstat 命令说明：</p><ul><li>Procs：r 显示有多少进程正在等待 CPU 时间。b 显示处于不可中断的休眠的进程数量。在等待 I/O。</li><li>Memory：swpd 显示被交换到磁盘的数据块的数量。未被使用的数据块，用户缓冲数据块，用于操作系统的数据块的数量。</li><li>Swap：操作系统每秒从磁盘上交换到内存和从内存交换到磁盘的数据块的数量。s1 和 s0 最好是 0。</li><li>IO：每秒从设备中读入 b1 的写入到设备 b0 的数据块的数量。反映了磁盘 I/O。</li><li>System：显示了每秒发生中断的数量(in)和上下文交换(cs)的数量。</li><li>CPU：显示用于运行用户代码，系统代码，空闲，等待 I/O 的 CPU 时间。</li></ul><p>iostat 命令说明：</p><ul><li>实例命令：iostat -dk 1 5;iostat -d -k -x 5 (查看设备使用率(%util)和响应时间(await))。</li><li>TPS：该设备每秒的传输次数。“一次传输”意思是“一次 I/O 请求”。多个逻辑请求可能会被合并为“一次 I/O 请求”。</li><li>iops ：硬件出厂的时候，厂家定义的一个每秒最大的 IO 次数。</li><li>“一次传输”请求的大小是未知的。</li><li>KB_read/s：每秒从设备(drive expressed)读取的数据量。</li><li>KB_wrtn/s：每秒向设备(drive expressed)写入的数据量。</li><li>KB_read：读取的总数据量。</li><li>KB_wrtn：写入的总数量数据量;这些单位都为 Kilobytes。</li></ul><p><strong>系统层面问题解决办法</strong></p><p>你认为到底负载高好，还是低好呢?在实际的生产中，一般认为 CPU 只要不超过 90% 都没什么问题。当然不排除下面这些特殊情况。</p><p>CPU 负载高，IO 负载低：</p><ul><li>内存不够</li><li>磁盘性能差</li><li>SQL 问题：去数据库层，进一步排查 SQL 问题</li><li>IO 出问题了(磁盘到临界了、raid 设计不好、raid 降级、锁、在单位时间内 TPS 过高)</li><li>TPS 过高：大量的小数据 IO、大量的全表扫描</li></ul><p>IO 负载高，CPU 负载低：</p><ul><li>大量小的 IO 写操作</li><li>autocommit，产生大量小 IO;IO/PS，磁盘的一个定值，硬件出厂的时候，厂家定义的一个每秒最大的 IO 次数。</li><li>大量大的 IO 写操作：SQL 问题的几率比较大</li></ul><p>IO和 CPU 负载都很高：</p><ul><li>硬件不够了或 SQL 存在问题</li></ul><p><strong>基础优化</strong></p><p><strong>优化思路</strong></p><p>定位问题点吮吸：硬件&gt;系统&gt;应用&gt;数据库&gt;架构(高可用、读写分离、分库分表)。</p><p>处理方向：明确优化目标、性能和安全的折中、防患未然。</p><p><strong>硬件优化</strong></p><p><strong>①主机方面</strong></p><p>根据数据库类型，主机 CPU 选择、内存容量选择、磁盘选择：</p><ul><li>平衡内存和磁盘资源</li><li>随机的 I/O 和顺序的 I/O</li><li>主机 RAID 卡的 BBU(Battery Backup Unit)关闭</li></ul><p><strong>②CPU 的选择</strong></p><p>CPU 的两个关键因素：核数、主频。根据不同的业务类型进行选择：</p><ul><li>CPU 密集型：计算比较多，OLTP 主频很高的 CPU、核数还要多。</li><li>IO 密集型：查询比较，OLAP 核数要多，主频不一定高的。</li></ul><p><strong>③内存的选择</strong></p><p>OLAP 类型数据库，需要更多内存，和数据获取量级有关。OLTP 类型数据一般内存是 CPU 核心数量的 2 倍到 4 倍，没有最佳实践。</p><p><strong>④存储方面</strong></p><p>根据存储数据种类的不同，选择不同的存储设备，配置合理的 RAID 级别(raid5、raid10、热备盘)。</p><p>对于操作系统来讲，不需要太特殊的选择，最好做好冗余(raid1)(ssd、sas、sata)。</p><p>主机 raid 卡选择：</p><ul><li>实现操作系统磁盘的冗余(raid1)</li><li>平衡内存和磁盘资源</li><li>随机的 I/O 和顺序的 I/O</li><li>主机 raid 卡的 BBU(Battery Backup Unit)要关闭</li></ul><p><strong>⑤网络设备方面</strong></p><p>使用流量支持更高的网络设备(交换机、路由器、网线、网卡、HBA 卡)。注意：以上这些规划应该在初始设计系统时就应该考虑好。</p><p><strong>服务器硬件优化</strong></p><p>服务器硬件优化关键点：</p><ul><li>物理状态灯</li><li>自带管理设备：远程控制卡(FENCE设备：ipmi ilo idarc)、开关机、硬件监控。</li><li>第三方的监控软件、设备(snmp、agent)对物理设施进行监控。</li><li>存储设备：自带的监控平台。EMC2(HP 收购了)、 日立(HDS)、IBM 低端 OEM HDS、高端存储是自己技术，华为存储。</li></ul><p><strong>系统优化</strong></p><p>CPU：基本不需要调整，在硬件选择方面下功夫即可。</p><p>内存：基本不需要调整，在硬件选择方面下功夫即可。</p><p>SWAP：MySQL 尽量避免使用 Swap。阿里云的服务器中默认 swap 为 0。</p><p>IO ：raid、no lvm、ext4 或 xfs、ssd、IO 调度策略。</p><p>Swap 调整(不使用 swap 分区)：</p><ol><li>/proc/sys/vm/swappiness的内容改成0(临时)，/etc/sysctl. conf上添加vm.swappiness=0(永久) </li></ol><p>这个参数决定了 Linux 是倾向于使用 Swap，还是倾向于释放文件系统 Cache。在内存紧张的情况下，数值越低越倾向于释放文件系统 Cache。</p><p>当然，这个参数只能减少使用 Swap 的概率，并不能避免 Linux 使用 Swap。</p><p>修改 MySQL 的配置参数 innodb_flush_ method，开启 O_DIRECT 模式。</p><p>这种情况下，InnoDB 的 buffer pool 会直接绕过文件系统 Cache 来访问磁盘，但是 redo log 依旧会使用文件系统 Cache。</p><p>值得注意的是，Redo log 是覆写模式的，即使使用了文件系统的 Cache，也不会占用太多。</p><p>IO 调度策略：</p><ol><li>#echo deadline&gt;/sys/block/sda/queue/scheduler 临时修改为deadline </li></ol><p>永久修改：</p><ol><li>vi /boot/grub/grub.conf </li><li>更改到如下内容: </li><li>kernel /boot/vmlinuz-2.6.18-8.el5 ro root=LABEL=/ elevator=deadline rhgb quiet </li></ol><p><strong>系统参数调整</strong></p><p>Linux 系统内核参数优化：</p><ol><li>vim/etc/sysctl.conf </li><li>net.ipv4.ip_local_port_range = 1024 65535：# 用户端口范围 </li><li>net.ipv4.tcp_max_syn_backlog = 4096 </li><li>net.ipv4.tcp_fin_timeout = 30 </li><li>fs.file-max=65535：# 系统最大文件句柄，控制的是能打开文件最大数量 </li></ol><p>用户限制参数(MySQL 可以不设置以下配置)：</p><ol><li>vim/etc/security/limits.conf </li><li><ul><li>soft nproc 65535 </li></ul></li><li><ul><li>hard nproc 65535 </li></ul></li><li><ul><li>soft nofile 65535 </li></ul></li><li><ul><li>hard nofile 65535 </li></ul></li></ol><p><strong>应用优化</strong></p><p>业务应用和数据库应用独立。</p><p>防火墙：iptables、selinux 等其他无用服务(关闭)：</p><ol><li>chkconfig –level 23456 acpid off </li><li>chkconfig –level 23456 anacron off </li><li>chkconfig –level 23456 autofs off </li><li>chkconfig –level 23456 avahi-daemon off </li><li>chkconfig –level 23456 bluetooth off </li><li>chkconfig –level 23456 cups off </li><li>chkconfig –level 23456 firstboot off </li><li>chkconfig –level 23456 haldaemon off </li><li>chkconfig –level 23456 hplip off </li><li>chkconfig –level 23456 ip6tables off </li><li>chkconfig –level 23456 iptables  off </li><li>chkconfig –level 23456 isdn off </li><li>chkconfig –level 23456 pcscd off </li><li>chkconfig –level 23456 sendmail  off </li><li>chkconfig –level 23456 yum-updatesd  off </li></ol><p>安装图形界面的服务器不要启动图形界面 runlevel 3。</p><p>另外，思考将来我们的业务是否真的需要 MySQL，还是使用其他种类的数据库。用数据库的最高境界就是不用数据库。</p><p><strong>数据库优化</strong></p><p>SQL 优化方向：</p><ul><li>执行计划</li><li>索引</li><li>SQL 改写</li></ul><p>架构优化方向：</p><ul><li>高可用架构</li><li>高性能架构</li><li>分库分表</li></ul><p><strong>数据库参数优化</strong></p><p><strong>①调整</strong></p><p>实例整体(高级优化，扩展)：</p><ol><li>thread_concurrency：# 并发线程数量个数 </li><li>sort_buffer_size：# 排序缓存 </li><li>read_buffer_size：# 顺序读取缓存 </li><li>read_rnd_buffer_size：# 随机读取缓存 </li><li>key_buffer_size：# 索引缓存 </li><li>thread_cache_size：# (1G—&gt;8, 2G—&gt;16, 3G—&gt;32, &gt;3G—&gt;64) </li></ol><p><strong>②连接层(基础优化)</strong></p><p>设置合理的连接客户和连接方式：</p><ol><li>max_connections           # 最大连接数，看交易笔数设置 </li><li>max_connect_errors        # 最大错误连接数，能大则大 </li><li>connect_timeout           # 连接超时 </li><li>max_user_connections      # 最大用户连接数 </li><li>skip-name-resolve         # 跳过域名解析 </li><li>wait_timeout              # 等待超时 </li><li>back_log                  # 可以在堆栈中的连接数量 </li></ol><p><strong>③SQL 层(基础优化)</strong></p><p>query_cache_size： 查询缓存 &gt;&gt;&gt; OLAP 类型数据库，需要重点加大此内存缓存，但是一般不会超过 GB。</p><p>对于经常被修改的数据，缓存会马上失效。我们可以使用内存数据库(redis、memecache)，替代它的功能。</p><p><strong>存储引擎层优化</strong></p><p>innodb 基础优化参数：</p><ol><li>default-storage-engine </li><li>innodb_buffer_pool_size       # 没有固定大小，50%测试值，看看情况再微调。但是尽量设置不要超过物理内存70% </li><li>innodb_file_per_table=(1,0) </li><li>innodb_flush_log_at_trx_commit=(0,1,2) # 1是最安全的，0是性能最高，2折中 </li><li>binlog_sync </li><li>Innodb_flush_method=(O_DIRECT, fdatasync) </li><li>innodb_log_buffer_size        # 100M以下 </li><li>innodb_log_file_size          # 100M 以下 </li><li>innodb_log_files_in_group     # 5个成员以下,一般2-3个够用（iblogfile0-N） </li><li>innodb_max_dirty_pages_pct   # 达到百分之75的时候刷写 内存脏页到磁盘。 </li><li>log_bin </li><li>max_binlog_cache_size         # 可以不设置 </li><li>max_binlog_size               # 可以不设置 </li><li>innodb_additional_mem_pool_size    #小于2G内存的机器，推荐值是20M。32G内存以上100M </li></ol><p>参考文章：</p><p><a href="https://www.cnblogs.com/zishengY/p/6892345.html" target="_blank" rel="noopener">https://www.cnblogs.com/zishengY/p/6892345.html</a></p><p><a href="https://www.jianshu.com/p/d7665192aaaf" target="_blank" rel="noopener">https://www.jianshu.com/p/d7665192aaaf</a></p><p>原文地址：<a href="http://database.51cto.com/art/201901/590564.htm" target="_blank" rel="noopener">http://database.51cto.com/art/201901/590564.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目前主流的消息中间件介绍</title>
      <link href="/2019/01/04/%E7%9B%AE%E5%89%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/01/04/%E7%9B%AE%E5%89%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>MQ 消息中间件可以理解为一个水池，水池的这头是消息生产者，水池的那头是消息消费者，生产者和消息者无需直接对接，这将带来很多好处：业务解耦、架构分布式化等，生产者和消费者互相完全透明。</p><p>但市面上的 MQ 消息中间件产品很多，作为 IM 系统中必不可少的一环，我们该如何选型?<br><a id="more"></a></p><p><strong>什么是消息队列中间件</strong></p><p>消息队列中间件(简称消息中间件)是指利用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。</p><p>通过提供消息传递和消息排队模型，它可以在分布式环境下提供应用解耦、弹性伸缩、冗余存储、流量削峰、异步通信、数据同步等等功能，其作为分布式系统架构中的一个重要组件，有着举足轻重的地位。</p><p><a href="http://s3.51cto.com/oss/201901/04/0e247e5820ff75eb3e0ef1289fbe6527.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201901/04/0e247e5820ff75eb3e0ef1289fbe6527.jpg" alt=""></a></p><p>目前开源的消息中间件可谓是琳琅满目，能让大家耳熟能详的就有很多，比如 ActiveMQ、RabbitMQ、Kafka、RocketMQ、ZeroMQ 等，不管选择其中的哪一款，都会有用的不趁手的地方，毕竟不是为你量身定制的。</p><p>可能有些大厂在长期的使用过程中积累了一定的经验，加上其消息队列的使用场景也相对稳定固化，或者目前市面上的消息中间件无法满足自身需求，同时它也具备足够的精力和人力而选择自研来为自己量身打造一款消息中间件。</p><p>但是绝大多数公司还是不会选择重复造轮子，那么选择一款适合自己的消息中间件显得尤为重要。</p><p>就算是前者，那么在自研出稳定且可靠的相关产品之前也会经历这样一个选型过程。</p><p>在整体架构中引入消息中间件，势必要考虑很多因素，比如成本及收益问题，怎么样才能达到最优的性价比?</p><p>虽然消息中间件种类繁多，但是各自都有各自的侧重点，选择合适自己、扬长避短无疑是最好的方式。如果你对此感到无所适从，本文或许可以参考一二。</p><p><strong>各类消息队列简述</strong></p><p><strong>ActiveMQ</strong></p><p>Apache 出品的、采用 Java 语言编写的完全基于 JMS1.1 规范的面向消息的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。</p><p>不过由于历史原因包袱太重，目前市场份额没有后面三种消息中间件多，其最新架构被命名为 Apollo，号称下一代 ActiveMQ，有兴趣的同学可自行了解。</p><p><strong>RabbitMQ</strong></p><p>采用 Erlang 语言实现的 AMQP 协议的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。</p><p>RabbitMQ 发展到今天，被越来越多的人认可，这和它在可靠性、可用性、扩展性、功能丰富等方面的卓越表现是分不开的。</p><p><strong>Kafka</strong></p><p>起初是由 LinkedIn 公司采用 Scala 语言开发的一个分布式、多分区、多副本且基于 ZooKeeper 协调的分布式消息系统，现已捐献给 Apache 基金会。</p><p>它是一种高吞吐量的分布式发布订阅消息系统，以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如 Cloudera、Apache Storm、Spark、Flink 等都支持与 Kafka 集成。</p><p><strong>RocketMQ</strong></p><p>是阿里开源的消息中间件，目前已经捐献给 Apache 基金会，它是由 Java 语言开发的，具备高吞吐量、高可用性、适合大规模分布式系统应用等特点，经历过双 11 的洗礼，实力不容小觑。</p><p><strong>ZeroMQ</strong></p><p>号称史上最快的消息队列，基于 C 语言开发。ZeroMQ 是一个消息处理队列库，可在多线程、多内核和主机之间弹性伸缩。</p><p>虽然大多数时候我们习惯将其归入消息队列家族之中，但是其和前面的几款有着本质的区别，ZeroMQ 本身就不是一个消息队列服务器，更像是一组底层网络通讯库，对原有的 Socket API 上加上一层封装而已。</p><p>目前市面上的消息中间件还有很多，比如腾讯系的 PhxQueue、CMQ、CKafka，又比如基于 Go 语言的 NSQ，有时人们也把类似 Redis 的产品也看做消息中间件的一种。</p><p>当然，它们都很优秀，但是本文篇幅限制无法穷其所有，下面会针对性地挑选 RabbitMQ 和 Kafka 两款典型的消息中间件来做分析，力求站在一个公平公正的立场来阐述消息中间件选型中的各个要点。</p><p><strong>消息中间件选型要点</strong></p><p>衡量一款消息中间件是否符合需求，需要从多个维度进行考察。</p><p>首要的就是功能维度，这个直接决定了你能否最大程度上地实现开箱即用，进而缩短项目周期、降低成本等。</p><p>如果一款消息中间件的功能达不到想要的功能，那么就需要进行二次开发，这样会增加项目的技术难度、复杂度以及增大项目周期等。</p><p>消息中间件具体选型指标</p><p><strong>功能维度</strong></p><p>功能维度又可以划分成多个子维度，大致可以分为以下这些。</p><p><strong>优先级队列</strong></p><p>优先级队列不同于先进先出队列，优先级高的消息具备优先被消费的特权，这样可以为下游提供不同消息级别的保证。</p><p>不过这个优先级也是需要有一个前提的：如果消费者的消费速度大于生产者的速度，并且消息中间件服务器(一般简单的称之为 Broker)中没有消息堆积。</p><p>那么对于发送的消息设置优先级也就没有什么实质性的意义了，因为生产者刚发送完一条消息就被消费者消费了，那么就相当于 Broker 中至多只有一条消息，对于单条消息来说优先级是没有什么意义的。</p><p><strong>延迟队列</strong></p><p>当你在网上购物的时候是否会遇到这样的提示：“三十分钟之内未付款，订单自动取消”，这个是延迟队列的一种典型应用场景。</p><p>延迟队列存储的是对应的延迟消息，所谓“延迟消息”是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。</p><p>延迟队列一般分为两种：</p><ul><li>基于消息的延迟，是指为每条消息设置不同的延迟时间，那么每当队列中有新消息进入的时候就会重新根据延迟时间排序，当然这也会对性能造成极大的影响。</li><li>基于队列的延迟，实际应用中大多采用这种，设置不同延迟级别的队列，比如5s、10s、30s、1min、5mins、10mins等，每个队列中消息的延迟时间都是相同的，这样免去了延迟排序所要承受的性能之苦，通过一定的扫描策略(比如定时)即可投递超时的消息。</li></ul><p><strong>死信队列</strong></p><p>由于某些原因消息无法被正确投递，为了确保消息不会被无故丢弃，一般将其置于一个特殊角色的队列，这个队列称为死信队列。</p><p>与此对应的还有一个“回退队列”的概念，试想如果消费者在消费时发生了异常，那么就不会对这一次消费进行确认(Ack), 进而发生回滚消息的操作之后消息始终会放在队列的顶部，然后不断被处理和回滚，导致队列陷入死循环。</p><p>为了解决这个问题，可以为每个队列设置一个回退队列，它和死信队列都是为异常的处理提供的一种机制保障。实际情况下，回退队列的角色可以由死信队列和重试队列来扮演。</p><p><strong>重试队列</strong></p><p>其实可以看成是一种回退队列，具体指消费端消费消息失败时，为防止消息无故丢失而重新将消息回滚到 Broker 中。</p><p>与回退队列不同的是重试队列一般分成多个重试等级，每个重试等级一般也会设置重新投递延时，重试次数越多投递延时就越大。</p><p>举个例子：消息第一次消费失败入重试队列 Q1，Q1 的重新投递延迟为 5s，在 5s 过后重新投递该消息。</p><p>如果消息再次消费失败则入重试队列 Q2，Q2 的重新投递延迟为 10s，在 10s 过后再次投递该消息。</p><p>以此类推，重试越多次重新投递的时间就越久，为此需要设置一个上限，超过投递次数就入死信队列。</p><p>重试队列与延迟队列有相同的地方，都是需要设置延迟级别，它们彼此的区别是：延迟队列动作由内部触发，重试队列动作由外部消费端触发;延迟队列作用一次，而重试队列的作用范围会向后传递。</p><p><strong>消费模式</strong></p><p>消费模式分为推(push)模式和拉(pull)模式：</p><ul><li>推模式，是指由 Broker 主动推送消息至消费端，实时性较好，不过需要一定的流制机制来确保服务端推送过来的消息不会压垮消费端。</li><li>拉模式，是指消费端主动向 Broker 端请求拉取(一般是定时或者定量)消息，实时性较推模式差，但是可以根据自身的处理能力而控制拉取的消息量。</li></ul><p><strong>广播消费</strong></p><p>消息一般有两种传递模式——点对点(P2P，Point-to-Point)模式和发布/订阅(Pub/Sub)模式：</p><ul><li>对于点对点的模式而言，消息被消费以后，队列中不会再存储，所以消息消费者不可能消费到已经被消费的消息。虽然队列可以支持多个消费者，但是一条消息只会被一个消费者消费。</li><li>发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题(Topic)，主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者则从主题中订阅消息。</li></ul><p>主题使得消息的订阅者与消息的发布者互相保持独立，不需要进行接触即可保证消息的传递，发布/订阅模式在消息的一对多广播时采用。</p><p>RabbitMQ 是一种典型的点对点模式，而 Kafka 是一种典型的发布订阅模式。</p><p>但是 RabbitMQ 中可以通过设置交换器类型来实现发布订阅模式而达到广播消费的效果，Kafka 中也能以点对点的形式消费，你完全可以把其消费组(Consumer Group)的概念看成是队列的概念。</p><p>不过对比来说，Kafka 中因为有了消息回溯功能的存在，对于广播消费的力度支持比 RabbitMQ 的要强。</p><p><strong>消息回溯</strong></p><p>一般消息在消费完成之后就被处理了，之后再也不能消费到该条消息。消息回溯正好相反，是指消息在消费完成之后，还能消费到之前被消费掉的消息。</p><p>对于消息而言，经常面临的问题是“消息丢失”，至于是真正由于消息中间件的缺陷丢失还是由于使用方的误用而丢失，一般很难追查。</p><p>如果消息中间件本身具备消息回溯功能的话，可以通过回溯消费复现“丢失的”消息进而查出问题的源头所在。</p><p>消息回溯的作用远不止于此，比如还有索引恢复、本地缓存重建，有些业务补偿方案也可以采用回溯的方式来实现。</p><p><strong>消息堆积+持久化</strong></p><p>流量削峰是消息中间件的一个非常重要的功能，而这个功能其实得益于其消息堆积能力。</p><p>从某种意义上来讲，如果一个消息中间件不具备消息堆积的能力，那么就不能把它看做是一个合格的消息中间件。</p><p>消息堆积分内存式堆积和磁盘式堆积：</p><ul><li>RabbitMQ 是典型的内存式堆积，但这并非绝对，在某些条件触发后会有换页动作来将内存中的消息换页到磁盘(换页动作会影响吞吐)，或者直接使用惰性队列来将消息直接持久化至磁盘中。</li><li>Kafka 是一种典型的磁盘式堆积，所有的消息都存储在磁盘中。</li></ul><p>一般来说，磁盘的容量会比内存的容量要大得多，对于磁盘式的堆积其堆积能力就是整个磁盘的大小。</p><p>从另外一个角度讲，消息堆积也为消息中间件提供了冗余存储的功能。援引《纽约时报》的案例，其直接将 Kafka 用作存储系统。</p><p><strong>消息追踪</strong></p><p>对于分布式架构系统中的链路追踪(Trace)，大家一定不陌生。对于消息中间件，消息的链路追踪(以下简称消息追踪)同样重要，最通俗来理解，就是要知道消息从哪来，存在哪里以及发往哪里去。</p><p>基于此功能，我们可以对发送或者消费完的消息进行链路追踪服务，进而可以进行问题的快速定位与排查。</p><p><strong>消息过滤</strong></p><p>消息过滤是指按照既定的过滤规则为下游用户提供指定类别的消息。</p><p>就以 Kafka 而言，完全可以将不同类别的消息发送至不同的 Topic 中，由此可以实现某种意义的消息过滤，或者 Kafka 还可以根据分区对同一个 Topic 中的消息进行分类。</p><p>不过，更加严格意义上的消息过滤，应该是对既定的消息采取一定的方式按照一定的过滤规则进行过滤。</p><p>同样以 Kafka 为例，可以通过客户端提供的 Consumer Interceptor 接口或者 Kafka Stream 的 Filter 功能进行消息过滤。</p><p><strong>多租户</strong></p><p>也可以称为多重租赁技术，是一种软件架构技术，主要用来实现多用户的环境下公用相同的系统或程序组件，并且仍可以确保各用户间数据的隔离性。</p><p>RabbitMQ 就能够支持多租户技术，每一个租户表示为一个 VHost，其本质上是一个独立的小型 RabbitMQ 服务器，又有自己独立的队列、交换器及绑定关系等，并且它拥有自己独立的权限。</p><p>VHost 就像是物理机中的虚拟机一样，它们在各个实例间提供逻辑上的分离，为不同程序安全保密地允许数据，它既能将同一个 RabbitMQ 中的众多客户区分开，又可以避免队列和交换器等命名冲突。</p><p><strong>多协议支持</strong></p><p>消息是信息的载体，为了让生产者和消费者都能理解所承载的信息(生产者需要知道如何构造消息，消费者需要知道如何解析消息)，它们就需要按照一种统一的格式描述消息，这种统一的格式称之为消息协议。</p><p>有效的消息一定具有某种格式，而没有格式的消息是没有意义的。</p><p>一般消息层面的协议有 AMQP、MQTT、STOMP、XMPP 等(消息领域中的 JMS 更多的是一个规范而不是一个协议)，支持的协议越多其应用范围就会越广，通用性越强。</p><p>比如 RabbitMQ 能够支持 MQTT 协议就让其在物联网应用中获得一席之地。还有的消息中间件是基于其本身的私有协议运转的，典型的如 Kafka。</p><p><strong>跨语言支持</strong></p><p>对很多公司而言，其技术栈体系中会有多种编程语言，如 C/C++、Java、Go、PHP 等，消息中间件本身具备应用解耦的特性，如果能够进一步的支持多客户端语言，那么就可以将此特性的效能扩大。</p><p>跨语言的支持力度也从侧面反映出一个消息中间件的流行程度。</p><p><strong>流量控制</strong></p><p>针对的是发送方和接收方速度不匹配的问题，提供一种速度匹配服务抑制发送速率使接收方应用程序的读取速率与之相适应。通常的流控方法有 Stop-and-Wait、滑动窗口以及令牌桶等。</p><p><strong>消息顺序性</strong></p><p>顾名思义，是指保证消息有序。这个功能有个很常见的应用场景就是 CDC(Change Data Chapture)。</p><p>以 MySQL 为例，如果其传输的 Binlog 的顺序出错，比如原本是先对一条数据加 1，然后再乘以 2，发送错序之后就变成了先乘以 2 后加 1，造成数据不一致。</p><p><strong>安全机制</strong></p><p>在 Kafka 0.9 版本之后就开始增加了身份认证和权限控制两种安全机制：</p><ul><li>身份认证，是指客户端与服务端连接进行身份认证，包括客户端与 Broker 之间、Broker 与 Broker 之间、Broker 与 ZooKeeper 之间的连接认证，目前支持 SSL、SASL 等认证机制。</li><li>权限控制，是指对客户端的读写操作进行权限控制，包括对消息或 Kafka 集群操作权限控制。权限控制是可插拔的，并支持与外部的授权服务进行集成。</li></ul><p>对于 RabbitMQ 而言，其同样提供身份认证(TLS/SSL、SASL)和权限控制(读写操作)的安全机制。</p><p><strong>消息幂等性</strong></p><p>确保消息在生产者和消费者之间进行传输，一般有三种传输保障(Delivery Guarantee)：</p><ul><li>At most once，至多一次，消息可能丢失，但绝不会重复传输。</li><li>At least once，至少一次，消息绝不会丢，但是可能会重复。</li><li>Exactly once，精确一次，每条消息肯定会被传输一次且仅一次。</li></ul><p>对于大多数消息中间件而言，一般只提供 At most once 和 At least once 两种传输保障，对于第三种一般很难做到，由此消息幂等性也很难保证。</p><p>Kafka 自 0.11 版本开始引入了幂等性和事务，Kafka 的幂等性是指单个生产者对于单分区单会话的幂等。</p><p>而事务可以保证原子性地写入到多个分区，即写入到多个分区的消息要么全部成功，要么全部回滚，这两个功能加起来可以让 Kafka 具备 EOS(Exactly Once Semantic)的能力。</p><p>不过如果要考虑全局的幂等，还需要从上下游方面综合考虑，即关联业务层面，幂等处理本身也是业务层面所需要考虑的重要议题。</p><p>以下游消费者层面为例，有可能消费者消费完一条消息之后没有来得及确认消息就发生异常，等到恢复之后又得重新消费原来消费过的那条消息，那么这种类型的消息幂等是无法由消息中间件层面来保证的。</p><p>如果要保证全局的幂等，需要引入更多的外部资源来保证，比如以订单号作为唯一性标识，并且在下游设置一个去重表。</p><p><strong>事务性消息</strong></p><p>事务本身是一个并不陌生的词汇，事务是由事务开始(Begin Transaction)和事务结束(End Transaction)之间执行的全体操作组成。</p><p>支持事务的消息中间件并不在少数，Kafka 和 RabbitMQ 都支持，不过此两者的事务是指生产者发生消息的事务，要么发送成功，要么发送失败。</p><p>消息中间件可以作为用来实现分布式事务的一种手段，但其本身并不提供全局分布式事务的功能。</p><p>下表是对 Kafka 与 RabbitMQ 功能的总结性对比及补充说明：</p><p><a href="http://s5.51cto.com/oss/201901/04/55fa6e5eb1a5effdf6dcc55d525c9c42.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201901/04/55fa6e5eb1a5effdf6dcc55d525c9c42.jpg" alt=""></a></p><p><strong>性能</strong></p><p>功能维度是消息中间件选型中的一个重要的参考维度，但这并不是唯一的维度，有时候性能比功能还要重要，况且性能和功能很多时候是相悖的，鱼和熊掌不可兼得。</p><p>Kafka 在开启幂等、事务功能的时候会使其性能降低;RabbitMQ 在开启 rabbitmq_tracing 插件的时候也会极大影响其性能。</p><p><strong>性能指什么?</strong></p><p>消息中间件的性能一般是指其吞吐量。虽然从功能维度上来说，RabbitMQ 的优势要大于 Kafka，但是 Kafka 的吞吐量要比 RabbitMQ 高出 1 至 2 个数量级。</p><p>一般 RabbitMQ 的单机 QPS 在万级别之内，而 Kafka 的单机 QPS 可以维持在十万级别，甚至可以达到百万级。</p><p>注明：消息中间件的吞吐量始终会受到硬件层面的限制。就以网卡带宽为例，如果单机单网卡的带宽为 1Gbps，如果要达到百万级的吞吐，那么消息体大小不得超过(1Gb/8)/100W，即约等于 134B。</p><p>换句话说如果消息体大小超过 134B，那么就不可能达到百万级别的吞吐。这种计算方式同样可以适用于内存和磁盘。</p><p><strong>性能的指标是什么?</strong></p><p>时延作为性能维度的一个重要指标，却往往在消息中间件领域被忽视，因为一般使用消息中间件的场景对时效性的要求并不是很高，如果要求时效性完全可以采用 RPC 的方式实现。</p><p>消息中间件具备消息堆积的能力，消息堆积越大也就意味着端到端的时延也就越长，与此同时延时队列也是某些消息中间件的一大特色。那么为什么还要关注消息中间件的时延问题呢?</p><p>消息中间件能够解耦系统，对于一个时延较低的消息中间件而言，它可以让上游生产者发送消息之后可以迅速的返回，也可以让消费者更加快速的获取到消息，在没有堆积的情况下，可以让整体上下游的应用之间的级联动作更加高效。</p><p>虽然不建议在时效性很高的场景下使用消息中间件，但是如果所使用的消息中间件的时延方面比较优秀，那么对于整体系统的性能将会是一个不小的提升。</p><p><strong>可靠性+可用性</strong></p><p>消息丢失是使用消息中间件时所不得不面对的一个痛点，其背后消息可靠性也是衡量消息中间件好坏的一个关键因素。尤其是在金融支付领域，消息可靠性尤为重要。</p><p>然而说到可靠性必然要说到可用性，注意这两者之间的区别：</p><ul><li>消息中间件的可靠性是指对消息不丢失的保障程度。</li><li>而消息中间件的可用性是指无故障运行的时间百分比，通常用几个 9 来衡量。</li></ul><p>从狭义的角度来说，分布式系统架构是一致性协议理论的应用实现，对于消息可靠性和可用性而言也可以追溯到消息中间件背后的一致性协议：</p><ul><li>对于 Kafka 而言，其采用的是类似 PacificA 的一致性协议，通过 ISR(In-Sync-Replica)来保证多副本之间的同步，并且支持强一致性语义(通过 Acks 实现)。</li><li>对应的 RabbitMQ 是通过镜像环形队列实现多副本及强一致性语义的。</li></ul><p>多副本可以保证在 Master 节点宕机异常之后可以提升 Slave 作为新的 Master 而继续提供服务来保障可用性。</p><p>Kafka 设计之初是为日志处理而生，给人们留下了数据可靠性要求不高的不良印象，但是随着版本的升级优化，其可靠性得到极大的增强，详细可以参考 KIP101。</p><p>就目前而言，在金融支付领域使用 RabbitMQ 居多，而在日志处理、大数据等方面 Kafka 使用居多，随着 RabbitMQ 性能的不断提升和 Kafka 可靠性的进一步增强，相信彼此都能在以前不擅长的领域分得一杯羹。</p><p>同步刷盘是增强一个组件可靠性的有效方式，消息中间件也不例外，Kafka 和 RabbitMQ 都可以支持同步刷盘。</p><p>但是笔者对同步刷盘有一定的疑问：绝大多数情景下，一个组件的可靠性不应该由同步刷盘这种极其损耗性能的操作来保障，而是采用多副本的机制来保证。</p><p>这里还要提及的一个方面是扩展能力，这里我狭隘地将此归纳到可用性这一维度，消息中间件的扩展能力能够增强其可用能力及范围，比如前面提到的 RabbitMQ 支持多种消息协议，这个就是基于其插件化的扩展实现。</p><p>还有从集群部署上来讲，归功于 Kafka 的水平扩展能力，其基本上可以达到线性容量提升的水平，在 LinkedIn 实践介绍中就提及了有部署超过千台设备的 Kafka 集群。</p><p><strong>运维管理</strong></p><p>在消息中间件的使用过程中难免会出现各式各样的异常情况，有客户端的，也有服务端的，那么怎样及时有效的进行监测及修复?</p><p>业务线流量有峰值有低谷，尤其是电商领域，那么怎样进行有效的容量评估，尤其是大促期间?脚踢电源、网线被挖等事件层出不穷，如何有效的做好异地多活?</p><p>这些都离不开消息中间件的衍生产品——运维管理。运维管理也可以进行进一步的细分，比如申请、审核、监控、告警、管理、容灾、部署等。</p><p>申请、审核很好理解，在源头对资源进行管控，既可以有效校正应用方的使用规范，配合监控也可以做好流量统计与流量评估工作。</p><p>一般申请、审核与公司内部系统交融性较大，不适合使用开源类的产品。</p><p>监控、告警也比较好理解，对消息中间件的使用进行全方位的监控，既可以为系统提供基准数据，也可以在检测到异常的情况配合告警，以便运维、开发人员的迅速介入。</p><p>除了一般的监控项(比如硬件、GC 等)之外，消息中间件还需要关注端到端时延、消息审计、消息堆积等方面：</p><ul><li>对于 RabbitMQ 而言，最正统的监控管理工具莫过于 rabbitmq_management 插件了，但是社区内还有 AppDynamics、Collectd、DataDog、Ganglia、Munin、Nagios、New Relic、Prometheus、Zenoss 等多种优秀的产品。</li><li>Kafka 在此方面也毫不逊色，比如：Kafka Manager、Kafka Monitor、Kafka Offset Monitor、Burrow、Chaperone、Confluent Control Center 等产品，尤其是 Cruise 还可以提供自动化运维的功能。</li></ul><p>不管是扩容、降级、版本升级、集群节点部署、还是故障处理都离不开管理工具的应用，一个配套完备的管理工具集可以在遇到变更时做到事半功倍。</p><p>故障可大可小，一般是一些应用异常，也可以是机器掉电、网络异常、磁盘损坏等单机故障，这些故障单机房内的多副本足以应付。</p><p>如果是机房故障就要涉及异地容灾了，关键点在于如何有效的进行数据复制，Kafka 可以参考 MirrorMarker、uReplicator 等产品，而 RabbitMQ 可以参考 Federation 和 Shovel。</p><p><strong>社区力度及生态发展</strong></p><p>对于目前流行的编程语言而言，如 Java、Python，如果你在使用过程中遇到了一些异常，基本上可以通过搜索引擎的帮助来得到解决，因为一个产品用的人越多，踩过的坑也就越多，对应的解决方案也就越多。</p><p>消息中间件也同样适用，如果你选择了一种“生僻”的消息中间件，可能在某些方面运用的得心应手，但是版本更新缓慢、遇到棘手问题也难以得到社区的支持而越陷越深。</p><p>相反如果你选择了一种“流行”的消息中间件，其更新力度大，不仅可以迅速的弥补之前的不足，而且也能顺应技术的快速发展来变更一些新的功能，这样可以让你也“站在巨人的肩膀上”。</p><p>在运维管理维度我们提及了 Kafka 和 RabbitMQ 都有一系列开源的监控管理产品，这些正是得益于其社区及生态的迅猛发展。</p><p><strong>消息中间件选型误区总结</strong></p><p><strong>选型误区</strong></p><p>在进行消息中间件选型之前可以先问自己一个问题：是否真的需要一个消息中间件?</p><p>在搞清楚这个问题之后，还可以继续问自己一个问题：是否需要自己维护一套消息中间件?</p><p>很多初创型公司为了节省成本会选择直接购买消息中间件有关的云服务，自己只需要关注收发消息即可，其余的都可以外包出去。</p><p>很多人面对消息中间件有一种自研的冲动，你完全可以对 Java 中的 ArrayBlockingQueue 做一个简单的封装，你也可以基于文件、数据库、Redis 等底层存储封装而形成一个消息中间件。</p><p>消息中间件做为一个基础组件并没有想象中的那么简单，其背后还需要配套的管理运维整个生态的产品集。</p><p>自研还会有交接问题，如果文档不齐全、运作不规范将会带给新人噩梦般的体验。</p><p>是否真的有自研的必要?如果不是 KPI 的压迫可以先考虑下面这两个问题：</p><ul><li>目前市面上的消息中间件是否都真的无法满足目前的业务需求?</li><li>团队是否有足够的能力、人力、财力、精力来支持自研?</li></ul><p>很多人在做消息中间件选型时会参考网络上的很多对比类的文章，但是其专业性、严谨性、以及其政治立场问题都有待考证，需要带着怀疑的态度去审视这些文章。</p><p>比如有些文章会在没有任何限定条件及场景的情况下直接定义某款消息中间件最好。</p><p>还有些文章没有指明消息中间件版本及测试环境就来做功能和性能对比分析，诸如此类的文章都可以唾弃之。</p><p>消息中间件犹如小马过河，选择合适的才最重要。这需要贴合自身的业务需求，技术服务于业务，大体上可以根据上一节所提及的功能、性能等 6 个维度来一一进行筛选。更深层次的抉择在于你能否掌握其魂。</p><p>笔者鄙见：RabbitMQ 在于 Routing，而 Kafka 在于 Streaming，了解其根本对于自己能够对症下药选择到合适的消息中间件尤为重要。</p><p>消息中间件选型切忌一味的追求性能或者功能，性能可以优化，功能可以二次开发。</p><p>如果要在功能和性能方面做一个抉择的话，那么首选性能，因为总体上来说性能优化的空间没有功能扩展的空间大。然而看长期发展，生态又比性能以及功能都要重要。</p><p><strong>可靠性误区</strong></p><p>很多时候，可靠性方面也容易存在一个误区：想要找到一个产品来保证消息的绝对可靠，很不幸的是这世界上没有绝对的东西，只能说尽量趋于完美。</p><p>想要尽可能的保障消息的可靠性也并非单单只靠消息中间件本身，还要依赖于上下游，需要从生产端、服务端和消费端这 3 个维度去努力保证。</p><p>消息中间件选型还有一个考量标准就是尽量贴合团队自身的技术栈体系，虽然说没有蹩脚的消息中间件，只有蹩脚的程序员，但是让一个 C 栈的团队去深挖 PhxQueue 总比去深挖 Scala 编写的 Kafka 要容易的多。</p><p>消息中间件大道至简：一发一存一消费，没有最好的消息中间件，只有最合适的消息中间件。</p><p>原文地址：<a href="http://os.51cto.com/art/201901/589744.htm" target="_blank" rel="noopener">http://os.51cto.com/art/201901/589744.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中间件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何提高界面设计中的层次感？来看这篇超全面的总结</title>
      <link href="/2018/12/10/%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E7%95%8C%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%B1%82%E6%AC%A1%E6%84%9F%EF%BC%9F%E6%9D%A5%E7%9C%8B%E8%BF%99%E7%AF%87%E8%B6%85%E5%85%A8%E9%9D%A2%E7%9A%84%E6%80%BB%E7%BB%93/"/>
      <url>/2018/12/10/%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E7%95%8C%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%B1%82%E6%AC%A1%E6%84%9F%EF%BC%9F%E6%9D%A5%E7%9C%8B%E8%BF%99%E7%AF%87%E8%B6%85%E5%85%A8%E9%9D%A2%E7%9A%84%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>引领初中级设计师摆脱「纠结设计」的弯路，通过对层次感的了解可以将有限的技法用在最该用的地方。</p><p>注：本文最好搭配作者之前的文章阅读：</p><ol><li><a href="https://www.uisdc.com/collection-of-ui-design-details" target="_blank" rel="noopener">《高手私藏的 UI 细节设计，这篇全都给你整理好了！》</a></li><li><a href="https://www.uisdc.com/layout-design-guide-in-interface-design" target="_blank" rel="noopener">《超全面！用户界面设计中的「版式设计」全方位指南》</a></li><li><a href="https://www.uisdc.com/technique-psychoanalysis-designers-temperament" target="_blank" rel="noopener">《从技法到心理，深度分析优秀设计师该有的气质》</a><a id="more"></a><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-1.jpg" alt=""></li></ol><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-3.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-2.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-4.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-5.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-6.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-7.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-8.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-9.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/12/uisdc-jm-20181203-10.jpg" alt=""></p><p>原文地址：<a href="https://www.uisdc.com/interface-design-sense-of-hierarchy" target="_blank" rel="noopener">https://www.uisdc.com/interface-design-sense-of-hierarchy</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 产品经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超全面！用户界面设计中的「版式设计」全方位指南</title>
      <link href="/2018/12/10/%E8%B6%85%E5%85%A8%E9%9D%A2%EF%BC%81%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%AD%E7%9A%84%E3%80%8C%E7%89%88%E5%BC%8F%E8%AE%BE%E8%AE%A1%E3%80%8D%E5%85%A8%E6%96%B9%E4%BD%8D%E6%8C%87%E5%8D%97/"/>
      <url>/2018/12/10/%E8%B6%85%E5%85%A8%E9%9D%A2%EF%BC%81%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E8%AE%BE%E8%AE%A1%E4%B8%AD%E7%9A%84%E3%80%8C%E7%89%88%E5%BC%8F%E8%AE%BE%E8%AE%A1%E3%80%8D%E5%85%A8%E6%96%B9%E4%BD%8D%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>一篇较为系统与全面的版式讲解，从规范化的布局形式到平面构成在用户界面设计中的应用与体现。<br><a id="more"></a><br><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-1.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-2.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-3.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-4.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-5.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-6.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/09/uisdc-pb-20180910-7.jpg" alt=""></p><p>原文地址：<a href="https://www.uisdc.com/layout-design-guide-in-interface-design" target="_blank" rel="noopener">https://www.uisdc.com/layout-design-guide-in-interface-design</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 产品经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从技法到心理，深度分析优秀设计师该有的气质</title>
      <link href="/2018/12/10/%E4%BB%8E%E6%8A%80%E6%B3%95%E5%88%B0%E5%BF%83%E7%90%86%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90%E4%BC%98%E7%A7%80%E8%AE%BE%E8%AE%A1%E5%B8%88%E8%AF%A5%E6%9C%89%E7%9A%84%E6%B0%94%E8%B4%A8/"/>
      <url>/2018/12/10/%E4%BB%8E%E6%8A%80%E6%B3%95%E5%88%B0%E5%BF%83%E7%90%86%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90%E4%BC%98%E7%A7%80%E8%AE%BE%E8%AE%A1%E5%B8%88%E8%AF%A5%E6%9C%89%E7%9A%84%E6%B0%94%E8%B4%A8/</url>
      
        <content type="html"><![CDATA[<p>这篇文章细致地剖析了一名优秀的设计者，他的气质到底从何而来，从设计技法到设计心理，或许这才是最深层次的挖掘。<br><a id="more"></a><br><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-3.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-4.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-5.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-6.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-7.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-8.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-9.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-10.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-11.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-12.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-13.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-14.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-15.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-16.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-17.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-18.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-1.gif" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/04/uisdc-sjsqz-20180429-2.jpg" alt=""></p><p>原文地址：<a href="https://www.uisdc.com/technique-psychoanalysis-designers-temperament" target="_blank" rel="noopener">https://www.uisdc.com/technique-psychoanalysis-designers-temperament</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 产品经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高手私藏的UI细节设计，这篇全都给你整理好了</title>
      <link href="/2018/12/10/%E9%AB%98%E6%89%8B%E7%A7%81%E8%97%8F%E7%9A%84UI%E7%BB%86%E8%8A%82%E8%AE%BE%E8%AE%A1%EF%BC%8C%E8%BF%99%E7%AF%87%E5%85%A8%E9%83%BD%E7%BB%99%E4%BD%A0%E6%95%B4%E7%90%86%E5%A5%BD%E4%BA%86/"/>
      <url>/2018/12/10/%E9%AB%98%E6%89%8B%E7%A7%81%E8%97%8F%E7%9A%84UI%E7%BB%86%E8%8A%82%E8%AE%BE%E8%AE%A1%EF%BC%8C%E8%BF%99%E7%AF%87%E5%85%A8%E9%83%BD%E7%BB%99%E4%BD%A0%E6%95%B4%E7%90%86%E5%A5%BD%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<p>这篇文章渗透进页面中的每一个 Kit控件，深入的分析每一个控件所能带给用户的视觉以及心理感受。<br><a id="more"></a><br><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-1.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-2.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-3.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-4.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-5.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-6.jpg" alt=""></p><p><img src="https://image.uisdc.com/wp-content/uploads/2018/10/uisdc-ui-20181011-77.jpg" alt=""></p><p>原文地址：<a href="https://www.uisdc.com/collection-of-ui-design-details" target="_blank" rel="noopener">https://www.uisdc.com/collection-of-ui-design-details</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 产品经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络入门（转载）</title>
      <link href="/2018/12/07/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/12/07/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>眼下最热门的技术，绝对是人工智能。</p><p>人工智能的底层模型是<a href="http://baike.baidu.com/item/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">“神经网络”</a>（neural network）。许多复杂的应用（比如模式识别、自动控制）和高级模型（比如深度学习）都基于它。学习人工智能，一定是从它开始。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071201.jpg" alt=""><br><a id="more"></a><br>什么是神经网络呢？网上似乎<a href="https://www.zhihu.com/question/22553761" target="_blank" rel="noopener">缺乏</a>通俗的解释。</p><p>前两天，我读到 Michael Nielsen 的开源教材<a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="noopener">《神经网络与深度学习》</a>（Neural Networks and Deep Learning），意外发现里面的解释非常好懂。下面，我就按照这本书，介绍什么是神经网络。</p><p>这里我要感谢<a href="http://cn.udacity.com/?utm_source=ruanyfarticle&amp;utm_medium=referral&amp;utm_campaign=FEND05" target="_blank" rel="noopener">优达学城</a>的赞助，本文<a href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html#support" target="_blank" rel="noopener">结尾</a>有他们的<a href="http://cn.udacity.com/course/front-end-web-developer-nanodegree--nd001-cn-advanced/?utm_source=ruanyfarticle&amp;utm_medium=referral&amp;utm_campaign=FEND05" target="_blank" rel="noopener">《前端开发（进阶）》</a>课程的消息，欢迎关注。</p><h2 id="一、感知器"><a href="#一、感知器" class="headerlink" title="一、感知器"></a>一、感知器</h2><p>历史上，科学家一直希望模拟人的大脑，造出可以思考的机器。人为什么能够思考？科学家发现，原因在于人体的神经网络。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071212.png" alt=""></p><blockquote><ol><li>外部刺激通过神经末梢，转化为电信号，转导到神经细胞（又叫神经元）。</li><li>无数神经元构成神经中枢。</li><li>神经中枢综合各种信号，做出判断。</li><li>人体根据神经中枢的指令，对外部刺激做出反应。</li></ol></blockquote><p>既然思考的基础是神经元，如果能够”人造神经元”（artificial neuron），就能组成人工神经网络，模拟思考。上个世纪六十年代，提出了最早的”人造神经元”模型，叫做<a href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8" target="_blank" rel="noopener">“感知器”</a>（perceptron），直到今天还在用。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071202.png" alt=""></p><p>上图的圆圈就代表一个感知器。它接受多个输入（x1，x2，x3…），产生一个输出（output），好比神经末梢感受各种外部环境的变化，最后产生电信号。</p><p>为了简化模型，我们约定每种输入只有两种可能：1 或 0。如果所有输入都是1，表示各种条件都成立，输出就是1；如果所有输入都是0，表示条件都不成立，输出就是0。</p><h2 id="二、感知器的例子"><a href="#二、感知器的例子" class="headerlink" title="二、感知器的例子"></a>二、感知器的例子</h2><p>下面来看一个例子。城里正在举办一年一度的游戏动漫展览，小明拿不定主意，周末要不要去参观。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071213.jpg" alt=""></p><p>他决定考虑三个因素。</p><blockquote><ol><li>天气：周末是否晴天？</li><li>同伴：能否找到人一起去？</li><li>价格：门票是否可承受？</li></ol></blockquote><p>这就构成一个感知器。上面三个因素就是外部输入，最后的决定就是感知器的输出。如果三个因素都是 Yes（使用<code>1</code>表示），输出就是1（去参观）；如果都是 No（使用<code>0</code>表示），输出就是0（不去参观）。</p><h2 id="三、权重和阈值"><a href="#三、权重和阈值" class="headerlink" title="三、权重和阈值"></a>三、权重和阈值</h2><p>看到这里，你肯定会问：如果某些因素成立，另一些因素不成立，输出是什么？比如，周末是好天气，门票也不贵，但是小明找不到同伴，他还要不要去参观呢？</p><p>现实中，各种因素很少具有同等重要性：某些因素是决定性因素，另一些因素是次要因素。因此，可以给这些因素指定权重（weight），代表它们不同的重要性。</p><blockquote><ul><li>天气：权重为8</li><li>同伴：权重为4</li><li>价格：权重为4</li></ul></blockquote><p>上面的权重表示，天气是决定性因素，同伴和价格都是次要因素。</p><p>如果三个因素都为1，它们乘以权重的总和就是 8 + 4 + 4 = 16。如果天气和价格因素为1，同伴因素为0，总和就变为 8 + 0 + 4 = 12。</p><p>这时，还需要指定一个阈值（threshold）。如果总和大于阈值，感知器输出1，否则输出0。假定阈值为8，那么 12 &gt; 8，小明决定去参观。阈值的高低代表了意愿的强烈，阈值越低就表示越想去，越高就越不想去。</p><p>上面的决策过程，使用数学表达如下。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071203.png" alt=""></p><p>上面公式中，<code>x</code>表示各种外部因素，<code>w</code>表示对应的权重。</p><h2 id="四、决策模型"><a href="#四、决策模型" class="headerlink" title="四、决策模型"></a>四、决策模型</h2><p>单个的感知器构成了一个简单的决策模型，已经可以拿来用了。真实世界中，实际的决策模型则要复杂得多，是由多个感知器组成的多层网络。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071205.png" alt=""></p><p>上图中，底层感知器接收外部输入，做出判断以后，再发出信号，作为上层感知器的输入，直至得到最后的结果。（注意：感知器的输出依然只有一个，但是可以发送给多个目标。）</p><p>这张图里，信号都是单向的，即下层感知器的输出总是上层感知器的输入。现实中，有可能发生循环传递，即 A 传给 B，B 传给 C，C 又传给 A，这称为<a href="https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">“递归神经网络”</a>（recurrent neural network），本文不涉及。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071211.png" alt=""></p><h2 id="五、矢量化"><a href="#五、矢量化" class="headerlink" title="五、矢量化"></a>五、矢量化</h2><p>为了方便后面的讨论，需要对上面的模型进行一些数学处理。</p><blockquote><ul><li>外部因素  <code>x1</code>、<code>x2</code>、<code>x3</code>  写成矢量  <code>&lt;x1, x2, x3&gt;</code>，简写为  <code>x</code></li><li>权重  <code>w1</code>、<code>w2</code>、<code>w3</code>  也写成矢量  <code>(w1, w2, w3)</code>，简写为  <code>w</code></li><li>定义运算  <code>w⋅x = ∑ wx</code>，即  <code>w</code>  和  <code>x</code>  的点运算，等于因素与权重的乘积之和</li><li>定义  <code>b</code>  等于负的阈值  <code>b = -threshold</code></li></ul></blockquote><p>感知器模型就变成了下面这样。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071206.png" alt=""></p><h2 id="六、神经网络的运作过程"><a href="#六、神经网络的运作过程" class="headerlink" title="六、神经网络的运作过程"></a>六、神经网络的运作过程</h2><p>一个神经网络的搭建，需要满足三个条件。</p><blockquote><ul><li>输入和输出</li><li>权重（<code>w</code>）和阈值（<code>b</code>）</li><li>多层感知器的结构</li></ul></blockquote><p>也就是说，需要事先画出上面出现的那张图。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071205.png" alt=""></p><p>其中，最困难的部分就是确定权重（<code>w</code>）和阈值（<code>b</code>）。目前为止，这两个值都是主观给出的，但现实中很难估计它们的值，必需有一种方法，可以找出答案。</p><p>这种方法就是试错法。其他参数都不变，<code>w</code>（或<code>b</code>）的微小变动，记作<code>Δw</code>（或<code>Δb</code>），然后观察输出有什么变化。不断重复这个过程，直至得到对应最精确输出的那组<code>w</code>和<code>b</code>，就是我们要的值。这个过程称为模型的训练。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071207.png" alt=""></p><p>因此，神经网络的运作过程如下。</p><blockquote><ol><li>确定输入和输出</li><li>找到一种或多种算法，可以从输入得到输出</li><li>找到一组已知答案的数据集，用来训练模型，估算<code>w</code>和<code>b</code></li><li>一旦新的数据产生，输入模型，就可以得到结果，同时对<code>w</code>和<code>b</code>进行校正</li></ol></blockquote><p>可以看到，整个过程需要海量计算。所以，神经网络直到最近这几年才有实用价值，而且一般的 CPU 还不行，要使用专门为机器学习定制的 GPU 来计算。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071214.jpg" alt=""></p><h2 id="七、神经网络的例子"><a href="#七、神经网络的例子" class="headerlink" title="七、神经网络的例子"></a>七、神经网络的例子</h2><p>下面通过车牌自动识别的例子，来解释神经网络。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071215.jpg" alt=""></p><p>所谓”车牌自动识别”，就是高速公路的探头拍下车牌照片，计算机识别出照片里的数字。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071216.jpg" alt=""></p><p>这个例子里面，车牌照片就是输入，车牌号码就是输出，照片的清晰度可以设置权重（<code>w</code>）。然后，找到一种或多种<a href="http://www.ruanyifeng.com/blog/2011/07/principle_of_similar_image_search.html" target="_blank" rel="noopener">图像比对算法</a>，作为感知器。算法的得到结果是一个概率，比如75%的概率可以确定是数字<code>1</code>。这就需要设置一个阈值（<code>b</code>）（比如85%的可信度），低于这个门槛结果就无效。</p><p>一组已经识别好的车牌照片，作为训练集数据，输入模型。不断调整各种参数，直至找到正确率最高的参数组合。以后拿到新照片，就可以直接给出结果了。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071217.png" alt=""></p><h2 id="八、输出的连续性"><a href="#八、输出的连续性" class="headerlink" title="八、输出的连续性"></a>八、输出的连续性</h2><p>上面的模型有一个问题没有解决，按照假设，输出只有两种结果：0和1。但是，模型要求<code>w</code>或<code>b</code>的微小变化，会引发输出的变化。如果只输出<code>0</code>和<code>1</code>，未免也太不敏感了，无法保证训练的正确性，因此必须将”输出”改造成一个连续性函数。</p><p>这就需要进行一点简单的数学改造。</p><p>首先，将感知器的计算结果<code>wx + b</code>记为<code>z</code>。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; </span><br><span class="line">&gt; z = wx + b</span><br><span class="line">&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>然后，计算下面的式子，将结果记为<code>σ(z)</code>。</p><blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; </span><br><span class="line">&gt; σ(z) = <span class="number">1</span> / (<span class="number">1</span> + e^(-z))</span><br><span class="line">&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>这是因为如果<code>z</code>趋向正无穷<code>z → +∞</code>（表示感知器强烈匹配），那么<code>σ(z) → 1</code>；如果<code>z</code>趋向负无穷<code>z → -∞</code>（表示感知器强烈不匹配），那么<code>σ(z) → 0</code>。也就是说，只要使用<code>σ(z)</code>当作输出结果，那么输出就会变成一个连续性函数。</p><p>原来的输出曲线是下面这样。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071208.png" alt=""></p><p>现在变成了这样。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071209.png" alt=""></p><p>实际上，还可以证明<code>Δσ</code>满足下面的公式。</p><p><img src="http://www.ruanyifeng.com/blogimg/asset/2017/bg2017071210.png" alt=""></p><p>即<code>Δσ</code>和<code>Δw</code>和<code>Δb</code>之间是线性关系，变化率是偏导数。这就有利于精确推算出<code>w</code>和<code>b</code>的值了。</p><p>（正文完）</p><p>原文地址：<a href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2017/07/neural-network.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新一代大数据与人工智能基础架构技术的发展与趋势</title>
      <link href="/2018/12/04/%E6%96%B0%E4%B8%80%E4%BB%A3%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E6%8A%80%E6%9C%AF%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%8E%E8%B6%8B%E5%8A%BF/"/>
      <url>/2018/12/04/%E6%96%B0%E4%B8%80%E4%BB%A3%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E6%8A%80%E6%9C%AF%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%8E%E8%B6%8B%E5%8A%BF/</url>
      
        <content type="html"><![CDATA[<p>2018年是开源软件历史上最精彩的一年，2个IPO加上5个并购，开源技术到达新的高度。回溯过去大数据时代的15年，开源技术的创新潮流，包括集群技术，流计算，数据库，容器技术和机器学习，可谓一波接着一波。<br><a id="more"></a><br>2003-2004年Google发布的GFS和MapReduce论文，对业界的影响可谓意义深远，它全面揭开了大数据的时代序幕。2006年，Doug Cutting加入Yahoo并发布Hadoop 0.1版本，同年Google发布BigTable论文。2008年，MySQL 10亿美元被Sun收购。2009年，Spark从Berkeley大学的AMPLab实验室诞生，程序员Johan Oskarsson举办第一个NoSQL数据库活动。2010年，RackSpace与NASA联合发起OpenStack云计算项目。2011年，451 Research分析师Matthew Aslett首次提出NewSQL概念。2012年，Red Hat首次达到10亿美元的年收入。2013年，Docker的崛起意味容器时代的开始。2014年， 随着Hadoop三驾马车之一的Hortonworks IPO，Hadoop到达高峰时期，但在同年Hadoop保持的记录被Spark打破。2015年，Google开源Tensorflow，Kubernetes 1.0发布并捐给CNCF组织。2017年， Hadoop 3.0正式发布。2018年，Cloudera和Hortonworks两大巨头合并，Elastic IPO，市值49亿美元。</p><p>这一幕幕精彩的故事背后隐藏着技术一次又一次的改革和创新，到底是上层业务应用的发展驱动底层基础架构技术的发展，还是基础架构的创新颠覆业务应用模式？下面我们一起回顾和预测这些关键技术的演变发展和未来趋势。</p><p><img src="http://s2.51cto.com/oss/201812/03/37838ecb6f2db5d7ed35ff604033d58f.jpg-wh_651x-s_1475789329.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/349747b955f54847f23d4c620eb1a5a4.jpg-wh_600x-s_3839100831.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/612a691b651dd8ca45b1fb7abb1a3abc.jpg-wh_600x-s_3749293788.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/9e616e33ab318c402d02184f5c20ff88.jpg-wh_600x-s_4029304846.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/dede99ed5cd094d4371eb53ee1f31fb0.jpg-wh_600x-s_2568445913.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/9f72eacb1e442e21437c1c0dbb93b0af.jpg-wh_600x-s_1273627498.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"><img src="http://s4.51cto.com/oss/201812/03/5838ccdc2359d47e08e01247299951b5.jpg-wh_600x-s_1607432063.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/63159dd5eeb76a8a03f5a28e37f96146.jpg-wh_600x-s_2240158432.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/fbcf3959bd75e9ba321ff6eb8ca4c56c.jpg-wh_600x-s_1178190839.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/7d305d02900148cb2546db685f13b846.jpg-wh_600x-s_228548920.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/aa1d0ec5d11ee843d9e8be03f0726c74.jpg-wh_600x-s_1669991102.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/7d8822c35358e8ea94b152a1d2394d3a.jpg-wh_600x-s_3760603095.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/364ccd2ea18714c3c89b58f2173d713b.jpg-wh_600x-s_3588934116.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/e334dbecf951c0b4cc5a0f69e02d1b9d.jpg-wh_600x-s_3397190841.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/0fbbb793740ae9ef73863f777d927eb8.jpg-wh_600x-s_2031823113.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/d77d2f1337b66d4e0f5bb48e84ed056b.jpg-wh_600x-s_1878772938.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/83aa556ce077885853465fb7356acd31.jpg-wh_600x-s_1663389395.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/8ddbcc4aa3c3b18ffaef126804e354f8.jpg-wh_600x-s_925838555.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/2c30f096b5e071a924574c35057f486e.jpg-wh_600x-s_167299430.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/36b6f0238bd6c01d7fb34eb688a45c20.jpg-wh_600x-s_2166568693.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/908f0478eabeede40dcbafc0100f94f7.jpg-wh_600x-s_1926970688.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/666f09820c5f8d7eeaab002738423a86.jpg-wh_600x-s_561290882.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/a39d71cd1d6507153e9fcad7d00d685c.jpg-wh_600x-s_646481320.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/86ebe38929dbb1b64885c5b6442f57b8.jpg-wh_600x-s_3418913684.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/f046ec1355e2c4bbd92cffab0a9de4b6.jpg-wh_600x-s_1495886493.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/4722c9aadf0d8988b7eb2f0fa297084a.jpg-wh_600x-s_99140833.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/020f221a26a4384d491e97a9329c922e.jpg-wh_600x-s_819301942.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/ce3cfc9fc8950de15db11a3aa649fbdf.jpg-wh_600x-s_2047622508.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/e0691d9e4032e9b126fe7e93b1d2b1d1.jpg-wh_600x-s_2548556028.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/182f3280733f15a8777b80da66bcd78a.jpg-wh_600x-s_375578151.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/b30843786f54b0703d04c682966523ca.jpg-wh_600x-s_2284094170.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/deee2b4329aaf6e3841bc14e75e1d4b5.jpg-wh_600x-s_2745199806.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/8046d1e5c69c9e8c32583e05c0f93df5.jpg-wh_600x-s_575015945.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/02889a648db7766e20fb5042b28427ef.jpg-wh_600x-s_764734545.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/5dd1e0afc1c5dc0ae0f1477986235075.jpg-wh_600x-s_2351627693.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/46f54fea56e12836a54edbe112cfbb47.jpg-wh_600x-s_1783306144.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/3f1dc05a732dc2aabc904467274bd3d5.jpg-wh_600x-s_405283744.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/1ed81aa2e360709db3ab4ba213ca5966.jpg-wh_600x-s_2888011704.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/c74418bf3949b9b182f5ed5da011805a.jpg-wh_600x-s_1863166041.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/6cc809b25ce3d6e95e9d362c651a3a09.jpg-wh_600x-s_2227008569.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/85b91d675c286eec0b2db5cacb63a71e.jpg-wh_600x-s_3421510576.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/dc0fcbf24511dd0422afa114458b42d7.jpg-wh_600x-s_3294829043.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/61ffde5c5bc5e24a1038dbba309d16ca.jpg-wh_600x-s_53003341.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/879588e99818aedcf6cdfaab7b7216a3.jpg-wh_600x-s_2273497091.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/2e8d7e54d30401f2c10ba1064027c24d.jpg-wh_600x-s_3552168640.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/df6afe38d41c80f60c2d2e0fb130a7e5.jpg-wh_600x-s_4069815753.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/21aa8fce3be8cf8052dcd754fe5a5076.jpg-wh_600x-s_3207869798.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/f1754961d99e1817717ade5389a7d538.jpg-wh_600x-s_1365055796.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/b9fa81ab3272ee7e2111120f976b886d.jpg-wh_600x-s_3531958893.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/a71f191e690b23532b631f2e143923dd.jpg-wh_600x-s_3203932873.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><a href="http://s1.51cto.com/oss/201812/04/9ec7df281ddae3eda5654c0c6c9212bd.jpg-wh_600x-s_449768349.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201812/04/9ec7df281ddae3eda5654c0c6c9212bd.jpg-wh_600x-s_449768349.jpg" alt=""></a></p><p><img src="http://s2.51cto.com/oss/201812/03/73ad5fe0d8755c6b8565c5d083c6906f.jpg-wh_600x-s_3063350141.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/5785c350951818875871a6ba1a09d215.jpg-wh_600x-s_1777349728.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/51fe96772a7738a64cd3c97fdd206ab7.jpg-wh_600x-s_2001858514.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/73d043027ebfd43d96330f47fd748720.jpg-wh_600x-s_2067620828.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/a4e096422e6baeb13533f871e43c691c.jpg-wh_600x-s_379167433.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/ba15b7ad44617390038a2fd0b490c552.jpg-wh_600x-s_3225131082.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/16a6cf48a1ffcbaa544678e45e7bcc18.jpg-wh_600x-s_423399093.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/ffc1fca17b138562cfcb9493229d59c6.jpg-wh_600x-s_2638637684.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/2f6f238228ea99c32d7b2c05ddbcac65.jpg-wh_600x-s_22927195.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/017a893796ad3795d0e6eecb5c382c07.jpg-wh_600x-s_159523507.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/16d9bf738f023f7022bf8ad74bdd0861.jpg-wh_600x-s_1450319879.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/691e6e03360d705bc2a478c1c2bc7fcc.jpg-wh_600x-s_3223424183.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/13261aa6820fea0414f288f2082f4dea.jpg-wh_600x-s_4248669072.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/252702fc86055a413d5d1c9b8b7b31ba.jpg-wh_600x-s_2022034996.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/94d1b2ec21a8bc95ea9006b3b495caeb.jpg-wh_600x-s_4061686210.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/74b1a7bb50eab3aee5b1c9dac5dc2e3e.jpg-wh_600x-s_3822000916.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/04d067474ef3a6ad879b85128a52c348.jpg-wh_600x-s_245270979.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/c733bbe4ff880f59a05b969e06e0683f.jpg-wh_600x-s_1791284164.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/4475949bdfb01defc7bb961c80c56659.jpg-wh_600x-s_1080875730.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/c805a309ffa7a60049f272d02797cae0.jpg-wh_600x-s_3995011078.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/796feafcbe59f98a60d88ee5c324ec50.jpg-wh_600x-s_1104289095.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/7866de523e620895940dc94473856c8d.jpg-wh_600x-s_3088280945.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/5c24d9de8158c1613887f2e7c7fab368.jpg-wh_600x-s_911459943.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/9820d969904f5f51234f3f0a9a69917c.jpg-wh_600x-s_366206124.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/b5e287a47c45b74952fa5cab12755fe7.jpg-wh_600x-s_3497140040.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/aa122fa86c9b689bec264474220182ec.jpg-wh_600x-s_376547459.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/0f4fa00bdb3152601385e1be23d5d531.jpg-wh_600x-s_1891887428.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/f7a1ebb4170179094f04944efc60db01.jpg-wh_600x-s_1939212157.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/54c3202d08cefa7b6146ab6c18673f27.jpg-wh_600x-s_2939624701.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/a669292e9fa726e775d75620f729e9bd.jpg-wh_600x-s_1917814505.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/4507944eee2e870aa4498f7ce5c843c7.jpg-wh_600x-s_3574845472.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/0a35e2817b453763063a3569173c584f.jpg-wh_600x-s_3396682584.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/0f495da8e9ba6cad012708abb359c669.jpg-wh_600x-s_2690281916.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/dae4899d353e7c00ba29fd411eeb05e5.jpg-wh_600x-s_2414696596.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/e14ae84da9dbccce22172b470ff7a361.jpg-wh_600x-s_718033460.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/df573379be2a23d933a310fcc310dbe1.jpg-wh_600x-s_2496733932.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/9eedf4f3d66aba3f664aa054340fe37d.jpg-wh_600x-s_282647006.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/f64a0dee6e1fa91c473c9d7c374fefa9.jpg-wh_600x-s_2553668020.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/6ac1222a4779cba6a94079e2c542d258.jpg-wh_600x-s_3162678429.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/3e1bc2598e1a643318ff921ac6a0cee7.jpg-wh_600x-s_2790845218.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/9ca8051d16fd11df7d115c2463da665d.jpg-wh_600x-s_3084866616.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/ae8ec84a93a29a8284dc99508001a026.jpg-wh_600x-s_3851277896.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/5c8f0235f9ce21a22492ffd81151a1a9.jpg-wh_600x-s_3763044451.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/92b3e4b111d2397077f9f6029a35d73e.jpg-wh_600x-s_1499590103.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/bf9fee8707f6dd49a00e189eda9469ca.jpg-wh_600x-s_783850480.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s4.51cto.com/oss/201812/03/1b685bf97abb138ece6e0058b24bb8dc.jpg-wh_600x-s_2158892310.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/a6942c718d6d231b1b84389b6b8d1513.jpg-wh_600x-s_1533667343.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/a457ca10549cab49777a260525c1531f.jpg-wh_600x-s_758438530.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/725294a33cdccc7417276516fabdd824.jpg-wh_600x-s_2428400491.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/78adb38804b1be8ab79ed90727841c6b.jpg-wh_600x-s_3180240288.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><a href="http://s3.51cto.com/oss/201812/03/394f356d6dfc8aec7e653c12cd390d6a.jpg-wh_600x-s_2154119360.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201812/03/394f356d6dfc8aec7e653c12cd390d6a.jpg-wh_600x-s_2154119360.jpg" alt=""></a></p><p><img src="http://s1.51cto.com/oss/201812/03/16cf9803fab44faefc89d15bb5b5cdf0.jpg-wh_600x-s_4120322407.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/5aca78a45f834540e25b6377d8832f70.jpg-wh_600x-s_1956984162.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/7b44e4fdb24b0c5e9f7cfbcd92ebc9fb.jpg-wh_600x-s_95411010.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><a href="http://s4.51cto.com/oss/201812/03/f511ecac96bc9edb95f2d4f1dbae7bf4.jpg-wh_600x-s_1969925563.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201812/03/f511ecac96bc9edb95f2d4f1dbae7bf4.jpg-wh_600x-s_1969925563.jpg" alt=""></a></p><p><img src="http://s5.51cto.com/oss/201812/03/6b1302b30d403351605a9926f4ef6f49.jpg-wh_600x-s_2249302930.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/21f52d15ea5fef2140d939d04a81f986.jpg-wh_600x-s_358561612.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s2.51cto.com/oss/201812/03/238db8c591c5a89b0bf5c448f92c7c64.jpg-wh_600x-s_3478646821.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s1.51cto.com/oss/201812/03/1fbb6fa69e28c3429cbc7e35dd4743de.jpg-wh_600x-s_2719839704.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s5.51cto.com/oss/201812/03/b9bdb2f581e935ba4b0207ef2a69f6cb.jpg-wh_600x-s_1389386807.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p><img src="http://s3.51cto.com/oss/201812/03/d240849c36889122d5fe6e901717ce77.jpg-wh_600x-s_2911413713.jpg" alt="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势" title="【113张PPT】新一代大数据与人工智能基础架构技术的发展与趋势"></p><p>原文地址：<a href="http://bigdata.51cto.com/art/201812/588089.htm" target="_blank" rel="noopener">http://bigdata.51cto.com/art/201812/588089.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文看懂区块链的3种类型：公有链、私有链和联盟链</title>
      <link href="/2018/12/03/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E5%8C%BA%E5%9D%97%E9%93%BE%E7%9A%843%E7%A7%8D%E7%B1%BB%E5%9E%8B%EF%BC%9A%E5%85%AC%E6%9C%89%E9%93%BE%E3%80%81%E7%A7%81%E6%9C%89%E9%93%BE%E5%92%8C%E8%81%94%E7%9B%9F%E9%93%BE/"/>
      <url>/2018/12/03/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E5%8C%BA%E5%9D%97%E9%93%BE%E7%9A%843%E7%A7%8D%E7%B1%BB%E5%9E%8B%EF%BC%9A%E5%85%AC%E6%9C%89%E9%93%BE%E3%80%81%E7%A7%81%E6%9C%89%E9%93%BE%E5%92%8C%E8%81%94%E7%9B%9F%E9%93%BE/</url>
      
        <content type="html"><![CDATA[<p>在具体介绍公有链、私有链和联盟链之前，我们先从最简单的字面意思上，对这几个概念有个大致了解：</p><p>＊公有链（Public Blockchain）：公有的区块链，读写权限对所有人开放。</p><p>＊私有链（Private Blockchain）：私有的区块链，读写权限对某个节点控制。</p><p>＊联盟链（Consortium Blockchain）：联盟区块链，读写权限对加入联盟的节点开放。</p><p><strong>它们的区别在读写权限以及去中心化的程度。一般情况下, 去中心化的程度越高，可信度越高，而交易速度越慢。</strong><br><a id="more"></a><br><strong>▶ 公有链</strong></p><p><strong>代表：</strong>比特币块链、以太坊智能合约</p><p>公有链的验证节点遍布于世界各地，所有人共同参与记账、维护区块链上的所有交易数据。</p><p>公有链能够稳定运行，得益于特定的共识机制，例如比特币块链依赖工作量证明（PoW）、以太坊目前依赖权益证明（PoS）等，其中Token（代币，也有人称“通证”）能够激励所有参与节点“愿意主动合作”，共同维护链上数据的安全性。因此，公有链的运行离不开代币。</p><p><strong>优点［1］：</strong></p><p><strong>1）所有交易数据公开、透明。</strong></p><p>虽然公有链上所有节点是匿名（更确切一点，“非实名”）加入网络，但任何节点都可以查看其他节点的账户余额以及交易活动。</p><p><strong>2）无法篡改。</strong></p><p>公有链是高度去中心化的分布式账本，篡改交易数据几乎不可能实现，除非篡改者控制了全网51%的算力，以及超过5亿RMB的运作资金。区块链观察网（<a href="http://www.blockob.com）在《区块链是什么》一文中提到过这点。" target="_blank" rel="noopener">www.blockob.com）在《区块链是什么》一文中提到过这点。</a></p><p><strong>缺点：</strong></p><p><strong>1）低吞吐量（TPS）。</strong></p><p>高度去中心化和低吞吐量是公有链不得不面对的两难境地，例如最成熟的公有链——比特币块链——每秒只能处理7笔交易信息（按照每笔交易大小为250字节），高峰期能处理的交易笔数就更低了。</p><p><strong>2）交易速度缓慢。</strong></p><p>低吞吐量的必然带来缓慢的交易速度。比特币网络极度拥堵，有时一笔交易需要几天才能处理完毕，还需要缴纳几百块转账费。</p><p><strong>▶ 私有链</strong></p><p><strong>代表：</strong>蚂蚁金服</p><p>根据《2017全球区块链企业专利排行榜》，阿里巴巴以49件的专利总量排名第一，而这些专利均出自蚂蚁金服技术实验室。</p><p>私有链的读写权限掌握在某个组织或机构手里，由该组织根据自身需求决定区块链链的公开程度；适用于数据管理、审计等金融场景。</p><p><strong>优点：</strong></p><p><strong>1）更快的交易速度、更低的交易成本</strong></p><p>链上只有少量的节点也都具有很高的信任度，并不需要每个节点来验证一个交易。因此，相比需要通过大多数节点验证的公有链，私有链的交易速度更快，交易成本也更低。</p><p><strong>2）不容易被恶意攻击</strong></p><p>相比中心化数据库，私有链能够防止内部某个节点篡改数据。故意隐瞒或篡改数据的情况很容易被发现，发生错误时也能追踪错误来源。</p><p><strong>3）更好地保护组织自身的隐私，交易数据不会对全网公开。</strong></p><p><strong>缺点：</strong></p><p>区块链是构建社会信任的最佳解决方案，“去中心化”是区块链的核心价值。而由某个组织或机构控制的私有链与“去中心化”理念有所出入。如果过于中心化，那就跟其他中心化数据库没有太大区别。</p><p><strong>▶ 联盟链</strong></p><p><strong>代表：</strong>超级账本（Hyperledger）</p><p>超级账本基于透明和去中心化的分布式账本技术，联盟内成员（包括英特尔、埃森哲等）共同合作，通过创建分布式账本的公开标准，实现价值交换，十分适合应用于金融行业，以及能源、保险、物联网等其他行业。</p><p><img src="http://5b0988e595225.cdn.sohucs.com/images/20180315/6d3b1aa704324c5fb7d7fce988043aa0.jpeg" alt=""></p><p>联盟链由联盟内成员节点共同维护，节点通过授权后才能加入联盟网络。</p><p>联盟链是私有链的一种，只是私有程度不同，而且其权限设计要求比私有链更复杂；但联盟链比纯粹的私有链更具可信度。</p><p><strong>总结：</strong></p><p>在对可信度、安全性有很高要求，而对交易速度不苛求的落地场景，公有链更有发展潜力。</p><p>对于更加注重隐私保护、交易速度和内部监管等的落地应用，开发私有链或联盟链则更加合适。</p><p><strong>编者注：</strong></p><p>［1］区块链在高效率、去中心化和安全三个方面，只选其二，这就是区块链的“不可能三角”悖论。因此，无论是公有链、私有链，还是联盟链，都会存在这样或那样的不足，或者说——它们没有绝对的优劣，应该根据具体的落地应用去看待不同的区块链类型。</p><p>原文地址：<a href="http://www.sohu.com/a/225590266_100126066" target="_blank" rel="noopener">http://www.sohu.com/a/225590266_100126066</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 区块链 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从零开始构建人脸识别模型</title>
      <link href="/2018/11/27/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2018/11/27/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p><strong>01 介绍</strong></p><p>你是否意识到，每当你上传照片到Facebook上，平台都会用人脸识别算法来识别图片中的人物？目前还有一些政府在用人脸识别技术来识别和抓捕罪犯。此外，最常见的应用就是通过自己的脸部解锁手机。</p><p>计算机视觉的子领域应用得非常广泛，并且全球很多商业活动都已经从中获益。人脸识别模型的使用在接下来的几年内还会继续增长，所以一起来了解如何从零开始构建人脸识别模型吧！</p><p><a href="http://s4.51cto.com/oss/201811/27/09ff09dca5186952b360c2fb474ed002.jpg-wh_651x-s_4252756916.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/27/09ff09dca5186952b360c2fb474ed002.jpg-wh_651x-s_4252756916.jpg" alt=""></a><br><a id="more"></a><br>本文首先会介绍人脸识别模型的内部工作原理。随后结合一个简单的案例，我们将通过Python进行案例实践。在本文的最后部分，你将完成你的第一个人脸识别模型！</p><p><strong>02 理解人脸识别的工作原理</strong></p><p>为了理解人脸识别算法工作原理，我们首先来了解一下特征向量的概念。（译者注：此处的特征向量指机器学习的概念，不同于矩阵理论。）</p><p>每个机器学习算法都会将数据集作为输入，并从中学习经验。算法会遍历数据并识别数据中的模式。例如，假定我们希望识别指定图片中人物的脸，很多物体是可以看作模式的：</p><ul><li>脸部的长度/宽度。</li><li>由于图片比例会被调整，长度和高度可能并不可靠。然而，在放缩图片后，比例是保持不变的——脸部长度和宽度的比例不会改变。</li><li>脸部肤色。</li><li>脸上局部细节的宽度，如嘴，鼻子等。</li></ul><p>显而易见，此时存在一个模式——不同的脸有不同的维度，相似的脸有相似的维度。有挑战性的是需要将特定的脸转为数字，因为机器学习算法只能理解数字。表示一张脸的数字（或训练集中的一个元素）可以称为特征向量。一个特征向量包括特定顺序的各种数字。</p><p>举一个简单的例子，我们可以将一张脸映射到一个特征向量上。特征向量由不同的特征组成，如：</p><ul><li>脸的长度（cm）</li><li>脸的宽度（cm）</li><li>脸的平均肤色（R，G，B）</li><li>唇部宽度（cm）</li><li>鼻子长度（cm）</li></ul><p>当给定一个图片时，我们可以标注不同的特征并将其转化为如下的特征向量：</p><p><a href="http://s5.51cto.com/oss/201811/27/40031d838e899383787359552cef5d6e.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/27/40031d838e899383787359552cef5d6e.jpg" alt=""></a></p><p>如此一来，我们的图片现在被转化为一个向量，可以表示为(23.1,15.8,255,224,189,5.2,4.4)。当然我们还可以从图片中衍生出无数的其他特征（如，头发颜色，胡须，眼镜等）。然而在这个简单的例子中，我们只考虑这五个简单的特征。</p><p>现在，一旦我们将每个图片解码为特征向量，问题就变得更简单。明显地，当我们使用同一个人的两张面部图片时，提取的特征向量会非常相似。换言之，两个特征向量的“距离”就变得非常小。</p><p>此时机器学习可以帮我们完成两件事：</p><ul><li><strong>提取特征向量</strong>。由于特征过多，手动列出所有特征是非常困难的。一个机器学习算法可以自动标注很多特征。例如，一个复杂的特征可能是：鼻子长度和前额宽度的比例。手动列出所有的这些衍生特征是非常困难的。</li><li><strong>匹配算法</strong>：一旦得到特征向量，机器学习算法需要将新图片和语料库中的特征向量进行匹配。</li></ul><p>既然我们对人脸识别如何工作有了基本的理解，让我们运用一些广泛使用的Python库来搭建自己的人脸识别算法。</p><p><strong>03 案例学习</strong></p><p>首先给定一些人物脸部的图片——可能是一些名人，如Mark Zuckerberg, Warren Buffett, Bill Gates, Shah Rukh Khan等，并把这些人脸看作我们的语料库。现在，我们给定一些其他名人的新图片（“新人物”），并判断这些“新人物”是否在语料库中。</p><p>以下是语料库中的图片：</p><p><a href="http://s5.51cto.com/oss/201811/27/ec77be6b9dec0a42ee764fc51790e687.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/27/ec77be6b9dec0a42ee764fc51790e687.jpg" alt=""></a></p><p>如图所示，我们所列举的名人有Barack Obama, Bill Gates, Jeff Bezos, Mark Zuckerberg, Ray Dalio 和Shah Rukh Khan。</p><p>现在，假定“新人物”如下：</p><p><a href="http://s4.51cto.com/oss/201811/27/0fcd1f5f608a34e96a376429d4a80ad9.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/27/0fcd1f5f608a34e96a376429d4a80ad9.jpg" alt=""></a></p><p>▲注：以上所有图片均来自Google图片</p><p>显而易见，这是Shah Rukh Khan。然而对电脑来说，这个任务很有挑战性。因为对于我们来说，我们可以轻易地将图片的多种特征结合来判断这是哪个人物。然而对电脑而言，学习如何识别人脸是非常不直观的。</p><p>有一个神奇但是简单的python库封装了以上提及的内容——可以根据脸部特征生成特征向量并且知道如何区分不同的脸。这个python库叫做face_recognition。它应用了dlib——一个现代C++工具包，其中包含了一些机器学习算法来帮助完成复杂的基于C++的应用。</p><p>Python中的face_recognition库可以完成大量的任务：</p><ul><li>发现给定图片中所有的脸。</li><li>发现并处理图片中的脸部特征。</li><li>识别图片中的脸。</li><li>实时的人脸识别。</li></ul><p>接下来，我们将探讨其中的第三种任务——识别图片中的脸。</p><p>你可以在github的如下链接中获取face_recognition库的源代码。</p><p>链接：</p><p><a href="https://github.com/ageitgey/face_recognition" target="_blank" rel="noopener">https://github.com/ageitgey/face_recognition</a></p><p>事实上，这里有一些如何安装face_recognition库的指导。</p><p>链接：</p><p><a href="https://github.com/ageitgey/face_recognition#installation-optionshttps://github.com/ageitgey/face_recognition#installation-options" target="_blank" rel="noopener">https://github.com/ageitgey/face_recognition#installation-options</a></p><p>在你安装face_recognition之前，还需要安装dlib包。你可以从如下链接中找到安装dlib的指导。</p><p>链接：</p><p><a href="https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf" target="_blank" rel="noopener">https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf</a></p><p><strong>04 Python应用</strong></p><p>这部分包括使用face_recognition库搭建简单人脸识别系统的代码。这是一个应用操作的部分，我们将在下一部分解读代码来理解更多细节。</p><ol><li><h1 id="import-the-libraries"><a href="#import-the-libraries" class="headerlink" title="import the libraries"></a>import the libraries</h1></li><li>import os  </li><li>import face_recognition  </li><li><h1 id="make-a-list-of-all-the-available-images"><a href="#make-a-list-of-all-the-available-images" class="headerlink" title="make a list of all the available images"></a>make a list of all the available images</h1></li><li>images = os.listdir(‘images’)  </li><li><h1 id="load-your-image"><a href="#load-your-image" class="headerlink" title="load your image"></a>load your image</h1></li><li>image_to_be_matched = face_recognition.load_image_file(‘my_image.jpg’)  </li><li><h1 id="encoded-the-loaded-image-into-a-feature-vector"><a href="#encoded-the-loaded-image-into-a-feature-vector" class="headerlink" title="encoded the loaded image into a feature vector"></a>encoded the loaded image into a feature vector</h1></li><li>image_to_be_matched_encoded = face_recognition.face_encodings(  </li><li>image_to_be_matched)[0] </li><li><h1 id="iterate-over-each-image"><a href="#iterate-over-each-image" class="headerlink" title="iterate over each image"></a>iterate over each image</h1></li><li>for image in images:  </li><li><h1 id="load-the-image"><a href="#load-the-image" class="headerlink" title="load the image"></a>load the image</h1></li><li>current_image = face_recognition.load_image_file(“images/“ + image) </li><li><h1 id="encode-the-loaded-image-into-a-feature-vector"><a href="#encode-the-loaded-image-into-a-feature-vector" class="headerlink" title="encode the loaded image into a feature vector"></a>encode the loaded image into a feature vector</h1></li><li>current_image_encoded = face_recognition.face_encodings(current_image)[0] </li><li><h1 id="match-your-image-with-the-image-and-check-if-it-matches"><a href="#match-your-image-with-the-image-and-check-if-it-matches" class="headerlink" title="match your image with the image and check if it matches"></a>match your image with the image and check if it matches</h1></li><li>result = face_recognition.compare_faces( </li><li>[image_to_be_matched_encoded], current_image_encoded) </li><li><h1 id="check-if-it-was-a-match"><a href="#check-if-it-was-a-match" class="headerlink" title="check if it was a match"></a>check if it was a match</h1></li><li>if result[0] == True: </li><li>print “Matched: “ + image </li><li>else: </li><li>print “Not matched: “ + image </li></ol><p>文件结构如下：</p><p>facialrecognition：</p><ul><li>fr.py</li><li>my_image.jpg</li><li><p>images/</p></li><li><p>barack_obama.jpg</p></li><li>bill_gates.jpg</li><li>jeff_bezos.jpg</li><li>mark_zuckerberg.jpg</li><li>ray_dalio.jpg</li><li>shah_rukh_khan.jpg</li><li>warren_buffett.jpg</li></ul><p>我们的根目录，facialrecognition包括：</p><ul><li>fr.py的形式的人脸识别代码。</li><li>my_image.jpg – 即将被识别的图片(“新人物”)。</li><li>images/ – 语料库。</li></ul><p>如果你按照前文创建文件结构并执行代码，如下是你能得到的结果：</p><ol><li>Matched: shah_rukh_khan.jpg  </li><li>Not matched: warren_buffett.jpg  </li><li>Not matched: barack_obama.jpg  </li><li>Not matched: ray_dalio.jpg  </li><li>Not matched: bill_gates.jpg  </li><li>Not matched: jeff_bezos.jpg  </li><li>Not matched: mark_zuckerberg.jpg </li></ol><p>显而易见，新名人是Shah Rukh Khan 并且我们的人脸识别系统可以识别!</p><p><strong>05 理解Python代码</strong></p><p>现在让我们解读代码来，并理解其工作原理：</p><ol><li><h1 id="import-the-libraries-1"><a href="#import-the-libraries-1" class="headerlink" title="import the libraries"></a>import the libraries</h1></li><li>import os  </li><li>import face_recognition </li></ol><p>以上是引入操作。我们将通过已经建好的os库来读入语料库中的所有图片，并且通过face_recognition来完成算法部分。</p><ol><li><h1 id="make-a-list-of-all-the-available-images-1"><a href="#make-a-list-of-all-the-available-images-1" class="headerlink" title="make a list of all the available images"></a>make a list of all the available images</h1></li><li>images = os.listdir(‘images’) </li></ol><p>这个简单的代码将帮助我们识别语料库中所有图片的路径。一旦执行这些代码，我们可以得到：</p><ol><li>images = [‘shah_rukh_khan.jpg’, ‘warren_buffett.jpg’, ‘barack_obama.jpg’, ‘ray_dalio.jpg’, ‘bill_gates.jpg’, ‘jeff_bezos.jpg’, ‘mark_zuckerberg.jpg’] </li></ol><p>现在，以下代码将加载新人物的图片：</p><ol><li><h1 id="load-your-image-1"><a href="#load-your-image-1" class="headerlink" title="load your image"></a>load your image</h1></li><li>image_to_be_matched = face_recognition.load_image_file(‘my_image.jpg’) </li></ol><p>为了保证算法可以解析图片，我们将人物脸部图片转化为特征向量：</p><ol><li><h1 id="encoded-the-loaded-image-into-a-feature-vector-1"><a href="#encoded-the-loaded-image-into-a-feature-vector-1" class="headerlink" title="encoded the loaded image into a feature vector"></a>encoded the loaded image into a feature vector</h1></li><li>image_to_be_matched_encoded = face_recognition.face_encodings(  </li><li>image_to_be_matched)[0] </li></ol><p>剩余的代码相对简单：</p><ol><li><h1 id="iterate-over-each-image-1"><a href="#iterate-over-each-image-1" class="headerlink" title="iterate over each image"></a>iterate over each image</h1></li><li>for image in images:  </li><li><h1 id="load-the-image-1"><a href="#load-the-image-1" class="headerlink" title="load the image"></a>load the image</h1></li><li>current_image = face_recognition.load_image_file(“images/“ + image) </li><li><h1 id="encode-the-loaded-image-into-a-feature-vector-1"><a href="#encode-the-loaded-image-into-a-feature-vector-1" class="headerlink" title="encode the loaded image into a feature vector"></a>encode the loaded image into a feature vector</h1></li><li>current_image_encoded = face_recognition.face_encodings(current_image)[0] </li><li><h1 id="match-your-image-with-the-image-and-check-if-it-matches-1"><a href="#match-your-image-with-the-image-and-check-if-it-matches-1" class="headerlink" title="match your image with the image and check if it matches"></a>match your image with the image and check if it matches</h1></li><li>result = face_recognition.compare_faces( </li><li>[image_to_be_matched_encoded], current_image_encoded) </li><li><h1 id="check-if-it-was-a-match-1"><a href="#check-if-it-was-a-match-1" class="headerlink" title="check if it was a match"></a>check if it was a match</h1></li><li>if result[0] == True: </li><li>print “Matched: “ + image </li><li>else: </li><li>print “Not matched: “ + image </li></ol><p>此时，我们：</p><ul><li>对每个图像进行循环操作。</li><li>将图像解析为特征向量。</li><li>比较语料库中已经加载的图片和被识别的新人物图片。</li><li>如果两者匹配，我们就显示出来。如果不匹配，我们也要显示结果。</li></ul><p>如上所示，结果显示这个简单的人脸识别算法进行得很顺利。让我们尝试将my_image替换为另一个图片：</p><p><a href="http://s1.51cto.com/oss/201811/27/fefa32c5957b835454f4313b4427be9e.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/27/fefa32c5957b835454f4313b4427be9e.jpg" alt=""></a></p><p>当你再次运行这个算法，将会看到如下结果：</p><ol><li>Not matched: shah_rukh_khan.jpg  </li><li>Not matched: warren_buffett.jpg  </li><li>Not matched: barack_obama.jpg  </li><li>Not matched: ray_dalio.jpg  </li><li>Not matched: bill_gates.jpg  </li><li>Not matched: jeff_bezos.jpg  </li><li>Not matched: mark_zuckerberg.jpg </li></ol><p>很明显，系统没有将马云识别为以上的任何一个名人。这意味着我们的算法在以下方面都表现得很好：</p><ul><li>正确地识别那些在语料库中存储的人。</li><li>对语料库中不存在的人物进行标注。</li></ul><p><strong>06 人脸识别算法的应用</strong></p><p>人脸识别是一个成熟的研究方向，已被广泛地应用在工业界和学术界。例如，一个罪犯在中国被捕可能就得益于人脸识别系统：系统识别了他的脸并发出警报。由此可见，面部识别可以用来减少犯罪。还有许多其他有趣的人脸识别案例:</p><ul><li>面部身份验证：Apple在iPhones中引入了Face ID以用于面部身份验证。一些银行也尝试使用面部身份验证来解锁。</li><li>用户服务：马来西亚的一些银行安装了新的人脸识别系统，用于识别有价值的银行客户，以便银行为其提供个人服务。进而银行可以通过维持这类用户并提升用户满意度来获取更多收益。</li><li>保险行业：很多保险公司正在通过运用人脸识别系统来匹配人的脸和ID提供的照片，使赔付过程变得更简单。</li></ul><p><strong>07 尾记</strong></p><p>综上所述，人脸识别是一个有趣的问题并且有很多强大的案例。这些应用可以有效地从各个方面为社会服务。尽管将这些技术商业化可能会带来伦理风险，但我们会把这个问题留到下次讨论。</p><p>希望你能从本文中有所收获。</p><p>原文标题：</p><p>Simple Introduction to Facial Recognition (with Python codes)</p><p>原文链接：</p><p><a href="https://www.analyticsvidhya.com/blog/2018/08/a-simple-introduction-to-facial-recognition-with-python-codes/" target="_blank" rel="noopener">https://www.analyticsvidhya.com/blog/2018/08/a-simple-introduction-to-facial-recognition-with-python-codes/</a></p><p>关于译者：王雨桐，统计学在读，数据科学硕士预备，跑步不停，弹琴不止。梦想把数据可视化当作艺术，目前日常是摸着下巴看机器学习。</p><p>原文地址：<a href="http://developer.51cto.com/art/201811/587696.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201811/587696.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读完这篇，你一定能真正理解Redis持久化</title>
      <link href="/2018/11/27/%E8%AF%BB%E5%AE%8C%E8%BF%99%E7%AF%87%EF%BC%8C%E4%BD%A0%E4%B8%80%E5%AE%9A%E8%83%BD%E7%9C%9F%E6%AD%A3%E7%90%86%E8%A7%A3Redis%E6%8C%81%E4%B9%85%E5%8C%96/"/>
      <url>/2018/11/27/%E8%AF%BB%E5%AE%8C%E8%BF%99%E7%AF%87%EF%BC%8C%E4%BD%A0%E4%B8%80%E5%AE%9A%E8%83%BD%E7%9C%9F%E6%AD%A3%E7%90%86%E8%A7%A3Redis%E6%8C%81%E4%B9%85%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p><strong>什么叫持久化?</strong></p><p>用一句话可以将持久化概括为：将数据(如内存中的对象)保存到可永久保存的存储设备中。</p><p>持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、 XML 数据文件中等等。</p><p>也可以从如下两个层面来理解持久化：</p><ul><li>应用层：如果关闭( Close )你的应用，然后重新启动则先前的数据依然存在。</li><li>系统层：如果关闭( Shut Down )你的系统(电脑)，然后重新启动则先前的数据依然存在。<a id="more"></a><strong>Redis 为什么要持久化?</strong></li></ul><p>Redis 中的数据类型都支持 Push/Pop、Add/Remove 及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。</p><p>在此基础上，Redis 支持各种不同方式的排序。与 Memcached 一样，为了保证效率，数据都是缓存在内存中。</p><p>因为数据都是缓存在内存中的，当你重启系统或者关闭系统后，缓存在内存中的数据都会消失殆尽，再也找不回来了。</p><p>所以，为了让数据能够长期保存，就要将 Redis 放在缓存中的数据做持久化存储。</p><p><a href="http://s2.51cto.com/oss/201811/27/64a7fb7cbea2aeaccad6b44e25d88f52.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/27/64a7fb7cbea2aeaccad6b44e25d88f52.jpg" alt=""></a></p><p><strong>Redis 怎么实现持久化?</strong></p><p>在设计之初，Redis 就已经考虑到了这个问题。官方提供了多种不同级别的数据持久化的方式：</p><ul><li>RDB 持久化方式能够在指定的时间间隔对你的数据进行快照存储。</li><li>AOF 持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，AOF 命令以 Redis 协议追加保存每次写的操作到文件末尾。</li></ul><p>Redis 还能对 AOF 文件进行后台重写，使得 AOF 文件的体积不至于过大。</p><ul><li>如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。</li><li>你也可以同时开启两种持久化方式，在这种情况下，当 Redis 重启的时候会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整。</li></ul><p>如果你不知道该选择哪一个级别的持久化方式，那我们就先来了解一下 AOF 方式和 RDB 方式有什么样的区别，并且它们各自有何优劣，学习完之后，再来考虑该选择哪一种级别。</p><p><strong>RDB 方式与 AOF 方式的优势对比</strong></p><p><strong>RDB 方式与 AOF 方式的优点对比</strong></p><p>首先我们来看一看官方对于两种方式的优点描述，并做个对比，然后再看一看两种方式的缺点描述。</p><p><a href="http://s1.51cto.com/oss/201811/27/0325749c24ebabdf22b20e3318e09398.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/27/0325749c24ebabdf22b20e3318e09398.jpg" alt=""></a></p><p>RDB 方式的优点：</p><ul><li>RDB 是一个非常紧凑的文件,它保存了某个时间点的数据集，非常适用于数据集的备份。</li><li>比如你可以在每个小时保存一下过去 24 小时内的数据，同时每天保存过去 30 天的数据，这样即使出了问题你也可以根据需求恢复到不同版本的数据集。</li><li>RDB 是一个紧凑的单一文件，很方便传送到另一个远端数据中心，非常适用于灾难恢复。</li><li>RDB 在保存 RDB 文件时父进程唯一需要做的就是 Fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，所以 RDB 持久化方式可以最大化 Redis 的性能。</li><li>与 AOF 相比，在恢复大的数据集的时候，RDB 方式会更快一些。</li></ul><p>当 Redis 需要保存 dump.rdb 文件时， 服务器执行以下操作：</p><ul><li>Redis 调用 Forks，同时拥有父进程和子进程。</li><li>子进程将数据集写入到一个临时 RDB 文件中。</li><li>当子进程完成对新 RDB 文件的写入时，Redis 用新 RDB 文件替换原来的 RDB 文件，并删除旧的 RDB 文件。</li></ul><p>这种工作方式使得 Redis 可以从写时复制(copy-on-write)机制中获益。</p><p><a href="http://s2.51cto.com/oss/201811/27/f4baad46a17f77fc25b81a24a0eae63a.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/27/f4baad46a17f77fc25b81a24a0eae63a.jpg" alt=""></a></p><p>AOF 方式的优点：</p><ul><li>使用 AOF 会让你的 Redis 更加耐久。</li><li>你可以使用不同的 Fsync 策略：无 Fsync、每秒 Fsync 、每次写的时候 Fsync 使用默认的每秒 Fsync 策略。</li></ul><p>Redis 的性能依然很好( Fsync 是由后台线程进行处理的，主线程会尽力处理客户端请求)，一旦出现故障，你最多丢失 1 秒的数据。</p><ul><li>AOF文件是一个只进行追加的日志文件，所以不需要写入 Seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也可使用 redis-check-aof 工具修复这些问题。</li><li>Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。</li></ul><p>整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。</p><p>而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。</p><ul><li>AOF 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 Redis 协议的格式保存。</li></ul><p>因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析(parse)也很轻松。导出(export) AOF 文件也非常简单。</p><p>举个例子，如果你不小心执行了 FLUSHALL 命令，但只要 AOF 文件未被重写，那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令，并重启 Redis ，就可以将数据集恢复到 FLUSHALL 执行之前的状态。</p><p>优点对比总结：</p><ul><li>RDB 方式可以保存过去一段时间内的数据，并且保存结果是一个单一的文件，可以将文件备份到其他服务器，并且在回复大量数据的时候，RDB 方式的速度会比 AOF 方式的回复速度要快。</li><li>AOF 方式默认每秒钟备份 1 次，频率很高，它的操作方式是以追加的方式记录日志而不是数据，并且它的重写过程是按顺序进行追加，所以它的文件内容非常容易读懂。</li></ul><p>可以在某些需要的时候打开 AOF 文件对其编辑，增加或删除某些记录，最后再执行恢复操作。</p><p><strong>RDB 方式与 AOF 方式的缺点对比</strong></p><p>RDB 方式的缺点：</p><ul><li>如果你希望在 Redis 意外停止工作(例如电源中断)的情况下丢失的数据最少的话，那么 RDB 不适合你。</li></ul><p>虽然你可以配置不同的 Save 时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作)，但是 Redis 要完整的保存整个数据集是一个比较繁重的工作。</p><p>你通常会每隔 5 分钟或者更久做一次完整的保存，万一 Redis 意外宕机，你可能会丢失几分钟的数据。</p><ul><li>RDB 需要经常 Fork 子进程来保存数据集到硬盘上，当数据集比较大的时，Fork 的过程是非常耗时的，可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。</li></ul><p>如果数据集巨大并且 CPU 性能不是很好的情况下，这种情况会持续 1 秒，AOF 也需要 Fork，但是你可以调节重写日志文件的频率来提高数据集的耐久度。</p><p>AOF 方式的缺点：</p><ul><li>对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。</li><li>根据所使用的 Fsync 策略，AOF 的速度可能会慢于 RDB。在一般情况下，每秒 Fsync 的性能依然非常高，而关闭 Fsync 可以让 AOF 的速度和 RDB 一样快，即使在高负荷之下也是如此。</li></ul><p>不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间(Latency)。</p><p>缺点对比总结：</p><ul><li>RDB 由于备份频率不高，所以在回复数据的时候有可能丢失一小段时间的数据，而且在数据集比较大的时候有可能对毫秒级的请求产生影响。</li><li>AOF 的文件提及比较大，而且由于保存频率很高，所以整体的速度会比 RDB 慢一些，但是性能依旧很高。</li></ul><p><strong>RDB 与 AOF 工作原理</strong></p><p><a href="http://s3.51cto.com/oss/201811/27/cbf5b40d1abbc791ebeaafb9a73a6533.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/27/cbf5b40d1abbc791ebeaafb9a73a6533.jpg" alt=""></a></p><p>AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制：</p><ul><li>Redis 执行 fork() ，现在同时拥有父进程和子进程。</li><li>子进程开始将新 AOF 文件的内容写入到临时文件。</li><li>对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾，这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。</li><li>当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。</li><li>现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。</li></ul><p><strong>付诸实践，RDB 与 AOF 的实现</strong></p><p><a href="http://s2.51cto.com/oss/201811/27/45b87b91865c5db9b53e2d83f0830107.gif" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/27/45b87b91865c5db9b53e2d83f0830107.gif" alt=""></a></p><p><strong>RDB 方式持久化的开启与配置</strong></p><p>Redis 默认的持久化方式是 RDB ，并且默认是打开的。RDB 的保存方式分为主动保存与被动保存。</p><p>主动保存可以在 redis-cli 中输入 Save 即可;被动保存需要满足配置文件中设定的触发条件，目前官方默认的触发条件可以在 redis.conf 中看到：</p><p>save 900 1save 300 10save 60 10000</p><p>其含义为：</p><p>服务器在900秒之内，对数据库进行了至少1次修改。服务器在300秒之内，对数据库进行了至少10次修改。服务器在60秒之内，对数据库进行了至少10000次修改。</p><p>满足触发条件后，数据就会被保存为快照，正是因为这样才说 RDB 的数据完整性是比不上 AOF 的。</p><p>触发保存条件后，会在指定的目录生成一个名为 dump.rdb 的文件，等到下一次启动 Redis 时，Redis 会去读取该目录下的 dump.rdb 文件，将里面的数据恢复到 Redis。</p><p>这个目录在哪里呢?我们可以在客户端中输入命令 config get dir 查看：</p><ol><li>gannicus@$ src/redis-cli </li><li>127.0.0.1:6379&gt; config get dir </li><li>1) “dir” </li><li>2) “/home/gannicus/Documents/redis-5.0.0” </li><li>127.0.0.1:6379&gt; </li></ol><p><a href="http://s1.51cto.com/oss/201811/27/32b1f5cc746e087e361f0dbe38d30023.gif-wh_600x-s_3694566268.gif" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/27/32b1f5cc746e087e361f0dbe38d30023.gif-wh_600x-s_3694566268.gif" alt=""></a></p><p>返回结果中的”/home/gannicus/Documents/redis-5.0.0”就是存放 dump.rdb 的目录。</p><p>在测试之前，说明一下前提：Redis 是直接从官网下载的压缩包，解压后得到 redis-x.x.x 文件夹。</p><p>比如我的是 redis-5.0.0，然后进入文件夹，在 redis-5.0.0 项目根目录使用 make 命令安装。</p><p><a href="http://s3.51cto.com/oss/201811/27/06334b7f36ad39c80f4f78819cb4c87b.gif-wh_600x-s_1720586276.gif" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/27/06334b7f36ad39c80f4f78819cb4c87b.gif-wh_600x-s_1720586276.gif" alt=""></a></p><p><strong>RDB 被动触发保存测试</strong></p><p>刚才提到它分为主动保存与被动触发，现在我们来测试一下被动触发。首先启动 redis-server，然后再打开客户端 redis-cli ，先增添几条记录：</p><p>127.0.0.1:6379&gt; set lca 1OK127.0.0.1:6379&gt; set lcb 1OK127.0.0.1:6379&gt; set lcc 1OK127.0.0.1:6379&gt; set lcd 1OK127.0.0.1:6379&gt; set lce 1OK127.0.0.1:6379&gt; set lcf 1OK127.0.0.1:6379&gt; set lcg 1OK127.0.0.1:6379&gt; set lch 1OK127.0.0.1:6379&gt; set lci 1OK127.0.0.1:6379&gt; set lcj 1OK127.0.0.1:6379&gt; set lck 1OK127.0.0.1:6379&gt; set lcl 1OK127.0.0.1:6379&gt; set lcm 1OK</p><p>可以看到，总共添加了 13 条记录：</p><p>127.0.0.1:6379&gt; keys * 1) “lca” 2) “lcd” 3) “lcg” 4) “lce” 5) “lcb” 6) “lcm” 7) “lcf” 8) “lci” 9) “lcl”10) “lcc”11) “lck”12) “lcj”13) “lch”127.0.0.1:6379&gt;</p><p>然后发现 redis-server 端的日志窗口中出现了如下的提示：</p><p>21971:M 21 Oct 2018 16:52:44.062 <em> 10 changes in 300 seconds. Saving…21971:M 21 Oct 2018 16:52:44.063 </em> Background saving started by pid 2255222552:C 21 Oct 2018 16:52:44.066 <em> DB saved on disk21971:M 21 Oct 2018 16:52:44.165 </em> Background saving terminated with success</p><p>从英文提示中可以大概读懂这些内容，它检测到 300 秒内有 10 条记录被改动，刚才我们添加了 13 条数据记录，满足 redis.conf 中对于 RDB 数据保存的条件。</p><p>所以这里执行数据保存操作，并且提示开辟了一个 22552 的进程出来执行保存操作，最后提示保存成功。并且在目录内看到有 dump.rdb 文件生成。</p><p>现在将 Redis 进程 Kill，哪些数据会被保存?通过命令 kill -9 pid ( pid 是进程编号)模拟 Redis 异常关闭，然后再启动 Redis 。</p><p>我们来看一看，到底是只保存了 10 条记录还是 13 条全都保存下来了?</p><p>127.0.0.1:6379&gt; keys * 1) “lcb” 2) “lcj” 3) “lcd” 4) “lch” 5) “lci” 6) “lcc” 7) “lcf” 8) “lce” 9) “lca”10) “lcg”127.0.0.1:6379&gt;</p><p>重启后查看记录，发现 13 条记录中只有 10 条记录会被保存，这也印证了之前所说，RDB 方式的数据完整性是不可靠的，除非断掉的那一刻正好是满足触发条件的条数。</p><p><strong>关闭 RDB</strong></p><p>刚才提到了，它是默认启用的，如果你不需要它可以在配置文件中将这 3 个配置注释掉，并新增 save “ “ 即可：</p><ol><li>save “” </li><li><h1 id="save-900-1"><a href="#save-900-1" class="headerlink" title="save 900 1"></a>save 900 1</h1></li><li><h1 id="save-300-10"><a href="#save-300-10" class="headerlink" title="save 300 10"></a>save 300 10</h1></li><li><h1 id="save-60-10000"><a href="#save-60-10000" class="headerlink" title="save 60 10000"></a>save 60 10000</h1></li></ol><p><a href="http://s5.51cto.com/oss/201811/27/2573c49536aa435a6e9ddb42646066f8.png-wh_600x-s_652721585.png" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/27/2573c49536aa435a6e9ddb42646066f8.png-wh_600x-s_652721585.png" alt=""></a></p><p>保存配置文件后需要重新启动 Redis 服务才会生效，然后继续添加十几条记录：</p><ol><li>127.0.0.1:6379&gt; keys * </li><li>1) “lcb” </li><li>… </li><li>23) “lca” </li><li>24) “lcg” </li><li>127.0.0.1:6379&gt; </li></ol><p>在之前已有 10 条的基础上我再增加了 14 条记录，这次同样要通过 kill 来模拟 Redis 异常关闭，再启动服务看一看，数据是否还被保存：</p><ol><li>127.0.0.1:6379&gt; keys * </li><li>1) “lcb” </li><li>2) “lcj” </li><li>3) “lcd” </li><li>4) “lch” </li><li>5) “lci” </li><li>6) “lcc” </li><li>7) “lcf” </li><li>8) “lce” </li><li>9) “lca” </li><li>10) “lcg” </li><li>127.0.0.1:6379&gt; </li></ol><p>发现后面添加的 14 条记录并没有被保存，恢复数据的时候仅仅只是恢复了之前的 10 条。</p><p>并且观察 Redis 服务端窗口日志，并未发现像之前一样的触发保存的提示，证明 RDB 方式已经被关闭。</p><p><strong>RDB 主动保存测试</strong></p><p>通过配置文件关闭被动触发，那么主动关闭是否还会生效呢?</p><p>在 Redis 客户端( redis-cli )通过 del 命令删除几条记录，然后输入 save 命令执行保存操作：</p><ol><li>127.0.0.1:6379&gt; keys * </li><li>1) “lcc” </li><li>2) “lch” </li><li>3) “lcb” </li><li>4) “lci” </li><li>5) “lce” </li><li>6) “lcj” </li><li>7) “lcg” </li><li>8) “lca” </li><li>9) “lcd” </li><li>10) “lcf” </li><li>127.0.0.1:6379&gt; del lca lcb lcc </li><li>(integer) 3 </li><li>127.0.0.1:6379&gt; save </li><li>OK </li><li>127.0.0.1:6379&gt; </li></ol><p><a href="http://s5.51cto.com/oss/201811/27/ad63a78005bd36bafa166b3d9f463f0f.gif-wh_600x-s_3503072780.gif" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/27/ad63a78005bd36bafa166b3d9f463f0f.gif-wh_600x-s_3503072780.gif" alt=""></a></p><p>可以看到 redis-server 的日志有新的提示：22598:M 21 Oct 2018 17:22:31.365 * DB saved on disk，它告诉我们数据已经保存。</p><p>那么继续模拟异常关闭，再打开服务，看一看是否真的保存了这些操作：</p><ol><li>127.0.0.1:6379&gt; keys * </li><li>1) “lci” </li><li>2) “lcj” </li><li>3) “lcd” </li><li>4) “lcg” </li><li>5) “lcf” </li><li>6) “lce” </li><li>7) “lch” </li><li>127.0.0.1:6379&gt; </li></ol><p><a href="http://s2.51cto.com/oss/201811/27/208cdb7dc8a4a82d15a342364ac64d2b.gif-wh_600x-s_2161094744.gif" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/27/208cdb7dc8a4a82d15a342364ac64d2b.gif-wh_600x-s_2161094744.gif" alt=""></a></p><p>果不其然，这几个删除操作都被保存了下来，恢复过来的数据中已经没有那 3 条记录了，证明主动关闭不受配置文件的影响。除了 Save 还有其他的保存方式么?</p><p><strong>Save 和 Bgsave 保存</strong></p><p>有的，Redis 提供了 Save 和 Bgsave 这两种不同的保存方式，并且这两个方式在执行的时候都会调用 rdbSave 函数。</p><p>但它们调用的方式各有不同：</p><ul><li>Save 直接调用 rdbSave方法 ，阻塞 Redis 主进程，直到保存完成为止。在主进程阻塞期间，服务器不能处理客户端的任何请求。</li><li>Bgsave 则 Fork 出一个子进程，子进程负责调用 rdbSave ，并在保存完成之后向主进程发送信号，通知保存已完成。</li></ul><p>因为 rdbSave 在子进程被调用，所以 Redis 服务器在 Bgsave 执行期间仍然可以继续处理客户端的请求。</p><p>Save 是同步操作，Bgsave 是异步操作。Bgsave 命令的使用方法和 Save 命令的使用方法是一样的：</p><ol><li>127.0.0.1:6379&gt; keys * </li><li>1) “lci” </li><li>2) “lcj” </li><li>3) “lcd” </li><li>4) “lcg” </li><li>5) “lcf” </li><li>6) “lce” </li><li>7) “lch” </li><li>127.0.0.1:6379&gt; del lci lcj </li><li>(integer) 2 </li><li>127.0.0.1:6379&gt; bgsave </li><li>Background saving started </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “lcd” </li><li>2) “lcg” </li><li>3) “lcf” </li><li>4) “lce” </li><li>5) “lch” </li><li>127.0.0.1:6379&gt; </li></ol><p><a href="http://s3.51cto.com/oss/201811/27/e248ff27424d6bf592f4268448ca75b6.gif-wh_600x-s_3968145510.gif" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/27/e248ff27424d6bf592f4268448ca75b6.gif-wh_600x-s_3968145510.gif" alt=""></a></p><p><strong>Shutdown 保存</strong></p><p>事实上，Shutdown 命令也是可以保存数据的，惊不惊喜。它会在关闭前将数据保存下来，意不意外?</p><ol><li>127.0.0.1:6379&gt; set app 1 </li><li>OK </li><li>127.0.0.1:6379&gt; set apps 1 </li><li>OK </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “apps” </li><li>2) “lcd” </li><li>3) “lcg” </li><li>4) “lcf” </li><li>5) “app” </li><li>6) “lce” </li><li>7) “lch” </li><li>127.0.0.1:6379&gt; shutdown </li><li>not connected&gt; quit </li><li>gannicus@$ </li></ol><p>然后 Redis 服务就被关闭掉了。我们需要重新启动 Redis 服务，到客户端中看一看是否生效：</p><ol><li>gannicus@$ src/redis-cli </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “lce” </li><li>2) “lcf” </li><li>3) “lcd” </li><li>4) “lch” </li><li>5) “lcg” </li></ol><p>竟然没有生效，刺不刺激?这是为什么呢?明明官方文档之 Shutdown 就说会保存了才退出的，你骗人~注意到，文档中有一句：</p><p><a href="http://s2.51cto.com/oss/201811/27/d988accd643e8bb8a9cd7f78fbdd941b.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/27/d988accd643e8bb8a9cd7f78fbdd941b.jpg" alt=""></a></p><p>恍然大悟，原来是要在持久化被打开的情况下，通过 Shutdown 命令关闭才不会丢失数据，那么就到配置文件中将那几个 Save 的配置项打开吧：</p><ol><li><h1 id="save-“”save-900-1"><a href="#save-“”save-900-1" class="headerlink" title="save “”save 900 1"></a>save “”save 900 1</h1></li><li>save 300 10 </li><li>save 60 10000 </li></ol><p>然后再开启 Redis 服务，再尝试一遍(过程为：添加 -&gt; shutdown -&gt; 重启服务 -&gt; 查看)：</p><ol><li>127.0.0.1:6379&gt; set app 1 </li><li>OK </li><li>127.0.0.1:6379&gt; set apps 1 </li><li>OK </li><li>127.0.0.1:6379&gt; shutdown </li><li>not connected&gt; quit </li><li>gannicus@$ src/redis-cli </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “lce” </li><li>2) “lch” </li><li>3) “app” </li><li>4) “lcf” </li><li>5) “apps” </li><li>6) “lcd” </li><li>7) “lcg” </li><li>127.0.0.1:6379&gt; </li></ol><p><a href="http://s5.51cto.com/oss/201811/27/286bcc0ebf534e550262b8c737125e6d.png-wh_600x-s_3032830573.png" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/27/286bcc0ebf534e550262b8c737125e6d.png-wh_600x-s_3032830573.png" alt=""></a></p><p>这下终于弄明白了。</p><p><a href="http://s4.51cto.com/oss/201811/27/a81c5dc7bfdd95c46c69e0a00a40e792.png-wh_600x-s_4117409205.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/27/a81c5dc7bfdd95c46c69e0a00a40e792.png-wh_600x-s_4117409205.png" alt=""></a></p><p><strong>AOF 方式持久化的开启与配置</strong></p><p><strong>开启 AOF</strong></p><p>默认是不开启 AOF 的，如果想要启用则需要到 redis.conf 配置文件中开启，打开 redis.conf：</p><ol><li>$ vim redis.conf </li></ol><p>然后在文件中找到 appendonly 并将 no 改为 yes：</p><ol><li>appendonly yes </li></ol><p>即为开启了 AOF 方式的持久化。</p><p><strong>设置同步方式</strong></p><p>AOF 还有支持几种同步方式，它们分别是：</p><ol><li>appendfsync always  # 每次有数据修改发生时都会写入AOF文件（安全但是费时）。 </li><li>appendfsync everysec  # 每秒钟同步一次，该策略为AOF的缺省策略。 </li><li>appendfsync no # 从不同步。高效但是数据不会被持久化。 </li></ol><p>默认配置是 everysec，你可以根据需求进行调整，这里我将配置改成 always：</p><ol><li>appendfsync always </li><li><h1 id="appendfsync-everysec"><a href="#appendfsync-everysec" class="headerlink" title="appendfsync everysec"></a>appendfsync everysec</h1></li><li><h1 id="appendfsync-no"><a href="#appendfsync-no" class="headerlink" title="appendfsync no"></a>appendfsync no</h1></li></ol><p>自定义 AOF 记录文件的文件名</p><p>Redis 设置有默认的文件名，在配置中显示为：</p><ol><li>appendfilename “appendonly.aof” </li></ol><p>你可以让其保持默认名字，也可以指定其他的文件名，比如：</p><ol><li>appendfilename “RNGLetme.aof” </li></ol><p><a href="http://s2.51cto.com/oss/201811/27/ee0f15b74c3269db86ae3547f35e3591.png-wh_600x-s_690977635.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/27/ee0f15b74c3269db86ae3547f35e3591.png-wh_600x-s_690977635.png" alt=""></a></p><p>将 appendonly、appendfsync 和 appendfilename 设置好并保存。重新启动 Redis 服务：</p><ol><li>$./redis-server </li></ol><p>通过命令 ls 查看本地文件，可以看到新生成了一个名为 RNGLetme.aof 的文件，可以使用：</p><ol><li>$cat RNGLetme.aof </li></ol><p>来查看里面的内容，由于当前未进行数据的改动，所以是空白的。然后打开 Redis 的客户端：</p><ol><li>$./redis-cli </li></ol><p>并且添加几条数据记录：</p><ol><li>127.0.0.1:6379&gt; set rng lpl </li><li>OK </li><li>127.0.0.1:6379&gt; set ig lpl </li><li>OK </li><li>127.0.0.1:6379&gt; set edg lpl </li><li>OK </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “edg” </li><li>2) “rng” </li><li>3) “ig” </li><li>127.0.0.1:6379&gt; </li></ol><p>可以看到，成功添加了 rng、edg、ig 这三条记录，然后打开 RNGLetme.aof 文件，看看里面的记录：</p><ol><li>*2 </li><li>$6 </li><li>SELECT </li><li>$1 </li><li>0 </li><li>*3 </li><li>$3 </li><li>set </li><li>$3 </li><li>rng </li><li>$3 </li><li>lpl </li><li>*3 </li><li>$3 </li><li>set </li><li>$2 </li><li>ig </li><li>$3 </li><li>lpl </li><li>*3 </li><li>$3 </li><li>set </li><li>$3 </li><li>edg </li><li>$3 </li><li>lpl </li></ol><p><a href="http://s1.51cto.com/oss/201811/27/a0a045368183950e1917428ccf03b16d.gif-wh_600x-s_3223891266.gif" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/27/a0a045368183950e1917428ccf03b16d.gif-wh_600x-s_3223891266.gif" alt=""></a></p><p>每一次的数据添加都被记录下来了。那如果是删除操作呢，也会被记录下来么?</p><ol><li>127.0.0.1:6379&gt; del edg </li><li>(integer) 1 </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “rng” </li><li>2) “ig” </li><li>127.0.0.1:6379&gt; </li></ol><p>执行完删除操作后，再看一看 RNGLetme.aof 文件中的记录：</p><p><img src="http://s2.51cto.com/oss/201811/27/2c47cb90226509513382a2607f0e4b1a.gif-wh_600x-s_4275075133.gif" alt=""></p><p>对比之前的记录，新增了 del edg 的操作记录。这就印证了之前对 AOF 的描述：以日志的方式将数据变动记录下来。</p><p><a href="http://s3.51cto.com/oss/201811/27/44a1ee945b2eab2455a5d4a52c8ab797.png-wh_600x-s_541861292.png" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/27/44a1ee945b2eab2455a5d4a52c8ab797.png-wh_600x-s_541861292.png" alt=""></a></p><p><strong>AOF 恢复测试</strong></p><p>下面同样是通过 Kill 命令模拟 Redis 异常关闭：</p><ol><li>gannicus@$ kill -9 22645 </li></ol><p>然后再重新启动 Redis 服务：</p><ol><li>$ src/redis-server redis.conf </li></ol><p>接着通过客户端看一看，那些数据是否都在：</p><ol><li>$ src/redis-cli </li><li>127.0.0.1:6379&gt; keys * </li><li>1) “ig” </li><li>2) “rng” </li></ol><p><a href="http://s4.51cto.com/oss/201811/27/b2616c714877fb3bc999b4dbfee2cda1.gif-wh_600x-s_3125308501.gif" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/27/b2616c714877fb3bc999b4dbfee2cda1.gif-wh_600x-s_3125308501.gif" alt=""></a></p><p>可以看到，rng 和 ig 都还在，意味着持久化是生效的。</p><p><strong>怎样从 RDB 方式切换为 AOF 方式</strong></p><p>在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换到 AOF ：</p><p>为最新的 dump.rdb 文件创建一个备份、将备份放到一个安全的地方。</p><p>执行以下两条命令：</p><ol><li>redis-cli config set appendonly yes </li><li>redis-cli config set save “” </li></ol><p>确保写命令会被正确地追加到 AOF 文件的末尾。执行的第一条命令开启了 AOF 功能：Redis 会阻塞直到初始 AOF 文件创建完成为止，之后 Redis 会继续处理命令请求，并开始将写入命令追加到 AOF 文件末尾。</p><p>执行的第二条命令用于关闭 RDB 功能。这一步是可选的，如果你愿意的话，也可以同时使用 RDB 和 AOF 这两种持久化功能。</p><p>注意：别忘了在 redis.conf 中打开 AOF 功能!否则服务器重启后，之前通过 CONFIG SET 命令设置的配置就会被遗忘，程序会按原来的配置来启动服务器。</p><p><strong>优先选择 RDB 还是 AOF 呢?</strong></p><p><a href="http://s1.51cto.com/oss/201811/27/5c80f3d98242e77f6a6449e3940e389b.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/27/5c80f3d98242e77f6a6449e3940e389b.jpg" alt=""></a></p><p>分析对比两种方式并做了测试后，发现这是两种不同风格的持久化方式。那么应该如何选择呢?</p><ul><li>对于企业级的中大型应用，如果不想牺牲数据完整性但是又希望保持高效率，那么你应该同时使用 RDB 和 AOF 两种方式。</li><li>如果你不打算耗费精力在这个地方，只需要保证数据完整性，那么优先考虑使用 AOF 方式。</li><li>RDB 方式非常适合大规模的数据恢复，如果业务对数据完整性和一致性要求不高，RDB 是很好的选择。</li></ul><p><strong>备份 Redis 数据的建议</strong></p><p>确保你的数据有完整的备份，磁盘故障、节点失效等问题可能让你的数据消失不见， 不进行备份是非常危险的。</p><p>Redis 对于数据备份是非常友好的，因为你可以在服务器运行的时候对 RDB 文件进行复制：RDB 文件一旦被创建，就不会进行任何修改。</p><p>当服务器要创建一个新的 RDB 文件时，它先将文件的内容保存在一个临时文件里面，当临时文件写入完毕时，程序才使用 rename(2) 原子地用临时文件替换原来的 RDB 文件。</p><p>这也就是说，无论何时，复制 RDB 文件都是绝对安全的：</p><ul><li>创建一个定期任务( cron job )，每小时将一个 RDB 文件备份到一个文件夹，并且每天将一个 RDB 文件备份到另一个文件夹。</li><li>确保快照的备份都带有相应的日期和时间信息，每次执行定期任务脚本时，使用 Find 命令来删除过期的快照：比如说你可以保留最近 48 小时内的每小时快照，还可以保留最近一两个月的每日快照。</li><li>至少每天一次，将 RDB 备份到你的数据中心之外，或者至少是备份到你运行 Redis 服务器的物理机器之外。</li></ul><p><strong>Redis 密码持久化</strong></p><p>在 Redis 中数据需要持久化，密码也要持久化。在客户端通过命令：</p><ol><li>config set requirepass zxc9527 </li></ol><p>可以为 Redis 设置值为 zxc9527 的密码，但是当 Redis 关闭并重新启动后，权限验证功能就会失效，再也不需要密码。</p><p>所以，密码也需要在 redis.conf 中持久化。打开 redis.conf 找到 requirepass 配置项，取消其注释并在后面设置密码：</p><ol><li>requirepass zxc9527 </li></ol><p><a href="http://s1.51cto.com/oss/201811/27/b29a2e1a15cba07a5723676e63b291bb.png-wh_600x-s_2657383200.png" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/27/b29a2e1a15cba07a5723676e63b291bb.png-wh_600x-s_2657383200.png" alt=""></a></p><p>保存后重启 Redis 服务，密码持久化即生效。</p><p>参考文章：</p><ul><li>Redis 源码剖析和注释(十七)— RDB 持久化机制</li><li>Redis 设计与实现</li><li><a href="http://www.redis.cn/" target="_blank" rel="noopener">www.redis.cn/</a></li><li>Redis 两种持久化方案 RDB 和 AOF 详解</li><li>Redis 持久化的几种方式</li><li>Redis 官方文档</li></ul><p>原文地址：<a href="http://developer.51cto.com/art/201811/587676.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201811/587676.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>美团即时物流的分布式系统架构设计</title>
      <link href="/2018/11/26/%E7%BE%8E%E5%9B%A2%E5%8D%B3%E6%97%B6%E7%89%A9%E6%B5%81%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
      <url>/2018/11/26/%E7%BE%8E%E5%9B%A2%E5%8D%B3%E6%97%B6%E7%89%A9%E6%B5%81%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>美团外卖已经发展了五年，即时物流探索也经历了3年多的时间，业务从零孵化到初具规模，在整个过程中积累了一些分布式高并发系统的建设经验。最主要的收获包括两点：</p><ol><li><p>即时物流业务对故障和高延迟的容忍度极低，在业务复杂度提升的同时也要求系统具备分布式、可扩展、可容灾的能力。即时物流系统阶段性的逐步实施分布式系统的架构升级，最终解决了系统宕机的风险。</p></li><li><p>围绕成本、效率、体验核心三要素，即时物流体系大量结合AI技术，从定价、ETA、调度、运力规划、运力干预、补贴、核算、语音交互、LBS挖掘、业务运维、指标监控等方面，业务突破结合架构升级，达到促规模、保体验、降成本的效果。</p><a id="more"></a>    </li></ol><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/01.png" alt=""></p><p>本文主要介绍在美团即时物流分布式系统架构逐层演变的进展中，遇到的技术障碍和挑战：</p><ul><li>订单、骑手规模大，供需匹配过程的超大规模计算问题。</li><li>遇到节假日或者恶劣天气，订单聚集效应，流量高峰是平常的十几倍。</li><li>物流履约是线上连接线下的关键环节，故障容忍度极低，不能宕机，不能丢单，可用性要求极高。</li><li>数据实时性、准确性要求高，对延迟、异常非常敏感。</li></ul><h2 id="美团即时物流架构"><a href="#美团即时物流架构" class="headerlink" title="美团即时物流架构"></a>美团即时物流架构</h2><p>美团即时物流配送平台主要围绕三件事展开：一是面向用户提供履约的SLA，包括计算送达时间ETA、配送费定价等；二是在多目标（成本、效率、体验）优化的背景下，匹配最合适的骑手；三是提供骑手完整履约过程中的辅助决策，包括智能语音、路径推荐、到店提醒等。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/02.png" alt=""></p><p>在一系列服务背后，是美团强大的技术体系的支持，并由此沉淀出的配送业务架构体系，基于架构构建的平台、算法、系统和服务。庞大的物流系统背后离不开分布式系统架构的支撑，而且这个架构更要保证高可用和高并发。</p><p>分布式架构，是相对于集中式架构而言的一种架构体系。分布式架构适用CAP理论（Consistency 一致性，Availability 可用性，Partition Tolerance 分区容忍性）。在分布式架构中，一个服务部署在多个对等节点中，节点之间通过网络进行通信，多个节点共同组成服务集群来提供高可用、一致性的服务。</p><p>早期，美团按照业务领域划分成多个垂直服务架构；随着业务的发展，从可用性的角度考虑做了分层服务架构。后来，业务发展越发复杂，从运维、质量等多个角度考量后，逐步演进到微服务架构。这里主要遵循了两个原则：不宜过早的进入到微服务架构的设计中，好的架构是演进出来的不是提前设计出来的。</p><h2 id="分布式系统实践"><a href="#分布式系统实践" class="headerlink" title="分布式系统实践"></a>分布式系统实践</h2><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/03.png" alt=""></p><p>上图是比较典型的美团技术体系下的分布式系统结构：依托了美团公共组件和服务，完成了分区扩容、容灾和监控的能力。前端流量会通过HLB来分发和负载均衡；在分区内，服务与服务会通过OCTO进行通信，提供服务注册、自动发现、负载均衡、容错、灰度发布等等服务。当然也可以通过消息队列进行通信，例如Kafka、RabbitMQ。在存储层使用Zebra来访问分布式数据库进行读写操作。利用<a href="https://tech.meituan.com/cat_pr.html" target="_blank" rel="noopener">CAT</a>（美团开源的分布式监控系统）进行分布式业务及系统日志的采集、上报和监控。分布式缓存使用Squirrel+Cellar的组合。分布式任务调度则是通过Crane。</p><p>在实践过程还要解决几个问题，比较典型的是集群的扩展性，有状态的集群可扩展性相对较差，无法快速扩容机器，无法缓解流量压力。同时，也会出现节点热点的问题，包括资源不均匀、CPU使用不均匀等等。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/04.png" alt=""></p><p>首先，配送后台技术团队通过架构升级，将有状态节点变成无状态节点，通过并行计算的能力，让小的业务节点去分担计算压力，以此实现快速扩容。</p><p>第二是要解决一致性的问题，对于既要写DB也要写缓存的场景，业务写缓存无法保障数据一致性，美团内部主要通过Databus来解决，Databus是一个高可用、低延时、高并发、保证数据一致性的数据库变更实时传输系统。通过Databus上游可以监控业务Binlog变更，通过管道将变更信息传递给ES和其他DB，或者是其他KV系统，利用Databus的高可用特性来保证数据最终是可以同步到其他系统中。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/05.png" alt=""></p><p>第三是我们一直在花精力解决的事情，就是保障集群高可用，主要从三个方面来入手，事前较多的是做全链路压测评，估峰值容量；周期性的集群健康性检查；随机故障演练（服务、机器、组件）。事中做异常报警（性能、业务指标、可用性）；快速的故障定位（单机故障、集群故障、IDC故障、组件异常、服务异常）；故障前后的系统变更收集。事后重点做系统回滚；扩容、限流、熔断、降级；核武器兜底。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/06.png" alt=""><br><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/07.png" alt=""></p><h3 id="单IDC的快速部署-amp-容灾"><a href="#单IDC的快速部署-amp-容灾" class="headerlink" title="单IDC的快速部署&amp;容灾"></a>单IDC的快速部署&amp;容灾</h3><p>单IDC故障之后，入口服务做到故障识别，自动流量切换；单IDC的快速扩容，数据提前同步，服务提前部署，Ready之后打开入口流量；要求所有做数据同步、流量分发的服务，都具备自动故障检测、故障服务自动摘除；按照IDC为单位扩缩容的能力。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/08.png" alt=""></p><h3 id="多中心尝试"><a href="#多中心尝试" class="headerlink" title="多中心尝试"></a>多中心尝试</h3><p>美团IDC以分区为单位，存在资源满排，分区无法扩容。美团的方案是多个IDC组成虚拟中心，以中心为分区的单位；服务无差别的部署在中心内；中心容量不够，直接增加新的IDC来扩容容量。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/09.png" alt=""></p><h3 id="单元化尝试"><a href="#单元化尝试" class="headerlink" title="单元化尝试"></a>单元化尝试</h3><p>相比多中心来说，单元化是进行分区容灾和扩容的更优方案。关于流量路由，美团主要是根据业务特点，采用区域或城市进行路由。数据同步上，异地会出现延迟状况。SET容灾上要保证同本地或异地SET出现问题时，可以快速把SET切换到其他SET上来承担流量。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/10.png" alt=""></p><h2 id="智能物流的核心技术能力和平台沉淀"><a href="#智能物流的核心技术能力和平台沉淀" class="headerlink" title="智能物流的核心技术能力和平台沉淀"></a>智能物流的核心技术能力和平台沉淀</h2><p>机器学习平台，是一站式线下到线上的模型训练和算法应用平台。之所以构建这个平台，目的是要解决算法应用场景多，重复造轮子的矛盾问题，以及线上、线下数据质量不一致。如果流程不明确不连贯，会出现迭代效率低，特征、模型的应用上线部署出现数据质量等障碍问题。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/11.png" alt=""></p><p>JARVIS是一个以稳定性保障为目标的智能化业务运维AIOps平台。主要用于处理系统故障时报警源很多，会有大量的重复报警，有效信息很容易被淹没等各种问题。此外，过往小规模分布式集群的运维故障主要靠人和经验来分析和定位，效率低下，处理速度慢，每次故障处理得到的预期不稳定，在有效性和及时性方面无法保证。所以需要AIOps平台来解决这些问题。</p><p><img src="https://tech.meituan.com/img/Instant_Logistics_Distributed_System_Architecture/12.png" alt=""></p><h2 id="未来的挑战"><a href="#未来的挑战" class="headerlink" title="未来的挑战"></a>未来的挑战</h2><p>经过复盘和Review之后，我们发现未来的挑战很大，微服务不再“微”了，业务复杂度提升之后，服务就会变得膨胀。其次，网状结构的服务集群，任何轻微的延迟，都可能导致的网络放大效应。另外复杂的服务拓扑，如何做到故障的快速定位和处理，这也是AIOps需要重点解决的难题。最后，就是单元化之后，从集群为单位的运维到以单元为单位的运维，业给美团业务部署能力带来很大的挑战。</p><h2 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a>作者简介</h2><p>宋斌，美团资深技术专家，长期参与分布式系统架构、高并发系统稳定性保障相关工作。目前担任即时物流团队后台技术负责人。2013年加入美团，参与过美团外卖C端、即时物流体系从零搭建。现在带领团队负责调度、清结算、LBS、定价等业务系统、算法数据平台、稳定性保障平台等技术平台的研发和运维。最近重点关注AIOps方向，探索在高并发、分布式系统架构下，如何更好的做好系统稳定性保障。</p><p>原文地址：<a href="https://tech.meituan.com/Instant_Logistics_Distributed_System_Architecture.html" target="_blank" rel="noopener">https://tech.meituan.com/Instant_Logistics_Distributed_System_Architecture.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详细的Docker入门总结（转载）</title>
      <link href="/2018/11/15/%E8%AF%A6%E7%BB%86%E7%9A%84Docker%E5%85%A5%E9%97%A8%E6%80%BB%E7%BB%93%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/11/15/%E8%AF%A6%E7%BB%86%E7%9A%84Docker%E5%85%A5%E9%97%A8%E6%80%BB%E7%BB%93%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p><strong>Docker 是什么?</strong></p><p>Docker 到底是个什么东西呢?我们在理解 Docker 之前，首先得先区分清楚两个概念，容器和虚拟机。</p><p>可能很多读者朋友都用过虚拟机，而对容器这个概念比较的陌生。我们用的传统虚拟机如 VMware ， VisualBox 之类的需要模拟整台机器包括硬件。</p><p>每台虚拟机都需要有自己的操作系统，虚拟机一旦被开启，预分配给它的资源将全部被占用。<br><a id="more"></a><br>每一台虚拟机包括应用，必要的二进制和库，以及一个完整的用户操作系统。</p><p>而容器技术是和我们的宿主机共享硬件资源及操作系统，可以实现资源的动态分配。</p><p>容器包含应用和其所有的依赖包，但是与其他容器共享内核。容器在宿主机操作系统中，在用户空间以分离的进程运行。<br><!--more--><br>容器技术是实现操作系统虚拟化的一种途径，可以让您在资源受到隔离的进程中运行应用程序及其依赖关系。</p><p>通过使用容器，我们可以轻松打包应用程序的代码、配置和依赖关系，将其变成容易使用的构建块，从而实现环境一致性、运营效率、开发人员生产力和版本控制等诸多目标。</p><p>容器可以帮助保证应用程序快速、可靠、一致地部署，其间不受部署环境的影响。</p><p>容器还赋予我们对资源更多的精细化控制能力，让我们的基础设施效率更高。</p><p>通过下面这幅图，我们可以很直观的反映出这两者的区别所在：</p><p><a href="http://s4.51cto.com/oss/201811/14/1e0bd50e3d65c76a2c951278560e6744.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/1e0bd50e3d65c76a2c951278560e6744.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。它是目前最流行的 Linux 容器解决方案。</p><p>而 Linux 容器是 Linux 发展出的另一种虚拟化技术，简单来讲， Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离，相当于是在正常进程的外面套了一个保护层。</p><p>对于容器里面的进程来说，它接触到的各种资源都是虚拟的，从而实现与底层系统的隔离。</p><p>Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。</p><p>程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker ，就不用担心环境问题。</p><p>总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。</p><p><strong>Docker 的优势</strong></p><p>Docker 相比于传统虚拟化方式具有更多的优势：</p><ul><li>Docker 启动快速属于秒级别。虚拟机通常需要几分钟去启动。</li><li>Docker 需要的资源更少。Docker 在操作系统级别进行虚拟化，Docker 容器和内核交互，几乎没有性能损耗，性能优于通过 Hypervisor 层与内核层的虚拟化。</li><li>Docker 更轻量。Docker 的架构可以共用一个内核与共享应用程序库，所占内存极小。同样的硬件环境，Docker 运行的镜像数远多于虚拟机数量，对系统的利用率非常高。</li><li>与虚拟机相比，Docker 隔离性更弱。Docker 属于进程之间的隔离，虚拟机可实现系统级别隔离。</li><li>安全性。Docker 的安全性也更弱，Docker 的租户 Root 和宿主机 Root 等同，一旦容器内的用户从普通用户权限提升为 Root 权限，它就直接具备了宿主机的 Root 权限，进而可进行无限制的操作。</li><li>虚拟机租户 Root 权限和宿主机的 Root 虚拟机权限是分离的，并且虚拟机利用如 Intel 的 VT-d 和 VT-x 的 ring-1 硬件隔离技术。</li><li>这种隔离技术可以防止虚拟机突破和彼此交互，而容器至今还没有任何形式的硬件隔离，这使得容器容易受到攻击。</li><li>可管理性。Docker 的集中化管理工具还不算成熟。各种虚拟化技术都有成熟的管理工具，例如 VMware vCenter 提供完备的虚拟机管理能力。</li><li>高可用和可恢复性。Docker 对业务的高可用支持是通过快速重新部署实现的。</li><li>虚拟化具备负载均衡，高可用，容错，迁移和数据保护等经过生产实践检验的成熟保障机制， VMware 可承诺虚拟机 99.999% 高可用，保证业务连续性。</li><li>快速创建、删除。虚拟化创建是分钟级别的，Docker 容器创建是秒级别的，Docker 的快速迭代性，决定了无论是开发、测试、部署都可以节约大量时间</li><li>交付、部署。虚拟机可以通过镜像实现环境交付的一致性，但镜像分发无法体系化。Docker 在 Dockerfile 中记录了容器构建过程，可在集群中实现快速分发和快速部署。</li></ul><p>我们可以从下面这张表格很清楚地看到容器相比于传统虚拟机的特性的优势所在：</p><p><a href="http://s5.51cto.com/oss/201811/14/7afe8872dffdecc351878e1ce42c3384.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/14/7afe8872dffdecc351878e1ce42c3384.jpg" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p><strong>Docker 的三个基本概念</strong></p><p><a href="http://s4.51cto.com/oss/201811/14/c6add06f6f7d91ca01b068e2a6ed4751.jpg-wh_600x-s_1235414922.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/c6add06f6f7d91ca01b068e2a6ed4751.jpg-wh_600x-s_1235414922.jpg" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>从上图我们可以看到，Docker 中包括三个基本的概念：</p><ul><li>Image(镜像)</li><li>Container(容器)</li><li>Repository(仓库)</li></ul><p>镜像是 Docker 运行容器的前提，仓库是存放镜像的场所，可见镜像更是 Docker 的核心。</p><p><strong>Image(镜像)</strong></p><p>那么镜像到底是什么呢?Docker 镜像可以看作是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数(如匿名卷、环境变量、用户等)。</p><p>镜像不包含任何动态数据，其内容在构建之后也不会被改变。镜像(Image)就是一堆只读层(read-only layer)的统一视角，也许这个定义有些难以理解，下面的这张图能够帮助读者理解镜像的定义：</p><p><a href="http://s3.51cto.com/oss/201811/14/c11aa4be2c219dc37659d24384e0066d.png" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/14/c11aa4be2c219dc37659d24384e0066d.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>从左边我们看到了多个只读层，它们重叠在一起。除了最下面一层，其他层都会有一个指针指向下一层。这些层是 Docker 内部的实现细节，并且能够在主机的文件系统上访问到。</p><p>统一文件系统(Union File System)技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角。</p><p>这样就隐藏了多层的存在，在用户的角度看来，只存在一个文件系统。我们可以在图片的右边看到这个视角的形式。</p><p><strong>Container(容器)</strong></p><p>容器(Container)的定义和镜像(Image)几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。</p><p><a href="http://s4.51cto.com/oss/201811/14/e2bd6eaa4a5fc14201648db225bd792d.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/e2bd6eaa4a5fc14201648db225bd792d.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>由于容器的定义并没有提及是否要运行容器，所以实际上，容器 = 镜像 + 读写层。</p><p><strong>Repository(仓库)</strong></p><p>Docker 仓库是集中存放镜像文件的场所。镜像构建完成后，可以很容易的在当前宿主上运行。</p><p>但是， 如果需要在其他服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry(仓库注册服务器)就是这样的服务。</p><p>有时候会把仓库(Repository)和仓库注册服务器(Registry)混为一谈，并不严格区分。</p><p>Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。</p><p>实际上，一个 Docker Registry 中可以包含多个仓库(Repository)，每个仓库可以包含多个标签(Tag)，每个标签对应着一个镜像。</p><p>所以说，镜像仓库是 Docker 用来集中存放镜像文件的地方，类似于我们之前常用的代码仓库。</p><p>通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。</p><p>我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 Latest 作为默认标签。</p><p>仓库又可以分为两种形式：</p><ul><li>Public(公有仓库)</li><li>Private(私有仓库)</li></ul><p>Docker Registry 公有仓库是开放给用户使用、允许用户管理镜像的 Registry 服务。</p><p>一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。</p><p>除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。</p><p>当用户创建了自己的镜像之后就可以使用 Push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 Pull 下来就可以了。</p><p>我们主要把 Docker 的一些常见概念如 Image，Container，Repository 做了详细的阐述，也从传统虚拟化方式的角度阐述了 Docker 的优势。</p><p>我们从下图可以直观地看到 Docker 的架构：</p><p><a href="http://s1.51cto.com/oss/201811/14/5acd0a58e31137b1d06a028c1b10f3ac.png" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/14/5acd0a58e31137b1d06a028c1b10f3ac.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>Docker 使用 C/S 结构，即客户端/服务器体系结构。Docker 客户端与 Docker 服务器进行交互，Docker服务端负责构建、运行和分发 Docker 镜像。</p><p>Docker 客户端和服务端可以运行在一台机器上，也可以通过 RESTful 、 Stock 或网络接口与远程 Docker 服务端进行通信。</p><p><a href="http://s3.51cto.com/oss/201811/14/cfdf87b8dc31cfab2c3c1a7557cfdceb.png-wh_600x-s_2564625788.png" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/14/cfdf87b8dc31cfab2c3c1a7557cfdceb.png-wh_600x-s_2564625788.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>这张图展示了 Docker 客户端、服务端和 Docker 仓库(即 Docker Hub 和 Docker Cloud )，默认情况下 Docker 会在 Docker 中央仓库寻找镜像文件。</p><p>这种利用仓库管理镜像的设计理念类似于 Git ，当然这个仓库是可以通过修改配置来指定的，甚至我们可以创建我们自己的私有仓库。</p><p><strong>Docker 的安装和使用</strong></p><p>Docker 的安装和使用有一些前提条件，主要体现在体系架构和内核的支持上。</p><p>对于体系架构，除了 Docker 一开始就支持的 X86-64 ，其他体系架构的支持则一直在不断地完善和推进中。</p><p>Docker 分为 CE 和 EE 两大版本。CE 即社区版，免费支持周期 7 个月;EE 即企业版，强调安全，付费使用，支持周期 24 个月。</p><p>我们在安装前可以参看官方文档获取最新的 Docker 支持情况，官方文档在这里：<a href="https://docs.docker.com/install/。" target="_blank" rel="noopener">https://docs.docker.com/install/。</a></p><p>Docker 对于内核支持的功能，即内核的配置选项也有一定的要求(比如必须开启 Cgroup 和 Namespace 相关选项，以及其他的网络和存储驱动等)。</p><p>Docker 源码中提供了一个检测脚本来检测和指导内核的配置，脚本链接在这里：<a href="https://raw.githubusercontent" target="_blank" rel="noopener">https://raw.githubusercontent</a>. … ig.sh。</p><p>在满足前提条件后，安装就变得非常的简单了。</p><p>Docker CE 的安装请参考官方文档：</p><ul><li>MacOS：<a href="https://docs.docker.com/docker-for-mac/install/" target="_blank" rel="noopener">https://docs.docker.com/docker-for-mac/install/</a></li><li>Windows：<a href="https://docs.docker.com/docker" target="_blank" rel="noopener">https://docs.docker.com/docker</a> … tall/</li><li>Ubuntu：<a href="https://docs.docker.com/instal" target="_blank" rel="noopener">https://docs.docker.com/instal</a> … untu/</li><li>Debian：<a href="https://docs.docker.com/instal" target="_blank" rel="noopener">https://docs.docker.com/instal</a> … bian/</li><li>CentOS：<a href="https://docs.docker.com/instal" target="_blank" rel="noopener">https://docs.docker.com/instal</a> … ntos/</li><li>Fedora：<a href="https://docs.docker.com/instal" target="_blank" rel="noopener">https://docs.docker.com/instal</a> … dora/</li></ul><p>其他 Linux 发行版：<a href="https://docs.docker.com/instal" target="_blank" rel="noopener">https://docs.docker.com/instal</a> … ries/</p><p>这里我们以 CentOS 7 作为演示。</p><p>环境准备：</p><ul><li>阿里云服务器(1 核 2G，1M 带宽)</li><li>CentOS 7.4 64 位</li></ul><p>由于 Docker-CE 支持 64 位版本的 CentOS 7 ，并且要求内核版本不低于 3.10，首先我们需要卸载掉旧版本的 Docker：</p><ol><li>$ sudo yum remove docker \  </li><li>docker-client \  </li><li>docker-client-latest \  </li><li>docker-common \  </li><li>docker-latest \  </li><li>docker-latest-logrotate \  </li><li>docker-logrotate \  </li><li>docker-selinux \  </li><li>docker-engine-selinux \  </li><li>docker-engine </li></ol><p><a href="http://s4.51cto.com/oss/201811/14/9021189d4664252b59231d6372afd14c.png-wh_600x-s_1328614105.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/9021189d4664252b59231d6372afd14c.png-wh_600x-s_1328614105.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>我们执行以下安装命令去安装依赖包：</p><ol><li>$ sudo yum install -y yum-utils \  </li><li>device-mapper-persistent-data \  </li><li>lvm2 </li></ol><p>这里我事先已经安装过了，所以提示我已经安装了最新版本：</p><p><a href="http://s2.51cto.com/oss/201811/14/fd59a5b8018ed1d6ab0840dd5657c586.png-wh_600x-s_1801127902.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/fd59a5b8018ed1d6ab0840dd5657c586.png-wh_600x-s_1801127902.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p><strong>安装 Docker</strong></p><p>Docker 软件包已经包括在默认的 CentOS-Extras 软件源里。因此想要安装 Docker，只需要运行下面的 yum 命令：</p><ol><li>$ sudo yum install docker </li></ol><p>当然在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，CentOS 系统上可以使用这套脚本安装：</p><ol><li>curl -fsSL get.docker.com -o get-docker.sh  </li><li>sh get-docker.sh </li></ol><p>具体可以参看 docker-install 的脚本：<a href="https://github.com/docker/docker-install。" target="_blank" rel="noopener">https://github.com/docker/docker-install。</a></p><p>执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。</p><p>安装完成后，运行下面的命令，验证是否安装成功：</p><ol><li>docker versionordocker info </li></ol><p>返回 Docker 的版本相关信息，证明 Docker 安装成功：</p><p><a href="http://s2.51cto.com/oss/201811/14/f0905c1ae2be4fe69ef1592deff431ad.png-wh_600x-s_1357052908.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/f0905c1ae2be4fe69ef1592deff431ad.png-wh_600x-s_1357052908.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>启动 Docker-CE：</p><ol><li>$ sudo systemctl enable docker$ sudo systemctl start docker </li></ol><p><strong>Docker 的简单运用 Hello World</strong></p><p>由于服务器日常崩溃了， Docker 出了点问题，所以以下案例的演示是基于 Kali Linux 环境下进行的。</p><p>我们通过最简单的 Image 文件 Hello World，感受一下 Docker 的魅力吧!</p><p>我们直接运行下面的命令，将名为 hello-world 的 image 文件从仓库抓取到本地：</p><ol><li>docker pull library/hello-world </li></ol><p>docker pull images 是抓取 image 文件，library/hello-world 是 image 文件在仓库里面的位置，其中 library 是 image 文件所在的组，hello-world 是 image 文件的名字。</p><p><a href="http://s1.51cto.com/oss/201811/14/4eca659d8824e64c6b05bf699f87d492.png-wh_600x-s_2161600604.png" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/14/4eca659d8824e64c6b05bf699f87d492.png-wh_600x-s_2161600604.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>抓取成功以后，就可以在本机看到这个 image 文件了：</p><ol><li>docker images </li></ol><p>我们可以看到如下结果：</p><p><a href="http://s3.51cto.com/oss/201811/14/c1b1833f2c4a607ee899a7116bdc5ba4.png-wh_600x-s_1381454007.png" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/14/c1b1833f2c4a607ee899a7116bdc5ba4.png-wh_600x-s_1381454007.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>现在，我们可以运行 hello-world 这个 image 文件：</p><ol><li>docker run hello-world </li></ol><p>我们可以看到如下结果：</p><p><a href="http://s2.51cto.com/oss/201811/14/4cbc9e7853eb9f658895151313e6b824.png-wh_600x-s_1655086547.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/4cbc9e7853eb9f658895151313e6b824.png-wh_600x-s_1655086547.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>输出这段提示以后，hello world 就会停止运行，容器自动终止。有些容器不会自动终止，因为提供的是服务，比如 MySQL 镜像等。</p><p>是不是很 Easy 呢?我们从上面可以看出，Docker 的功能是十分强大的，除此之外，我们还可以拉取一些 Ubuntu，Apache 等镜像，在未来的教程中我们将会一一提到。</p><p>Docker 提供了一套简单实用的命令来创建和更新镜像，我们可以通过网络直接下载一个已经创建好了的应用镜像，并通过 Docker RUN 命令就可以直接使用。</p><p>当镜像通过 RUN 命令运行成功后，这个运行的镜像就是一个 Docker 容器啦。</p><p>容器可以理解为一个轻量级的沙箱，Docker 利用容器来运行和隔离应用，容器是可以被启动、停止、删除的，这并不会影响 Docker 镜像。</p><p>我们可以看看下面这幅图：</p><p><a href="http://s2.51cto.com/oss/201811/14/380e32d620c29c014d7112f192721e3d.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/380e32d620c29c014d7112f192721e3d.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>Docker 客户端是 Docker 用户与 Docker 交互的主要方式。当您使用 Docker 命令行运行命令时，Docker 客户端将这些命令发送给服务器端，服务端将执行这些命令。</p><p>Docker 命令使用 Docker API 。Docker 客户端可以与多个服务端进行通信。</p><p>我们将剖析一下 Docker 容器是如何工作的，学习好 Docker 容器工作的原理，我们就可以自己去管理我们的容器了。</p><p><strong>Docker 架构</strong></p><p>在上面的学习中，我们简单地讲解了 Docker 的基本架构。了解到了 Docker 使用的是 C/S 结构，即客户端/服务器体系结构。</p><p>明白了 Docker 客户端与 Docker 服务器进行交互时，Docker 服务端负责构建、运行和分发 Docker 镜像。</p><p>知道了 Docker 客户端和服务端可以运行在一台机器上，我们可以通过 RESTful 、Stock 或网络接口与远程 Docker 服务端进行通信。</p><p>我们从下图可以很直观的了解到 Docker 的架构：</p><p><a href="http://s4.51cto.com/oss/201811/14/987c8285bea392f1f4e3edecd00f461b.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/987c8285bea392f1f4e3edecd00f461b.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>Docker 的核心组件包括：</p><ul><li>Docker Client</li><li>Docker Daemon</li><li>Docker Image</li><li>Docker Registry</li><li>Docker Container</li></ul><p>Docker 采用的是 Client/Server 架构。客户端向服务器发送请求，服务器负责构建、运行和分发容器。</p><p>客户端和服务器可以运行在同一个 Host 上，客户端也可以通过 Socket 或 REST API 与远程的服务器通信。</p><p>可能很多朋友暂时不太理解一些东西，比如 REST API 是什么东西等，不过没关系，在后面的文章中会一一给大家讲解清楚。</p><p><strong>Docker Client</strong></p><p>Docker Client ，也称 Docker 客户端。它其实就是 Docker 提供命令行界面(CLI)工具，是许多 Docker 用户与 Docker 进行交互的主要方式。</p><p>客户端可以构建，运行和停止应用程序，还可以远程与 Docker_Host 进行交互。</p><p>最常用的 Docker 客户端就是 Docker 命令，我们可以通过 Docker 命令很方便地在 Host 上构建和运行 Docker 容器。</p><p><a href="http://s5.51cto.com/oss/201811/14/b9aac3746aeb03dd7682b28eb0dc1ce1.png" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/14/b9aac3746aeb03dd7682b28eb0dc1ce1.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p><strong>Docker Daemon</strong></p><p>Docker Daemon 是服务器组件，以 Linux 后台服务的方式运行，是 Docker 最核心的后台进程，我们也把它称为守护进程。</p><p>它负责响应来自 Docker Client 的请求，然后将这些请求翻译成系统调用完成容器管理操作。</p><p>该进程会在后台启动一个 API Server ，负责接收由 Docker Client 发送的请求，接收到的请求将通过 Docker Daemon 内部的一个路由分发调度，由具体的函数来执行请求。</p><p><a href="http://s2.51cto.com/oss/201811/14/992d6ecb19b108c72b61ff57f9cac84c.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/992d6ecb19b108c72b61ff57f9cac84c.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>我们大致可以将其分为以下三部分：</p><ul><li>Docker Server</li><li>Engine</li><li>Job</li></ul><p>Docker Daemon 的架构如下所示：</p><p><a href="http://s2.51cto.com/oss/201811/14/69faf6995cdb673d431b770668b76618.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/69faf6995cdb673d431b770668b76618.jpg" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>Docker Daemon 可以认为是通过 Docker Server 模块接受 Docker Client 的请求，并在 Engine 中处理请求，然后根据请求类型，创建出指定的 Job 并运行。</p><p>Docker Daemon 运行在 Docker Host 上，负责创建、运行、监控容器，构建、存储镜像。</p><p>运行过程的作用有以下几种可能：</p><ul><li>向 Docker Registry 获取镜像。</li><li>通过 GraphDriver 执行容器镜像的本地化操作。</li><li>通过 NetworkDriver 执行容器网络环境的配置。</li><li>通过 ExecDriver 执行容器内部运行的执行工作。</li></ul><p>由于 Docker Daemon 和 Docker Client 的启动都是通过可执行文件 Docker 来完成的，因此两者的启动流程非常相似。</p><p>Docker 可执行文件运行时，运行代码通过不同的命令行 Flag 参数，区分两者，并最终运行两者各自相应的部分。</p><p>启动 Docker Daemon 时，一般可以使用以下命令来完成：</p><ol><li>docker –daemon = truedocker –d   </li><li>docker –d = true </li></ol><p>再由 Docker 的 main() 函数来解析以上命令的相应 Flag 参数，并最终完成 Docker Daemon 的启动。</p><p>下图可以很直观地看到 Docker Daemon 的启动流程：</p><p><a href="http://s3.51cto.com/oss/201811/14/1c5667f53cc5c2984eaa063c11d46e7c.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/14/1c5667f53cc5c2984eaa063c11d46e7c.jpg" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>默认配置下，Docker Daemon 只能响应来自本地 Host 的客户端请求。如果要允许远程客户端请求，需要在配置文件中打开 TCP 监听。</p><p>我们可以照着如下步骤进行配置：</p><p><strong>1、编辑配置文件/etc/systemd/system/multi-user.target.wants/docker.service，在环境变量 ExecStart 后面添加 -H tcp://0.0.0.0，允许来自任意 IP 的客户端连接。</strong></p><p><a href="http://s4.51cto.com/oss/201811/14/ff8949a0595ce571e25c0a4babd3c660.jpg-wh_600x-s_1204743145.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/ff8949a0595ce571e25c0a4babd3c660.jpg-wh_600x-s_1204743145.jpg" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p><strong>2、重启 Docker Daemon：</strong></p><ol><li>systemctl daemon-reload  </li><li>systemctl restart docker.service </li></ol><p><strong>3、我们通过以下命令即可实现与远程服务器通信：</strong></p><ol><li>docker -H 服务器IP地址 info </li></ol><p>-H 是用来指定服务器主机，info 子命令用于查看 Docker 服务器的信息。</p><p><strong>Docker Image</strong></p><p>Docker 镜像可以看作是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数(如匿名卷、环境变量、用户等)。</p><p>镜像不包含任何动态数据，其内容在构建之后也不会被改变。我们可将 Docker 镜像看成只读模板，通过它可以创建 Docker 容器。</p><p>镜像有多种生成方法：</p><ul><li>从无到有开始创建镜像</li><li>下载并使用别人创建好的现成的镜像</li><li>在现有镜像上创建新的镜像</li></ul><p>我们可以将镜像的内容和创建步骤描述在一个文本文件中，这个文件被称作 Dockerfile ，通过执行 docker build 命令可以构建出 Docker 镜像。</p><p><strong>Docker Registry</strong></p><p>Docker Registry 是存储 Docker Image 的仓库，它在 Docker 生态环境中的位置如下图所示：</p><p><a href="http://s4.51cto.com/oss/201811/14/909a017f35cc8ad675d57e50bd71e502.jpg-wh_600x-s_6161075.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/909a017f35cc8ad675d57e50bd71e502.jpg-wh_600x-s_6161075.jpg" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>运行 docker push、docker pull、docker search 时，实际上是通过 Docker Daemon 与 Docker Registry 通信。</p><p><strong>Docker Container</strong></p><p>Docker 容器就是 Docker 镜像的运行实例，是真正运行项目程序、消耗系统资源、提供服务的地方。</p><p>Docker Container 提供了系统硬件环境，我们可以使用 Docker Images 这些制作好的系统盘，再加上我们所编写好的项目代码，Run 一下就可以提供服务啦。</p><p><strong>Docker 组件是如何协作运行容器</strong></p><p>看到这里，我相信各位读者朋友们应该已经对 Docker 基础架构熟悉的差不多了，我们还记得运行的第一个容器吗?</p><p>现在我们再通过 hello-world 这个例子来体会一下 Docker 各个组件是如何协作的。</p><p>容器启动过程如下：</p><ul><li>Docker 客户端执行 docker run 命令。</li><li>Docker Daemon 发现本地没有 hello-world 镜像。</li><li>Daemon 从 Docker Hub 下载镜像。</li><li>下载完成，镜像 hello-world 被保存到本地。</li><li>Docker Daemon 启动容器。</li></ul><p>具体过程可以看如下这幅演示图：</p><p><a href="http://s2.51cto.com/oss/201811/14/f49e21eddbc1e71b842cd51781d56b6b.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/f49e21eddbc1e71b842cd51781d56b6b.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>我们可以通过 Docker Images 可以查看到 hello-world 已经下载到本地：</p><p><a href="http://s2.51cto.com/oss/201811/14/742e3e7e16ed38049bcf31a5e790750a.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/742e3e7e16ed38049bcf31a5e790750a.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>我们可以通过 Docker Ps 或者 Docker Container ls 显示正在运行的容器，我们可以看到，hello-world 在输出提示信息以后就会停止运行，容器自动终止，所以我们在查看的时候没有发现有容器在运行。</p><p><a href="http://s4.51cto.com/oss/201811/14/ffe2a6286ab2df89325f1e667a0c69fc.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/ffe2a6286ab2df89325f1e667a0c69fc.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>我们把 Docker 容器的工作流程剖析的十分清楚了，我们大体可以知道 Docker 组件协作运行容器可以分为以下几个过程：</p><ul><li>Docker 客户端执行 docker run 命令。</li><li>Docker Daemon 发现本地没有我们需要的镜像。</li><li>Daemon 从 Docker Hub 下载镜像。</li><li>下载完成后，镜像被保存到本地。</li><li>Docker Daemon 启动容器。</li></ul><p>了解了这些过程以后，我们再来理解这些命令就不会觉得很突兀了，下面我来给大家讲讲 Docker 常用的一些命令操作吧。</p><p><strong>Docker 常用命令</strong></p><p>我们可以通过 docker -h 去查看命令的详细的帮助文档。在这里我只会讲一些日常我们可能会用的比较多的一些命令。</p><p><a href="http://s4.51cto.com/oss/201811/14/bfd3fcd830dd29777aff87a505719a57.png-wh_600x-s_4178705744.png" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/bfd3fcd830dd29777aff87a505719a57.png-wh_600x-s_4178705744.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>例如，我们需要拉取一个 Docker 镜像，我们可以用如下命令：</p><ol><li>docker pull image_name </li></ol><p>image_name 为镜像的名称，而如果我们想从 Docker Hub 上去下载某个镜像，我们可以使用以下命令：</p><ol><li>docker pull centos:latest </li></ol><p>cento：lastest 是镜像的名称，Docker Daemon 发现本地没有我们需要的镜像，会自动去 Docker Hub 上去下载镜像，下载完成后，该镜像被默认保存到 /var/lib/docker 目录下。</p><p>接着我们如果想查看主机下存在多少镜像，我们可以用如下命令：</p><ol><li>docker images </li></ol><p>我们要想知道当前有哪些容器在运行，我们可以用如下命令：</p><ol><li>docker ps -a </li></ol><p>-a 是查看当前所有的容器，包括未运行的。我们该如何去对一个容器进行启动，重启和停止呢?</p><p>我们可以用如下命令：</p><ol><li>docker start container_name/container_id  </li><li>docker restart container_name/container_id  </li><li>docker stop container_name/container_id </li></ol><p>这个时候我们如果想进入到这个容器中，我们可以使用 attach 命令：</p><ol><li>docker attach container_name/container_id </li></ol><p>那如果我们想运行这个容器中的镜像的话，并且调用镜像里面的 bash ，我们可以使用如下命令：</p><ol><li>docker run -t -i container_name/container_id /bin/bash </li></ol><p>那如果这个时候，我们想删除指定镜像的话，由于 Image 被某个 Container 引用(拿来运行)，如果不将这个引用的 Container 销毁(删除)，那 Image 肯定是不能被删除。</p><p>我们首先得先去停止这个容器：</p><ol><li>docker ps  </li><li>docker stop container_name/container_id </li></ol><p>然后我们用如下命令去删除这个容器：</p><ol><li>docker rm container_name/container_id </li></ol><p>然后这个时候我们再去删除这个镜像：</p><ol><li>docker rmi image_name </li></ol><p>此时，常用的 Docker 相关的命令就讲到这里为止了，我们在后续的文章中还会反复地提到这些命令。</p><p><strong>Dockerfile 是什么</strong></p><p>前面我们已经提到了 Docker 的一些基本概念。以 CTF 的角度来看，我们可以去使用 Dockerfile 定义镜像，依赖镜像来运行容器，可以去模拟出一个真实的漏洞场景。</p><p>因此毫无疑问的说， Dockerfile 是镜像和容器的关键，并且 Dockerfile 还可以很轻易的去定义镜像内容，说了这么多，那么 Dockerfile 到底是个什么东西呢?</p><p>Dockerfile 是自动构建 Docker 镜像的配置文件，用户可以使用 Dockerfile 快速创建自定义的镜像。Dockerfile 中的命令非常类似于 Linux 下的 Shell 命令。</p><p>我们可以通过下面这幅图来直观地感受下 Docker 镜像、容器和 Dockerfile 三者之间的关系：</p><p><a href="http://s2.51cto.com/oss/201811/14/ee5213091aa5d5386d4d5878b556fcc5.png-wh_600x-s_849636784.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/ee5213091aa5d5386d4d5878b556fcc5.png-wh_600x-s_849636784.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>我们从上图中可以看到，Dockerfile 可以自定义镜像，通过 Docker 命令去运行镜像，从而达到启动容器的目的。Dockerfile 是由一行行命令语句组成，并且支持已 # 开头的注释行。</p><p>一般来说，我们可以将 Dockerfile 分为四个部分：</p><ul><li>基础镜像(父镜像)信息指令 FROM。</li><li>维护者信息指令 MAINTAINER。</li><li>镜像操作指令 RUN 、EVN 、ADD 和 WORKDIR 等。</li><li>容器启动指令 CMD 、ENTRYPOINT 和 USER 等。</li></ul><p>下面是一段简单的 Dockerfile 的例子：</p><ol><li>FROM python:2.7MAINTAINER Angel_Kitty <a href="mailto:&#x61;&#x6e;&#103;&#101;&#108;&#107;&#x69;&#x74;&#116;&#121;&#54;&#54;&#57;&#56;&#x40;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#x63;&#111;&#x6d;" target="_blank" rel="noopener">&#x61;&#x6e;&#103;&#101;&#108;&#107;&#x69;&#x74;&#116;&#121;&#54;&#54;&#57;&#56;&#x40;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#x63;&#111;&#x6d;</a>COPY . /app </li><li>WORKDIR /app </li><li>RUN pip install -r requirements.txt </li><li>EXPOSE 5000ENTRYPOINT [“python”]CMD [“app.py”] </li></ol><p>我们可以分析一下上面这个过程：</p><ul><li>从 Docker Hub 上 Pull 下 Python 2.7 的基础镜像。</li><li>显示维护者的信息。</li><li>Copy 当前目录到容器中的 /App 目录下 复制本地主机的 ( Dockerfile 所在目录的相对路径)到容器里 。</li><li>指定工作路径为 /App。</li><li>安装依赖包。</li><li>暴露 5000 端口。</li><li>启动 App。</li></ul><p>这个例子是启动一个 Python Flask App 的 Dockerfile(Flask 是 Python 的一个轻量的 Web 框架)，相信大家从这个例子中能够稍微理解了 Dockfile 的组成以及指令的编写过程。</p><p><strong>Dockerfile 常用的指令</strong></p><p>根据上面的例子，我们已经差不多知道了 Dockerfile 的组成以及指令的编写过程，我们再来理解一下这些常用命令就会得心应手了。</p><p>由于 Dockerfile 中所有的命令都是以下格式：INSTRUCTION argument ，指令(INSTRUCTION)不分大小写，但是推荐大写和 SQL 语句是不是很相似呢?下面我们正式来讲解一下这些指令集吧。</p><p><strong>FROM</strong></p><p>FROM 是用于指定基础的 images ，一般格式为 FROM<image>or FROM <image>:<tag></tag></image></image></p><p>所有的 Dockerfile 都应该以 FROM 开头，FROM 命令指明 Dockerfile 所创建的镜像文件以什么镜像为基础，FROM 以后的所有指令都会在 FROM 的基础上进行创建镜像。</p><p>可以在同一个 Dockerfile 中多次使用 FROM 命令用于创建多个镜像。比如我们要指定 Python 2.7 的基础镜像，我们可以像如下写法一样：</p><ol><li>FROM python:2.7 </li></ol><p><strong>MAINTAINER</strong></p><p>MAINTAINER 是用于指定镜像创建者和联系方式，一般格式为 MAINTAINER 。</p><p>这里我设置成我的 ID 和邮箱：</p><ol><li>MAINTAINER Angel_Kitty </li></ol><p><strong>COPY</strong></p><p>COPY 是用于复制本地主机的 (为 Dockerfile 所在目录的相对路径)到容器中的 。</p><p>当使用本地目录为源目录时，推荐使用 COPY 。一般格式为 COPY 。</p><p>例如我们要拷贝当前目录到容器中的 /app 目录下，我们可以这样操作：</p><ol><li>COPY . /app </li></ol><p><strong>WORKDIR</strong></p><p>WORKDIR 用于配合 RUN，CMD，ENTRYPOINT 命令设置当前工作路径。</p><p>可以设置多次，如果是相对路径，则相对前一个 WORKDIR 命令。默认路径为/。一般格式为 WORKDIR /path/to/work/dir。</p><p>例如我们设置 /app 路径，我们可以进行如下操作：</p><ol><li>WORKDIR /app </li></ol><p><strong>RUN</strong></p><p>RUN 用于容器内部执行命令。每个 RUN 命令相当于在原有的镜像基础上添加了一个改动层，原有的镜像不会有变化。一般格式为 RUN 。</p><p>例如我们要安装 Python 依赖包，我们做法如下：</p><ol><li>RUN pip install -r requirements.txt </li></ol><p><strong>EXPOSE</strong></p><p>EXPOSE 命令用来指定对外开放的端口。一般格式为 EXPOSE […]。</p><p>例如上面那个例子，开放5000端口：</p><ol><li>EXPOSE 5000 </li></ol><p><strong>ENTRYPOINT</strong></p><p>ENTRYPOINT 可以让你的容器表现得像一个可执行程序一样。一个 Dockerfile 中只能有一个 ENTRYPOINT，如果有多个，则最后一个生效。</p><p>ENTRYPOINT 命令也有两种格式：</p><ul><li>ENTRYPOINT [“executable”, “param1”, “param2”] ：推荐使用的 Exec 形式。</li><li>ENTRYPOINT command param1 param2 ：Shell 形式。</li></ul><p>例如下面这个，我们要将 Python 镜像变成可执行的程序，我们可以这样去做：</p><ol><li>ENTRYPOINT [“python”] </li></ol><p><strong>CMD</strong></p><p>CMD 命令用于启动容器时默认执行的命令，CMD 命令可以包含可执行文件，也可以不包含可执行文件。</p><p>不包含可执行文件的情况下就要用 ENTRYPOINT 指定一个，然后 CMD 命令的参数就会作为 ENTRYPOINT 的参数。</p><p>CMD 命令有三种格式：</p><ul><li>CMD [“executable”,”param1”,”param2”]：推荐使用的 exec 形式。</li><li>CMD [“param1”,”param2”]：无可执行程序形式。</li><li>CMD command param1 param2：Shell 形式。</li></ul><p>一个 Dockerfile 中只能有一个 CMD，如果有多个，则最后一个生效。而 CMD 的 Shell 形式默认调用 /bin/sh -c 执行命令。</p><p>CMD 命令会被 Docker 命令行传入的参数覆盖：docker run busybox /bin/echo Hello Docker 会把 CMD 里的命令覆盖。</p><p>例如我们要启动 /app ，我们可以用如下命令实现：</p><ol><li>CMD [“app.py”] </li></ol><p>当然还有一些其他的命令，我们在用到的时候再去一一讲解一下。</p><p><strong>构建 Dockerfile</strong></p><p>我们大体已经把 Dockerfile 的写法讲述完毕，我们可以自己动手写一个例子：</p><ol><li>mkdir static_web </li><li>cd static_web </li><li>touch Dockerfile </li></ol><p>然后 vi Dockerfile 开始编辑该文件，输入 i 开始编辑。以下是我们构建的 Dockerfile 内容：</p><ol><li>FROM nginx </li><li>MAINTAINER Angel_Kitty <a href="mailto:&#x61;&#110;&#x67;&#101;&#x6c;&#107;&#105;&#x74;&#x74;&#x79;&#x36;&#x36;&#x39;&#x38;&#x40;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#99;&#x6f;&#x6d;" target="_blank" rel="noopener">&#x61;&#110;&#x67;&#101;&#x6c;&#107;&#105;&#x74;&#x74;&#x79;&#x36;&#x36;&#x39;&#x38;&#x40;&#103;&#109;&#x61;&#105;&#108;&#x2e;&#99;&#x6f;&#x6d;</a> </li><li>RUN echo ‘<h1>Hello, Docker!</h1>‘ &gt; /usr/share/nginx/html/index.html </li></ol><p>编辑完后按 esc 退出编辑，然后 ：wq写入，退出。</p><p>我们在 Dockerfile 文件所在目录执行：</p><ol><li>docker build -t angelkitty/nginx_web:v1 . </li></ol><p>我们解释一下：</p><ul><li>-t 是为新镜像设置仓库和名称</li><li>angelkitty 为仓库名</li><li>nginx_web 为镜像名</li><li>：v1 为标签(不添加为默认 latest )</li></ul><p>我们构建完成之后，使用 Docker Images 命令查看所有镜像，如果存在 REPOSITORY 为 Nginx 和 TAG 是 v1 的信息，就表示构建成功。</p><p><a href="http://s1.51cto.com/oss/201811/14/eb520ca716b3c29f00af11ecf412f08c.png-wh_600x-s_4220077836.png" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/14/eb520ca716b3c29f00af11ecf412f08c.png-wh_600x-s_4220077836.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a></p><p>接下来使用 docker run 命令来启动容器：</p><ol><li>docker run –name nginx_web -d -p 8080:80 angelkitty/nginx_web:v1 </li></ol><p>这条命令会用 Nginx 镜像启动一个容器，命名为 nginx_web ，并且映射了 8080 端口。</p><p>这样我们可以用浏览器去访问这个 Nginx 服务器：<a href="http://localhost:8080/" target="_blank" rel="noopener">http://localhost:8080/</a> 或者 http://本机的 IP 地址：8080/，页面返回信息：</p><p><a href="http://s2.51cto.com/oss/201811/14/3051e73398b4fc498b1260fa3cf0bf1c.png-wh_600x-s_3739567784.png" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/3051e73398b4fc498b1260fa3cf0bf1c.png-wh_600x-s_3739567784.png" alt="详细的Docker入门总结 看这一篇就够了" title="详细的Docker入门总结 看这一篇就够了"></a><br>原文地址：<a href="http://developer.51cto.com/art/201811/586912.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201811/586912.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 虚拟化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何构建一个低成本，高可用，少运维的ES平台？</title>
      <link href="/2018/11/15/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BD%8E%E6%88%90%E6%9C%AC%EF%BC%8C%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%8C%E5%B0%91%E8%BF%90%E7%BB%B4%E7%9A%84ES%E5%B9%B3%E5%8F%B0%EF%BC%9F/"/>
      <url>/2018/11/15/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BD%8E%E6%88%90%E6%9C%AC%EF%BC%8C%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%8C%E5%B0%91%E8%BF%90%E7%BB%B4%E7%9A%84ES%E5%B9%B3%E5%8F%B0%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<p>本文作者将从以下几个方面介绍在这两个阶段的发展中为业务解决了哪些痛点以及是如何去解决这些痛点的：</p><ul><li><p><strong>源动力</strong></p></li><li><p><strong>ES 平台</strong></p></li><li><p><strong>回看业务</strong></p></li><li><p><strong>搜索中台</strong></p></li></ul><a id="more"></a><p>源动力：架构复杂、运维艰难</p><p>和大多数大型企业一样，蚂蚁内部也有一套自研的搜索系统，我们称之为主搜。</p><p>但是由于这种系统可定制性高，所以一般业务接入比较复杂，周期比较长。而对于大量新兴的中小业务而言，迭代速度尤为关键，因此难以用主搜去满足。</p><p>主搜不能满足，业务又实际要用，怎么办呢？那就只能自建了。在前几年蚂蚁内部有很多小的搜索系统，有 ES，也有 Solr，甚至还有自己用 Lucene 的。</p><p><strong>业务痛点</strong></p><p><a href="http://s1.51cto.com/oss/201811/14/a8842daf08f1abad8465d6fad1844d53.jpg-wh_600x-s_184107609.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/14/a8842daf08f1abad8465d6fad1844d53.jpg-wh_600x-s_184107609.jpg" alt=""></a></p><p>业务由于自身迭代速度很快，去运维这些搜索系统成本很大。就像 ES，虽然搭建一套很是简单，但是用在真实生产环境中还是需要很多专业知识的。</p><p>作为业务部门很难去投入人力去运维维护。并且由于蚂蚁自身的业务特性，很多业务都是需要高可用保证的。</p><p>而我们都知道 ES 本身的高可用目前只能跨机房部署了，先不谈跨机房部署时的分配策略，光是就近访问一点，业务都很难去完成。</p><p>因为这些原因，导致这类场景基本都没有高可用，业务层宁愿写两套代码，准备一套兜底方案。他们觉得容灾时直接降级也比高可用简单。</p><p><strong>架构痛点</strong></p><p><a href="http://s1.51cto.com/oss/201811/14/8ff181e4e336a0ac538eedb1fe44e8e8.jpg-wh_600x-s_2302061528.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201811/14/8ff181e4e336a0ac538eedb1fe44e8e8.jpg-wh_600x-s_2302061528.jpg" alt=""></a></p><p>从整体架构层面看，各个业务自行搭建搜索引擎造成了烟囱林立，各种重复建设。</p><p>并且这种中小业务一般数据量都比较小，往往一个业务一套三节点集群只有几万条数据，造成整体资源利用率很低。</p><p>而且由于搜索引擎选用的版本，部署的方式都不一致，也难以保证质量。在架构层面只能当做不存在搜索能力。</p><p>低成本，高可用，少运维的 Elasticsearch 平台应运而生</p><p>基于以上痛点，我们产生了构建一套标准搜索平台的想法，将业务从运维中解放出来，也从架构层面统一基础设施，提供一种简单可信的搜索服务。</p><h3 id="架构图如下："><a href="#架构图如下：" class="headerlink" title="架构图如下："></a>架构图如下：</h3><p><a href="http://s2.51cto.com/oss/201811/14/99c286692aa28ff253f3e758fb2cc598.jpg-wh_600x-s_2865496202.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/99c286692aa28ff253f3e758fb2cc598.jpg-wh_600x-s_2865496202.jpg" alt=""></a></p><p>如何做低成本，高可用，少运维呢？我们先来一起看一下整体架构，如上图。</p><p>首先说明一下我们这两个框框代表两个机房，我们整体就是一种多机房的架构，用来保证高可用：</p><ul><li><p><strong>最上层是用户接入层，</strong>有 API，Kibana，Console 三种方式，用户和使用 ES 原生的 API 一样可以直接使用我们的产品。</p></li><li><p><strong>中间为路由层（Router)，</strong>负责将用户请求真实发送到对应集群中，负责一些干预处理逻辑。</p></li><li><p><strong>下面每个机房中都有队列（Queue），</strong>负责削峰填谷和容灾多写。</p></li><li><p><strong>每个机房中有多个 ES 集群，</strong>用户的数据最终落在一个真实的集群中，或者一组对等的高可用集群中。</p></li><li><p><strong>右边红色的是 Meta，</strong>负责所有组件的一站式自动化运维和元数据管理。</p></li><li><p><strong>最下面是 Kubernetes，</strong>我们所有的组件均是以容器跑在 K8S 上的，这解放了我们很多物理机运维操作，使得滚动重启这些变得非常方便。</p></li></ul><p><strong>低成本：多租户</strong></p><p>看完了整体，下面就逐点介绍下我们是怎么做的，第一个目标是低成本。在架构层面，成本优化是个每年必谈的话题。</p><p>那么降低成本是什么意思？实际上就是提高资源利用率。提高资源利用率方法有很多，比如提高压缩比，降低查询开销。但是在平台上最简单有效的方式则是多租户。</p><p><strong>今天我就主要介绍下我们的多租户方案：</strong>多租户的关键就是租户隔离，租户隔离分为逻辑隔离和物理隔离。</p><h4 id="逻辑隔离"><a href="#逻辑隔离" class="headerlink" title="逻辑隔离"></a><strong>逻辑隔离</strong></h4><p><a href="http://s2.51cto.com/oss/201811/14/07559e6e35fbfdf0288d89937098002b.jpg-wh_600x-s_3954410652.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/07559e6e35fbfdf0288d89937098002b.jpg-wh_600x-s_3954410652.jpg" alt=""></a></p><p>首先介绍下我们的逻辑隔离方案，逻辑隔离就是让业务还是和之前自己搭 ES 一样的用法，也就是透明访问。</p><p>但是实际上访问的只是真实集群中属于自己的一部分数据，而看不到其他人的数据，也就是保证水平权限。</p><p>而 ES 有一点很适合用来做逻辑隔离，ES 的访问实际上都是按照 Index 的。因此我们逻辑隔离的问题就转化为如何让用户只能看到自己的表了。</p><p>我们是通过 Console 保存用户和表的映射关系，然后在访问时通过 Router，也就是前面介绍的路由层进行干预，使得用户只能访问自己的 Index。</p><p>具体而言，我们路由层采用 OpenResty+Lua 实现，将请求过程分为了右图的四步：Dispatch，Filter，Router，Reprocess。</p><p><strong>①</strong>在 Dispatch 阶段我们将请求结构化，抽出其用户，App，Index，Action 数据。</p><p><strong>②</strong>然后进入 Filter 阶段，进行写过滤和改写。</p><p>Filter 又分为三步：</p><ul><li><p>Access 进行限流和验权这类的准入性拦截。</p></li><li><p>Action 对具体的操作进行拦截处理，比如说 DDL，也就是建表，删表，修改结构这些操作，我们将其转发到 Console 进行处理。</p><p>一方面方便记录其 Index 和 App 的对应信息；另一方面由于建删表这种还是很影响集群性能的，我们通过转发给 Console 可以对用户进行进一步限制，防止恶意行为对系统的影响。</p></li><li><p>Params 则是请求改写，在这一步我们会根据具体的 Index 和 Action 进行相应的改写。</p><p>比如去掉用户没有权限的 Index；比如对于 Kibana 索引将其改为用户自己的唯一 Kibana 索引以实现 Kibana 的多租户；比如对 ES 不同版本的简单兼容。</p><p>在这一步我们可以做很多，不过需要注意的有两点：一是尽量不要解析 Body，解 Body 是一种非常影响性能的行为，除了特殊的改写外应该尽力避免，比如 Index 就应该让用户写在 URL 上，并利用 ES 本身的参数关闭 Body 中指定 Index 的功能，这样改写速度可以快很多。</p><p>二是对于 _all 和 getMapping 这种对所有 Index 进行访问的，如果我们替换为用户所有的索引会造成 URL 过长，我们采用的是创建一个和应用名同名的别名，然后将其改写成这个别名。</p></li></ul><p><strong>③</strong>进行完 Filter 就到了真实的 Router 层，这一层就是根据 Filter 的结果做真实的路由请求，可能是转发到真实集群也可能是转发到我们其他的微服务中。</p><p><strong>④</strong>最后是 Reprocess ,这是拿到业务响应后的最终处理，我们在这边会对一些结果进行改写，并且异步记录日志。</p><p>上面这四步就是我们路由层的大致逻辑，通过 App 和 Index 的权限关系控制水平权限，通过 Index 改写路由进行共享集群。</p><h4 id="物理隔离"><a href="#物理隔离" class="headerlink" title="物理隔离"></a><strong>物理隔离</strong></h4><p><a href="http://s5.51cto.com/oss/201811/14/f4a4bae9aee58ebb5601a596c287191c.jpg-wh_600x-s_483190614.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/14/f4a4bae9aee58ebb5601a596c287191c.jpg-wh_600x-s_483190614.jpg" alt=""></a></p><p>做完了逻辑隔离，我们可以保证业务的水平权限了，那么是否就可以了呢？</p><p>显然不是的，实际中不同业务访问差异还是很大的，只做逻辑隔离往往会造成业务间相互影响。</p><p>这时候就需要物理隔离了。不过物理隔离我们目前也没有找到非常好的方案，这边给大家分享下我们的一些尝试。</p><p>首当其冲，我们采用的方法是服务分层，也就是将不同用途，不同重要性的业务分开，对于关键性的主链路业务甚至可以独占集群。</p><p><strong>对于其他的，我们主要分为两类：</strong>写多查少的日志型和查多写少的检索型业务，按照其不同的要求和流量预估将其分配在我们预设的集群中。</p><p>不过需要注意的是申报的和实际的总会有差异的，所以我们还有定期巡检机制，会将已上线业务按照其真实流量进行集群迁移。</p><p>做完了服务分层，我们基本可以解决了低重要性业务影响高重要性业务的场景，但是在同级业务中依旧会有些业务因为比如说做营销活动这种造成突发流量。</p><p>对于这种问题怎么办？一般而言就是全局限流，但是由于我们的访问都是长连接，所以限流并不好做。</p><p>如右图所示，用户通过一个 LVS 访问了我们多个 Router，然后我们又通过了 LVS 访问了多个 ES 节点，我们要做限流，也就是要保证所有 Router 上的令牌总数。</p><p>一般而言全局限流有两种方案：</p><ul><li><p><strong>一是以限流维度将所有请求打在同一实例上，</strong>也就是将同一表的所有访问打在一台机器上。</p><p>但是在 ES 访问量这么高的场景下，这种并不合适，并且由于我们前面已经有了一层 LVS 做负载均衡，再做一层路由会显得过于复杂。</p></li><li><p><strong>第二种方案就是均分令牌，</strong>但是由于长连接的问题，会造成有些节点早已被限流，但是其他节点却没有什么流量。</p></li></ul><p>那么怎么办呢？</p><p><a href="http://s2.51cto.com/oss/201811/14/3724d1f05f75f11af9df7bf32b262688.jpg-wh_600x-s_1027664568.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/3724d1f05f75f11af9df7bf32b262688.jpg-wh_600x-s_1027664568.jpg" alt=""></a></p><p>既然是令牌使用不均衡，那么我们就让其分配也不均衡就好了呗。所以我们采用了一种基于反馈的全局限流方案，什么叫基于反馈呢？</p><p>就是我们用巡检去定时采集用量，用的多就多给一些，用的少就少给你一点。</p><p>那么多给一些少给一点到底是什么样的标准呢？这时我们就需要决策单元来处理了，目前我们采取的方案是简单的按比例分配。</p><p>这边需要注意的一点是当有新机器接入时，不是一开始就达到终态的，而是渐进的过程。</p><p>所以需要对这个收敛期设置一些策略，目前因为我们机器性能比较好，不怕突发毛刺，所以我们设置的是全部放行，到稳定后再进行限流。</p><p>这里说到长连接就顺便提一个 Nginx 的小参数：keepalive_timeout。用过 Nginx 的同学应该都见过，表示长连接超时时间，默认有 75s。</p><p>但是这个参数实际上还有一个可选配置，表示写在响应头里的超时时间，如果这个参数没写的话就会出现在服务端释放的瞬间客户端正好复用了这个连接，造成 Connection Reset 或者 NoHttpResponse 的问题。</p><p>出现频率不高，但是真实影响用户体验，因为随机低频出现，我们之前一直以为是客户端问题，后来才发现原来是这个释放顺序的问题。</p><p>至此服务分层，全局限流都已经完成了，是不是可以睡个好觉了呢？ 很遗憾，还是不行，因为 ES 语法非常灵活，并且有许多大代价的操作。</p><p>比如上千亿条数据做聚合，或者是用通配符做个中缀查询，写一个复杂的 Script 都有可能造成拖垮我们整个集群，那么对于这种情况怎么办呢？</p><p>我们目前也是处于探索阶段，比较有用的一种方式是事后补救，也就是我们通过巡检去发现一些耗时大的 Task，然后对其应用的后继操作进行惩罚，比如降级，甚至熔断。</p><p>这样就可以避免持续性的影响整个集群。但是一瞬间的 RT 上升还是不可避免的，因此我们也在尝试事前拦截，不过这个比较复杂，感兴趣的同学可以一起线下交流一下。</p><p><strong>高可用：对等多集群</strong></p><p><a href="http://s4.51cto.com/oss/201811/14/3e9b6b82db69868d2382ee92e18084cc.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/3e9b6b82db69868d2382ee92e18084cc.jpg" alt=""></a></p><p>讲完了低成本，那么就来到了我们第二个目标，高可用。正如我之前提到那样，ES 本身其实提供了跨机房部署的方案，通过打标就可以进行跨机房部署，然后通过 Preference 可以保证业务就近查询。</p><p>我这里就不再详细说了，但是这种方案需要两地三中心， 而我们很多对外输出的场景出于成本考虑，并没有三中心，只有两地两中心，因此双机房如何保证高可用就是我们遇到的一个挑战。</p><p>下面我主要就给大家分享下我们基于对等多机房的高可用方案，提供了两种类型，共三种方案分别适用于不同的业务场景。</p><p><strong>我们有单写多读和多写多读两种类型：</strong>单写多读我们采用的是跨集群复制的方案，通过修改 ES，我们增加了利用 Translog 将主集群数据推送给备库的能力。</p><p>就和 6.5 的 ccr 类似，但是我们采用的是推模式，而不是拉模式，因为我们之前做过测试，对于海量数据写入，推比拉的性能好了不少。</p><p>容灾时进行主备互换，然后恢复后再补上在途数据。由上层来保证单写，多读和容灾切换逻辑。</p><p>这种方案通过 ES 本身的 Translog 同步，部署结构简单，数据也很准确，类似于数据库的备库，比较适合对写入 RT 没有过高要求的高可用场景。</p><p><a href="http://s4.51cto.com/oss/201811/14/86e5a8ac00766fa7dccaebf312258536.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/86e5a8ac00766fa7dccaebf312258536.jpg" alt=""></a></p><p>多写多读，我们提供了两种方案：</p><ul><li><p>第一种方案比较取巧，就是因为很多关键链路的业务场景都是从 DB 同步到搜索中的，因此我们打通了数据通道，可以自动化的从 DB 写入到搜索，用户无需关心。</p><p>那么对于这类用户的高可用，我们采用的就是利用 DB 的高可用，搭建两条数据管道，分别写入不同的集群。这样就可以实现高可用了，并且还可以绝对保证最终一致性。</p></li><li><p>第二种方案则是在对写入 RT 有强要求，有没有数据源的情况下，我们会采用中间层的多写来实现高可用。</p><p>我们利用消息队列作为中间层，来实现双写。就是用户写的时候，写成功后保证队列也写成功了才返回成功，如果一个不成功就整体失败。</p><p>然后由队列去保证推送到另一个对等集群中。用外部版本号去保证一致性。但是由于是中间层，对于 Delete by Query 的一致性保证就有些无能为力了。所以也仅适合特定的业务场景。</p></li></ul><p>最后，在高可用上我还想说的一点是对于平台产品而言，技术方案有哪些，怎么实现的业务其实并不关心，业务关心的仅仅是他们能不能就近访问降低 RT，和容灾时自动切换保证可用。</p><p>因此我们在平台上屏蔽了这些复杂的高可用类型和这些适用的场景，完全交由我们的后端去判断，让用户可以轻松自助接入。</p><p>并且在交互上也将读写控制，容灾操作移到了我们自己系统内，对用户无感知。</p><p>只有用户可以这样透明拥有高可用能力了，我们的平台才真正成为了高可用的搜索平台。</p><p><strong>少运维</strong></p><p><a href="http://s5.51cto.com/oss/201811/14/d7e3045946fa9640eb1c8be0f0d2a017.jpg-wh_600x-s_495068290.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/14/d7e3045946fa9640eb1c8be0f0d2a017.jpg-wh_600x-s_495068290.jpg" alt=""></a></p><p>最后一个目标，少运维，就简单介绍一下我们在整体运维系统搭建过程中沉淀出的四个原则：</p><ul><li><p><strong>自包含</strong>：ES 做的就很不错了，一个 Jar 就可以启动，而我们的整套系统也都应该和单个 ES 一样，一条很简单的命令就能启动，没有什么外部依赖，这样就很好去输出。</p></li><li><p><strong>组件化：</strong>是指我们每个模块都应该可以插拔，来适应不同的业务场景，比如有的不需要多租户，有的不需要削峰填谷。</p></li><li><p><strong>一站到底：</strong>是指我们的所有组件，Router，Queue，ES，还有很多微服务的管控都应该在一个系统中去管控，万万不能一个组件一套自己的管控。</p></li><li><p><strong>自动化</strong>就不说了，大家都懂。</p></li><li><p>右边就是我们的一个大盘页面，展现了 Router，ES 和 Queue 的访问情况。当然，这是 Mock 的数据。</p></li></ul><p>回看业务：无需运维，却依旧不爽</p><p>至此我们已经拥有了一套低成本，高可用，少运维的 Elasticsearch 平台了，也解决了之前谈到的业务痛点，那么用户用的是否就爽了呢？</p><p>我们花了大半个月的时间，对我们的业务进行了走访调研，发现业务虽然已经从运维中解放了出来，但是身上还是有不少搜索的枷锁。</p><p><a href="http://s4.51cto.com/oss/201811/14/b47119538a3546b4868d7ae57344e907.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/b47119538a3546b4868d7ae57344e907.jpg" alt=""></a></p><p>我们主要分为两类用户，数据分析和全文检索的：</p><ul><li><p><strong>数据分析主要觉得配置太复杂，</strong>它只是想导入一个日志数据，要学一堆的字段配置，而且很久才会用到一次，每次学完就忘，用到再重学，很耽误事情。</p><p>其次，无关逻辑重，因为数据分析类的一般都是保留多天的数据，过期的数据就可以删除了，为了实现这一个功能，数据分析的同学要写很多代码，还要控制不同的别名，很是麻烦。</p></li><li><p><strong>而全文检索类的同学主要痛点有三个，</strong>一是分词配置复杂；二是难以修改字段，Reindex 太复杂，还要自己先创建别名，再控制无缝切换；第三点是 Debug 艰难。</p><p>虽然现在有 Explain，但是用过的同学应该都懂，想要整体梳理出具体的算分原因还是需要自己在脑中开辟很大的一块缓存的。对于不熟悉 ES 的同学就太痛苦了。</p></li></ul><p>整理一下，这些痛点归类起来就两个痛点：学习成本高和接口过于原子。</p><p>搜索中台：抽象逻辑，解放业务</p><p>学习成本高和接口过于原子，虽然是业务的痛点，但是对 ES 本身而言却反而是优点，为什么学习成本高呢？因为功能丰富。而为什么接口原子呢？为了让上层可以灵活使用。</p><p>这些对于专家用户而言，非常不错，但是对于业务而言，的确很是麻烦。因此我们开始了我们第二个阶段，搜索中台。</p><p>什么叫中台呢，就是把一些通用的业务逻辑下移，来减少业务的逻辑，让业务专注于业务本身。</p><p>而为什么业务不能做这些呢？当然也能做。但是俗话说『天下武功，唯快不破』，前台越轻，越能适应这变化极快的业务诉求。</p><p>因此我们的搜索中台的主要目标就是两点：</p><ul><li><p><strong>一是降低业务学习成本，加快上手速度。</strong>我们这次介绍的主要是如何降低对于配置类这种低频操作的学习成本。</p></li><li><p><strong>二是抽象复杂逻辑来加速业务迭代，</strong>我们这次主要会介绍抽象了哪两种业务逻辑。</p></li></ul><p><strong>降低学习成本</strong></p><p><a href="http://s5.51cto.com/oss/201811/14/80b07b680cc59f3b3c173f952f5da957.jpg-wh_600x-s_3085686789.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/14/80b07b680cc59f3b3c173f952f5da957.jpg-wh_600x-s_3085686789.jpg" alt=""></a></p><p>降低学习成本，这个怎么做呢？众所周知，黑屏变白屏，也就是白屏化。但是很多的白屏化就是把命令放在了 Web 上，回车变按钮。这样真的可以降低用户学习成本么？ 我想毋庸置疑，这样是不行的。</p><p>我们在可视化上尝试了许多方案，也走了许多弯路，最后发现要想真正降低用户学习成本，需要把握三个要点：</p><p><strong>①用户分层，</strong>区分出小白用户和专家用户，不要让专家用户的意见影响整体产品的极简设计，对于小白用户一定是越少越好，选择越少，路径越短，反馈越及时，效果越好。</p><p>正如所谓的沉默的大多数，很多小白用户并不会去主动发声，只会随着复杂的配置而放弃使用。</p><p>下图就是我们对于专家用户和小白用户在配置表结构时不同的页面，对于专家用户，基本就是 ES 所有的功能可视化，加快使用速度。对于小白用户而言，则是完全屏蔽这些功能点，让其可以直接使用。</p><p><a href="http://s2.51cto.com/oss/201811/14/fd5887217d43e578202d95ef25b3f619.jpg-wh_600x-s_1125191377.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/14/fd5887217d43e578202d95ef25b3f619.jpg-wh_600x-s_1125191377.jpg" alt=""></a></p><p><strong>②引导式配置，</strong>引导式配置其实也就是加上限制，通过对用户的上一步输入决定下一步的可选。</p><p>要避免一个页面打开一堆配置项，这样用户就会无从下手，更不要谈学习成本了。</p><p>通过引导式配置来减少用户的选择，降低用户的记忆成本。限制不一定就意味着约束用户，合适的限制更可以降低用户的理解成本。</p><p>比如右图就是我们的一个分词器配置，很简单的引导，用户选择了中文字典后才可以选择相应的词典。</p><p><a href="http://s4.51cto.com/oss/201811/14/46a537fe7ebf14325e3a13662aa841f7.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/14/46a537fe7ebf14325e3a13662aa841f7.jpg" alt=""></a></p><p><strong>③深层次结构打平，</strong>什么叫深层次结构打平，就是指像现在的分词器，相似度这些都是在 Index 级别下的，我们将其抽象出来，变为全局的。</p><p>用户可以自行创建全局的分词器，相似度，并且还可以共享给其他人，就像一个资源一样。然后在 Index 中则是引用这个分词器。</p><p>虽然这边做的仅仅是将分词器从 Index 级别变为了全局，但是却真正的减少了很多业务操作，因为在一个业务场景中，往往存在多张表，而多张表往往会使用同一套分词器。</p><p>通过这种全局性的分词器用户仅需修改一处即可作用于所有位置。</p><p><strong>抽象复杂逻辑</strong></p><p><a href="http://s3.51cto.com/oss/201811/14/18e9d2ed402c93b0e7119d1868e53bf0.jpg-wh_600x-s_650692007.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/14/18e9d2ed402c93b0e7119d1868e53bf0.jpg-wh_600x-s_650692007.jpg" alt=""></a></p><p>好的，说完了白屏化的一些经验，这边给大家分享我们对于复杂逻辑的抽象封装的两种新型表结构。</p><p>这两种分别是数据分析类场景，我们抽象出了日志型表，另一种是全文检索类场景，我们抽象出了别名型表。</p><p>日志型表的作用顾名思义就是存日志，也就是之前说的对于数据分析类业务，往往只保留几天。</p><p>比如我们现在有个业务场景，有张 ES 的日志表，只想保留 3 天，于是我们就给它按天创建索引。</p><p>然后写入索引挂载到今天，查询索引挂载所有的，用 Router 去自动改写别名，用户还是传入 ES，但是执行写入操作时实际就是在 es_write 执行，查询就是在 es_read 执行。</p><p>当然实际中我们并不是按天建的索引，我们会利用 Rollover 创建很多的索引来保证海量写入下的速度。但是整体逻辑还是和这个是一样的。</p><p><a href="http://s5.51cto.com/oss/201811/14/6abdaca3c84787c7e5baeb7f298de218.jpg-wh_600x-s_3086835012.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/14/6abdaca3c84787c7e5baeb7f298de218.jpg-wh_600x-s_3086835012.jpg" alt=""></a></p><p>而对于全文检索类场景，主要的痛点就是表结构的变更和分词器，字典类的变更，需要重建索引。</p><p>所以我们抽象了一个叫别名表的表结构，用户创建一张表 ES，实际创建的是一个 ES 的别名，我们会把它和我们真实的 Index 一一对应上。这样利用这个别名，我们就可以自动帮用户完成索引重建的操作。</p><p>而索引重建，我们有两种方式，一是用户配置了数据源的，我们会直接从数据源进行重建，重建完成后直接切换。</p><p>另外对于没有数据源，直接 API 写入的，目前我们是利用了 ES 的 Reindex 再配合我们消息队列的消息回放实现的。</p><p>具体而言，我们就是首先提交 Reindex，同时数据开始进 Queue 转发，然后待 Reindex 完成后，Queue 再从 Reindex 开始时进行回放，追平时切别名即可。</p><p>总结</p><p>总结一下这次分享的内容，我们首先构建了一个低成本，高可用，少运维的 ES 平台将业务从运维中解脱出来，然后又进一步构建了搜索中台，通过降低业务学习成本，下沉通用业务逻辑来加速业务迭代，赋能业务。</p><p>当然，这里介绍的搜索中台只是最基础的中台能力，我们还在进一步探索些复杂场景下如何抽象来降低业务成本，也就是垂直化的搜索产品。</p><p><em>作者：__善仁</em></p><p><em>编辑：陶家龙、孙淑娟</em></p><p>_出处：转载自金融级分布式架构（ID：Antfin_SOFA）微信公众号，本文根据他在 2018 Elastic 中国开发者大会的分享整理。_</p><p><em>完整PPT：<a href="http://www.sofastack.tech/posts/2018-11-12-01" target="_blank" rel="noopener">http://www.sofastack.tech/posts/2018-11-12-01</a></em></p><p>原文地址：<a href="http://developer.51cto.com/art/201811/586913.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201811/586913.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 搜索引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>应对大流量的一些思路（转载）</title>
      <link href="/2018/11/15/%E5%BA%94%E5%AF%B9%E5%A4%A7%E6%B5%81%E9%87%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%B7%AF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/11/15/%E5%BA%94%E5%AF%B9%E5%A4%A7%E6%B5%81%E9%87%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%B7%AF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p><strong>1. 首先，我们来说一下什么是大流量?</strong></p><p>大流量，我们很可能会冒出：TPS(每秒事务量)，QPS(每秒请求量)，1W+，5W+，10W+，100W+…。其实并没有一个绝对的数字，如果这个量造成了系统的压力，影响了系统的性能，那么这个量就可以称之为大流量了。<br><a id="more"></a><br><strong>2. 其次，应对大流量的一些常见手段是什么?</strong></p><ul><li>缓存：说白了，就是让数据尽早进入缓存，离程序近一点，不要大量频繁的访问DB。</li><li>降级：如果不是核心链路，那么就把这个服务降级掉。打个比喻，现在的APP都讲究千人千面，拿到数据后，做个性化排序展示，如果在大流量下，这个排序就可以降级掉!</li><li>限流：大家都知道，北京地铁早高峰，地铁站都会做一件事情，就是限流了!想法很直接，就是想在一定时间内把请求限制在一定范围内，保证系统不被冲垮，同时尽可能提升系统的吞吐量。<!--more-->注意到，有些时候，缓存和降级是解决不了问题的，比如，电商的双十一，用户的购买，下单等行为，是涉及到大量写操作，而且是核心链路，无法降级的，这个时候，限流就比较重要了。</li></ul><p>那么接下来，我们重点说一下，限流。</p><p><strong>限流的常用方式</strong></p><p>限流的常用处理手段有：计数器、滑动窗口、漏桶、令牌。</p><p><strong>1. 计数器</strong></p><p>计数器是一种比较简单的限流算法，用途比较广泛，在接口层面，很多地方使用这种方式限流。在一段时间内，进行计数，与阀值进行比较，到了时间临界点，将计数器清0。</p><p><a href="http://s5.51cto.com/oss/201811/15/cba4b9cd595e652e0112fc5b96a150f7.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/15/cba4b9cd595e652e0112fc5b96a150f7.jpg" alt=""></a></p><p>代码实例</p><p><a href="http://s5.51cto.com/oss/201811/15/099406f503f527d8b34c13838c4c28e1.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/15/099406f503f527d8b34c13838c4c28e1.jpg" alt=""></a></p><p>这里需要注意的是，存在一个时间临界点的问题。举个栗子，在12:01:00到12:01:58这段时间内没有用户请求，然后在12:01:59这一瞬时发出100个请求，OK，然后在12:02:00这一瞬时又发出了100个请求。</p><p>这里你应该能感受到，在这个临界点可能会承受恶意用户的大量请求，甚至超出系统预期的承受。</p><p><strong>2. 滑动窗口</strong></p><p>由于计数器存在临界点缺陷，后来出现了滑动窗口算法来解决。</p><p><a href="http://s5.51cto.com/oss/201811/15/22b7d88afbf7084239d7448205a7dafc.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/15/22b7d88afbf7084239d7448205a7dafc.jpg" alt=""></a></p><p>滑动窗口的意思是说把固定时间片，进行划分，并且随着时间的流逝，进行移动，这样就巧妙的避开了计数器的临界点问题。也就是说这些固定数量的可以移动的格子，将会进行计数判断阀值，因此格子的数量影响着滑动窗口算法的精度。</p><p><strong>3. 漏桶</strong></p><p>虽然滑动窗口有效避免了时间临界点的问题，但是依然有时间片的概念，而漏桶算法在这方面比滑动窗口而言，更加先进。</p><p>有一个固定的桶，进水的速率是不确定的，但是出水的速率是恒定的，当水满的时候是会溢出的。</p><p><a href="http://s4.51cto.com/oss/201811/15/e7afad0707d78ae63450d21fc29a09aa.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201811/15/e7afad0707d78ae63450d21fc29a09aa.jpg" alt=""></a></p><p>代码实现</p><p><a href="http://s2.51cto.com/oss/201811/15/cd8035934966d5399072a035daedfd16.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201811/15/cd8035934966d5399072a035daedfd16.jpg" alt=""></a></p><p><strong>4. 令牌桶</strong></p><p><a href="http://s5.51cto.com/oss/201811/15/7c7493ad5bb7f56a2ebec871ca90bb20.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201811/15/7c7493ad5bb7f56a2ebec871ca90bb20.jpg" alt=""></a></p><p>注意到，漏桶的出水速度是恒定的，那么意味着如果瞬时大流量的话，将有大部分请求被丢弃掉(也就是所谓的溢出)。为了解决这个问题，令牌桶进行了算法改进。</p><p>生成令牌的速度是恒定的，而请求去拿令牌是没有速度限制的。这意味，面对瞬时大流量，该算法可以在短时间内请求拿到大量令牌，而且拿令牌的过程并不是消耗很大的事情。(有一点生产令牌，消费令牌的意味)</p><p>不论是对于令牌桶拿不到令牌被拒绝，还是漏桶的水满了溢出，都是为了保证大部分流量的正常使用，而牺牲掉了少部分流量，这是合理的，如果因为极少部分流量需要保证的话，那么就可能导致系统达到极限而挂掉，得不偿失。</p><p>代码实现</p><p><a href="http://s3.51cto.com/oss/201811/15/0cf3223246b8b0a9dbd9720ab43acd80.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/15/0cf3223246b8b0a9dbd9720ab43acd80.jpg" alt=""></a></p><p><strong>限流神器：Guava RateLimiter</strong></p><p>Guava不仅仅在集合、缓存、异步回调等方面功能强大，而且还给我们封装好了限流的API!</p><p>Guava RateLimiter基于令牌桶算法，我们只需要告诉RateLimiter系统限制的QPS是多少，那么RateLimiter将以这个速度往桶里面放入令牌，然后请求的时候，通过tryAcquire()方法向RateLimiter获取许可(令牌)。</p><p>代码示例</p><p><a href="http://s3.51cto.com/oss/201811/15/642343bd1b87cd3351962928899a4fba.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201811/15/642343bd1b87cd3351962928899a4fba.jpg" alt=""></a></p><p><strong>分布式场景下的限流</strong></p><p>上面所说的限流的一些方式，都是针对单机而言的，其实大部分的场景，单机的限流已经足够了。分布式下限流的手段常常需要多种技术相结合，比如Nginx+Lua，Redis+Lua等去做。</p><p>本文主要讨论的是单机的限流，这里就不在详细介绍分布式场景下的限流了。</p><p>一句话，让系统的流量，先到队列中排队、限流，不要让流量直接打到系统上。</p><p>原文地址：<a href="http://developer.51cto.com/art/201811/586948.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201811/586948.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10种室内定位技术原理深度解析</title>
      <link href="/2018/10/26/10%E7%A7%8D%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2018/10/26/10%E7%A7%8D%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p><strong>1</strong></p><p><strong><a href="http://www.elecfans.com/uploads/allimg/171126/091945J45-1.gif" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/091945J45-1.gif" alt=""></a></strong></p><h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p>人类为了不让自己迷失在茫茫大自然中，先后发明罗盘、指南针等工具，卫星定位的问世，解决了“我在哪里”的问题。在高度城市化的今天，室内空间越来越庞大复杂。人类战胜了大自然，却在自己构筑的钢筋水泥中迷了路。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919454451-2.gif" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919454451-2.gif" alt=""></a><br><a id="more"></a><br>在医院，即使有楼层分布图以及引导标志，但看病的大部分时间仍然会浪费在寻找科室上。在停车场，找不着停车位而四处乱转的人也比比皆是。</p><p>在越来越迫切的需求下，近年室内定位引起了高度的关注。</p><p>室内定位顾名思义就是在室内环境中实现定位。其意义，诺基亚在多年前阐述他们为什么要做室内定位时，把问题说得很明白了。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919455420-3.png" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919455420-3.png" alt=""></a></p><p>数据来源：诺基亚</p><p>卫星无法在室内定位，人们大部分的时间又在室内度过，而相关服务并未普及，可以说室内定位隐藏着巨大商机。</p><p>商业应用中，根据不同的应用场景，室内定位技术又分消费级和工业级。</p><p>消费市场应用有：商场导购、停车场反向寻车、家人防走散等。对定位精度要求不高，1m的精度已经可以满足大多数应用，不过它要求系统兼容现已普及的移动智能终端。</p><p>企业市场应用有：人流监控和分析、智能制造、紧急救援和人员资产管理等。工业级技术的定位精度要求更高，要区分操作对象、人群中的个人等，与专用标签和<a href="http://www.elecfans.com/tags/%E4%BC%A0%E6%84%9F%E5%99%A8/" target="_blank" rel="noopener">传感器</a>配套使用，一般不考虑与现有智能终端的兼容性。</p><p><strong>2</strong></p><p><strong><a href="http://www.elecfans.com/uploads/allimg/171126/0919455312-4.gif" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919455312-4.gif" alt=""></a></strong></p><h3 id="技术门派"><a href="#技术门派" class="headerlink" title="技术门派"></a>技术门派</h3><p>与室外卫星定位一统天下的情况不一样，室内定位各种技术呈现出百花齐放的场景。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/091945AT-5.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/091945AT-5.jpg" alt=""></a></p><p>室内定位百花齐放</p><p>室内定位的商业价值跟精度成正比。当精度是3-5米的时候，能判定你是站在7-11便利店门口还是杰克琼斯门口。当精度是1米的时候，则能判定你是站在可口可乐的货架前还是杜蕾斯的货架前。</p><p>目前，室内定位常用的定位方法，从原理上来说，主要分为：邻近探测法、质心法、极点法、多边定位法、指纹法和航位推算法。</p><p>主要室内定位方法对比</p><p><img src="../10种室内定位技术原理深度解析/01.png" alt="定位方法对比"></p><p>各种原理各有优劣，在不同应用场景、不同预算要求下，也可将不同的原理组合使用。主流技术有以下几种：</p><p><strong>WiFi定位技术</strong></p><p>目前WiFi是相对成熟且应用较多的技术，这几年有不少公司投入到了这个领域。WiFi室内定位技术主要有两种。</p><p>WiFi定位一般采用“近邻法”判断，即最靠近哪个热点或基站，即认为处在什么位置，如附近有多个信源，则可以通过交叉定位（三边定位），提高定位精度。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/091945J50-6.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/091945J50-6.jpg" alt=""></a></p><p>由于WiFi已普及，因此不需要再铺设专门的设备用于定位。用户在使用智能手机时开启过<a href="http://www.elecfans.com/tags/wi-fi/" target="_blank" rel="noopener">Wi-Fi</a>、移动蜂窝网络，就可能成为数据源。该技术具有便于扩展、可自动更新数据、成本低的优势，因此最先实现了规模化。</p><p>不过，WiFi热点受到周围环境的影响会比较大，精度较低。为了做得准一点有公司就做了WiFi指纹采集，事先记录巨量的确定位置点的信号强度，通过用新加入的设备的信号强度对比拥有巨量数据的数据库，来确定位置。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919453029-7.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919453029-7.jpg" alt=""></a></p><p>由于采集工作需要大量的人员来进行，并且要定期进行维护，技术难以扩展，很少有公司能把国内的这么多商场定期的更新指纹数据。</p><p>WiFi定位可以实现复杂的大范围定位，但精度只能达到2米左右，无法做到精准定位。因此适用于对人或者车的定位导航，可以于医疗机构、主题公园、工厂、商场等各种需要定位导航的场合。</p><p><strong>代表公司有：WIFISLAM、Sensewhere、图聚智能</strong></p><p>另，地磁定位技术是利用室内不同位置的地磁场差异，来确定室内位置。与WiFi指纹类似，故不再作介绍。</p><p><strong>惯性导航技术</strong></p><p>这是一种纯客户端的技术，主要利用终端惯性传感器采集的运动数据，如加速度传感器、<a href="http://www.elecfans.com/tags/%E9%99%80%E8%9E%BA%E4%BB%AA/" target="_blank" rel="noopener">陀螺仪</a>等测量物体的速度、方向、加速度等信息，基于航位推测法，经过各种运算得到物体的位置信息。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/09194541W-8.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/09194541W-8.jpg" alt=""></a></p><p>随着行走时间增加，惯性导航定位的误差也在不断累积。需要外界更高精度的数据源对其进行校准。所以现在惯性导航一般和WiFi指纹结合在一起， 每过一段时间通过WiFi请求室内位置，以此来对MEMS产生的误差进行修正。该技术目前的商用得也比较成熟，在扫地机器人中得到广泛应用。</p><p><strong><a href="http://www.elecfans.com/tags/%E8%93%9D%E7%89%99/" target="_blank" rel="noopener">蓝牙</a>信标技术</strong></p><p>蓝牙信标技术目前部署的也比较多，也是相对比较成熟的技术。蓝牙跟WiFi的区别不是太大，精度会比WiFi稍微高一点。</p><p>该技术最先由诺基亚最先发起，但影响不大。2013年，苹果发布了基于蓝牙4.0低功耗协议（BLE）的iBeacon协议，主要针对零售业应用，引起广泛关注。</p><p>iBeacon蓝牙信标技术的正常运作，需要蓝牙信标硬件、智能终端上的应用、云端上的应用后台协同工作。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/09194541S-9.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/09194541S-9.jpg" alt=""></a></p><p>信标通过蓝牙向周围广播自身的ID，终端上的应用在获得附近信标的ID后会采取相应行动，如从云端后台拉取此ID对应的位置信息、营销资讯等。终端可以测量其所在处的接收信号强度，以此估算与信标间的距离。因此，只要终端附近有三个或以上信标，就可以用三边定位方法计算出终端的位置。</p><p>在苹果强大的号召力影响下，大量创业公司争先恐后涌入iBeacon应用的开发和推广。目前主要问题在于beacon电池更换，如果一个厂家部署了几万个beacon装置，一年之后或者电池耗尽之后的电池更换工作量是很繁重的。</p><p><strong>代表公司：Es<a href="http://www.elecfans.com/tags/%E5%BE%B7%E5%B7%9E%E4%BB%AA%E5%99%A8/" target="_blank" rel="noopener">ti</a>mo<a href="http://www.elecfans.com/tags/te/" target="_blank" rel="noopener">te</a>、寻息电子</strong></p><p>另，<a href="http://www.elecfans.com/tags/zigbee/" target="_blank" rel="noopener">ZigBee</a>技术和蓝牙类似，故不再作介绍。</p><p><strong><a href="http://www.elecfans.com/tags/rfid/" target="_blank" rel="noopener">RFID</a>技术</strong></p><p>RFID定位的基本原理是，通过一组固定的阅读器读取目标RFID标签的特征信息（如身份ID、接收信号强度等），同样可以采用近邻法、多边定位法、接收信号强度等方法确定标签所在位置。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919455C4-10.gif" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919455C4-10.gif" alt=""></a></p><p>射频识别室内定位技术作用距离很近，但它可以在几毫秒内得到厘米级定位精度的信息，且由于电磁场非视距等优点，传输范围很大，而且标识的体积比较小，造价比较低。但其不具有通信能力，抗干扰能力较差，不便于整合到其他系统之中，且用户的安全隐私保障和国际标准化都不够完善。</p><p>目前有大量成熟的商用定位方案基于RFID技术，广泛应用于紧急救援、资产管理、人员追踪等领域。</p><p><strong>红外技术</strong></p><p>红外定位主要有两种具体实现方法，一种是将定位对象附上一个会发射红外线的电子标签，通过室内安放的多个红外传感器测量信号源的距离或角度，从而计算出对象所在的位置。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/09194535X-11.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/09194535X-11.jpg" alt=""></a></p><p>这种方法在空旷的室内容易实现较高精度，可实现对红外辐射源的被动定位，但红外很容易被障碍物遮挡，传输距离也不长，因此需要大量密集部署传感器，造成较高的硬件和施工成本。此外红外易受热源、灯光等干扰，造成定位精度和准确度下降。</p><p>该技术目前主要用于军事上对飞行器、坦克、导弹等红外辐射源的被动定位，此外也用于室内自走机器人的位置定位。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919453124-12.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919453124-12.jpg" alt=""></a></p><p>另一种红外定位的方法是红外织网，即通过多对发射器和接收器织成的红外线网覆盖待测空间，直接对运动目标进行定位。</p><p>这种方式的优势在于不需要定位对象携带任何终端或标签，隐蔽性强，常用于安防领域。劣势在于要实现精度较高的定位需要部署大量红外接收和发射器，成本非常高，因此只有高等级的安防才会采用此技术。</p><p><strong>超声波技术</strong></p><p>超声波定位主要采用反射式测距法，通过多边定位等方法确定物体位置，系统由一个主测距器和若干接收器组成，主测距仪可放置在待测目标上，接收器固定于室内环境中。定位时，向接收器发射同频率的信号，接收器接收后又反射传输给主测距器，根据回波和发射波的时间差计算出距离，从而确定位置。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919454105-13.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919454105-13.jpg" alt=""></a></p><p>超声波定位整体定位精度较高，结构简单，但超声波受多径效应和非视距传播影响很大，且超声波频率受多普勒效应和温度影响，同时也需要大量基础硬件设施，成本较高。</p><p><strong>代表公司：Shopkick</strong></p><p><strong>超宽带技术</strong></p><p>超宽带（UWB）定位技术利用事先布置好的已知位置的锚节点和桥节点，与新加入的盲节点进行通讯，并利用三角定位或者“指纹”定位方式来确定位置。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919454M2-14.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919454M2-14.jpg" alt=""></a></p><p>从技术上看，无论是从定位精度、安全性、抗干扰、功耗等角度来分析，UWB无疑是最理想的工业定位技术之一。</p><p>UWB其他几种技术的综合比较：</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919455U2-15.png" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919455U2-15.png" alt=""></a></p><p>不过UWB的劣势也很突出，一方面难以实现大范围室内覆盖，另一方面系统建设成本远高于RFID、蓝牙信标等技术，这也限制了该技术的推广和普及。</p><p><strong>代表公司：Ubisense、中海达子公司联睿电子、清华系公司清研讯科。</strong></p><p><strong>LED可见光技术</strong></p><p>可见光是一个新兴领域，通过对每个LED灯进行编码，将ID调制在灯光上，灯会不断发射自己的ID，通过利用手机的前置摄像头来识别这些编码。利用所获取的识别信息在地图数据库中确定对应的位置信息，完成定位。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919452318-16.png" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919452318-16.png" alt=""></a></p><p>根据灯光到达的角度进一步细化定位的结果，<a href="http://www.elecfans.com/tags/%E9%AB%98%E9%80%9A/" target="_blank" rel="noopener">高通</a>公司做到了厘米级定位精度。由于不需要额外部署基础设施，终端数量的扩大对性能没有任何的影响，并且可以达到一个非常高的精度，该技术被高通公司所看好。</p><p>目前，可见光技术在北美有很多商场已经在部署。用户下载应用后，到达商场里的某一个货架，通过检测货架周围的灯光即可知晓具体位置，商家在通过这样的方法向消费者推动商品的折扣等信息。</p><p><strong>代表企业：华策光通信</strong></p><p><a href="http://www.elecfans.com/uploads/allimg/171126/0919454C5-17.png" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919454C5-17.png" alt=""></a></p><p><strong>3</strong></p><p><strong><a href="http://www.elecfans.com/uploads/allimg/171126/0919452135-18.gif" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/0919452135-18.gif" alt=""></a></strong></p><h3 id="技术融合是趋势"><a href="#技术融合是趋势" class="headerlink" title="技术融合是趋势"></a>技术融合是趋势</h3><p>室内定位技术处在不断的发展中，是当前热门研究领域，有着良好的应用前景。不过，当前还缺乏特别大规模的商用案例。行业主要存在以下难点：</p><p><strong>1. 室内环境复杂</strong></p><p>室内环境布局复杂多变，障碍物很多，包括家具、房间和行人等。同时室内环境干扰源多，灯光、温度、声音等干扰源都会对定位造成一定影响。</p><p>各技术综合比较</p><p><img src="../10种室内定位技术原理深度解析/02.png" alt="各技术综合比较"></p><p><strong>2. 缺乏统一的规范</strong></p><p>室内定位技术众多，各种技术都有自己的局限性，彼此间又在一定程度上存在互相竞争。市场相对混乱，极大地影响了室内定位行业的发展。如室外定位卫星定位成为事实上的标准，目前没有其他技术可以和卫星定位进行竞争。</p><p><strong>3. 精度与成本难以兼顾</strong></p><p>目前的高精度室内定位技术均需要比较昂贵的额外辅助设备或前期大量的人工处理，这些都大大制约了技术的推广普及。低成本的定位技术则在定位精度上需要提高。在提供高精度定位的基础上降低成本也是室内定位的一个方向。</p><p><a href="http://www.elecfans.com/uploads/allimg/171126/091945J54-19.jpg" target="_blank" rel="noopener"><img src="http://www.elecfans.com/uploads/allimg/171126/091945J54-19.jpg" alt=""></a></p><p>天下大势，分久必合，合久必分。目前，室内定位技术实在太多，已严重阻碍行业发展，未来的趋势一定是多种技术融合使用，实现优势互补，以面对复杂环境。其中成本越低、兼容性越好、精度越高的技术越容易普及。</p><p>原文地址：<a href="http://www.elecfans.com/d/586741.html" target="_blank" rel="noopener">http://www.elecfans.com/d/586741.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 物联网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018年最值得关注的15大技术趋势</title>
      <link href="/2018/10/25/2018%E5%B9%B4%E6%9C%80%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%8415%E5%A4%A7%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF/"/>
      <url>/2018/10/25/2018%E5%B9%B4%E6%9C%80%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%8415%E5%A4%A7%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF/</url>
      
        <content type="html"><![CDATA[<p>编者按：本文列举了2018年最值得关注的15大技术趋势，分别是区块链、狭义人工智能、地下交通系统、5G无线网络、比特币、增强现实、无人驾驶汽车、新太阳能技术、家庭虚拟助理、量子计算、外科手术机器人、飞行出租车、激光互联网、物联网、AR头戴设备。</p><p>通常情况下，技术趋势是很难准确预测的，因为预测未来本身就极其困难。但是我们还是可以从2017 年的一些显著的数据指标来推测2018年科技行业的一些发展趋势的。<br><a id="more"></a><br>许多人对科技行业概念的理解过于具体和狭隘了，他们只将如智能手机、无人机等视为科技行业，而本篇列出的技术趋势也包含很多更加复杂的概念，比如机器人技术、激光传输互联网、分散式医疗记录系统等等，这些与那些老生常谈的技术路线是不尽相同的。即使所列出的这些技术趋势中仅有一半能取得比现在更大的进展，2018年也势必会成为一个令人兴奋的一年。<br><!--more--></p><h3 id="（1）区块链将得到更广泛的应用。"><a href="#（1）区块链将得到更广泛的应用。" class="headerlink" title="（1）区块链将得到更广泛的应用。"></a>（1）区块链将得到更广泛的应用。</h3><p><img src="https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=641153543,2730159210&amp;fm=173&amp;s=1B19F3A283E886F91060D40B0300A0C2&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>区块链是一种每一个人都能够分享和访问的电子分类账，交易的双方可通过区块链来跟踪交易记录。区块链这个词在整个2017年都备受大家关注，这是因为加密货币比特币采用了一个分散式区块链来跟踪它的所有交易记录，然而区块链技术的应用范围远不限于比特币，它还有更广泛的应用范围。</p><p>有些人希望将区块链技术能够应用在病历记录上，病人的病史可通过不同的数据库和软件集中导入一个加密数据库。这么做的益处在于：病人的个人病史可以存储在一个安全的、分散的地方，医生在不通过的办公室和医疗中心软件都能访问这个数据库，这样一来，这样就不存在让不同软件或办公室间相互协作来升级数据库的问题了。</p><h3 id="（2）狭义人工智能进一步扩展。"><a href="#（2）狭义人工智能进一步扩展。" class="headerlink" title="（2）狭义人工智能进一步扩展。"></a>（2）狭义人工智能进一步扩展。</h3><p><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2757719872,3704918492&amp;fm=173&amp;s=9695216C53B5BE760E5558090100E0C0&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>最近一段时间以来，关于人工智能有很多不同的说法，但是让很多公司在2018年真正获益的地方在于采用那些擅于解决一个非常具体的任务的算法和电脑智能，这通常被称为狭义人工智能。</p><p>狭义人工智能与通用人工智能（想象一下试图接管世界的自动机器人）是有区别的。狭义人工智能只是让电脑完成一些非常具体的工作，比如驾驶汽车。狭义人工智能可以应用在所有的地方，从语言翻译软件到 Facebook Newsfeed中的内容算法，2018年，很有可能我们将看到有更多公司使用狭义人工智能来让公司的业务变得更高效，同时进一步提高公司产能。</p><h3 id="（3）地下交通方式将会获得更多牵引力。"><a href="#（3）地下交通方式将会获得更多牵引力。" class="headerlink" title="（3）地下交通方式将会获得更多牵引力。"></a>（3）地下交通方式将会获得更多牵引力。</h3><p><img src="https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=1409043692,3548513542&amp;fm=173&amp;s=771257815E57980D6314E8D10300F0B3&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>2016年年末，特斯拉CEO Elon Musk宣布，他将成立一家致力于解决洛杉矶众所周知的交通问题新公司。就在去年，Musk的新公司 Boring Company挖掘了一条长达 150 多米的隧道来测试一个全新的地下高速轨道系统，旨在为乘客提供更加便利和高速的出行服务。</p><p>Boring Company可能会提供两种类型的交通方式，第一种方式是一种能够一次将超过12名乘客以241 千米/小时的速度从一个城市运送到另外一个城市的轨道系统；另一种方式则是 Musk 提出的Hyperloop超级高铁概念，它由在近真空电子管中以700英里/小时的速度飞行的舱室组成。</p><p>Boring Company目前正在洛杉矶打造 一个6.5英里长的隧道用于概念验证，如果这个项目获得该城市的最终批准，它最终可能会变成洛杉矶的地下隧道系统。Boring公司在马里兰州也在挖掘一个用于测试的地下交通轨道，这个轨道最终也将变成华盛顿特区和纽约市之间的一个地下交通系统。这些测试隧道在 2018 年不会完工，不过我们在2018年肯定能了解有关它们的更多信息。</p><h3 id="（4）5G无线带宽将推出。"><a href="#（4）5G无线带宽将推出。" class="headerlink" title="（4）5G无线带宽将推出。"></a>（4）5G无线带宽将推出。</h3><p><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2444212636,480987629&amp;fm=173&amp;s=C8231D721B617C1FC6D3C5C20300F0BF&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>AT&amp;T和Verizon 在 2017 年就已经开始测试 5G 无线网络了。这两家公司表示， 它们将在2018将这项服务进行有限地商用化。与 4G 和 LTE 不同，5G网络最初将主要用于家用网络领域，允许电信运营商通过无线连接向家庭提供多千兆比特的网速。作为对比，1G 的网速是目前美国家用带宽平均网速的 15 倍。</p><p>相比传统的有线网络连接，5G网络不仅速度更快，同时也能极大地降低家用网络的拥堵情况（试想一下你和邻居同时在线看电影时网速有多慢），延迟也会更低（可以很快地发送信息）。Verizon将在2018年在加利福尼亚的 Sacramento以及另外四个尚未宣布布的市场中推出 5G 网络。</p><h3 id="（5）比特币将会得到更多的关注和报道。"><a href="#（5）比特币将会得到更多的关注和报道。" class="headerlink" title="（5）比特币将会得到更多的关注和报道。"></a>（5）比特币将会得到更多的关注和报道。</h3><p><img src="https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=3608733206,1893236794&amp;fm=173&amp;s=FA1D7C8C52FA199ED059C10F0300F0CB&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>2017年年末，比特币价格的快速上涨得到了众多投资者和普通大众的关注，2018年，我们还会看到类似的趋势。比特币是一种允许它的所有者以匿名的方式安全地购买的加密货币。</p><p>有些零售商已经开始接受比特币富康了，比如Home Depot和Overstock.com，预计2018年将有更多的零售商接受比特币支付，尤其是在当比特币的价格持续上涨的情况下更是如此。但比特币的波动性仍然很高，它的在线交易场所（你买卖比特币的地方）经常成为黑客攻击的目标。这意味着，在2018年，比特币的价格既有可能进一步上涨，也有可能会下跌。不管是哪种情况，我们在2018年都会看到有过加密货币的更多报道。</p><h3 id="（6）增强现实将在移动设备上成为主流。"><a href="#（6）增强现实将在移动设备上成为主流。" class="headerlink" title="（6）增强现实将在移动设备上成为主流。"></a>（6）增强现实将在移动设备上成为主流。</h3><p><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=975013891,3034759054&amp;fm=173&amp;s=55303BD572201913AD0441280300F053&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>2017年，通过发布内置了增强显示功能的iOS11，苹果让数亿iPhone和iPad用户都使用上了增强现实功能，这是一个巨大的进展。苹果公司允许开发者可以开发自己的AR应用，比如宜家的 Place应用，这款应用允许用户扫描家中的房间，并在房间内添加虚拟家具，看看家居在房间内的实际效果。</p><p>Facebook在2017年也采取了一些措施，让用户在2018 年在Facebook的平台上更多地使用增强现实应用。Facebook新推出的 AR Studio 允许开发者为 Facebook 和 Messenger 应用开发滤镜和镜头效果插件，比如在自拍时添加增强现实面具。此外，苹果还将AR 滤镜的应用范围扩展到了 Instragram 应用里。根据德勤会计师事务所的研究，苹果和 Facebook对移动 AR的重视是2018年预计有 10 亿智能手机用户通过手机生成 AR 内容的重要原因之一。</p><h3 id="（7）我们将会看到世界上第一个真正的无人驾驶汽车服务的出现。"><a href="#（7）我们将会看到世界上第一个真正的无人驾驶汽车服务的出现。" class="headerlink" title="（7）我们将会看到世界上第一个真正的无人驾驶汽车服务的出现。"></a>（7）我们将会看到世界上第一个真正的无人驾驶汽车服务的出现。</h3><p><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=1600336291,2292672576&amp;fm=173&amp;s=DE80EA0946BD39820689C1DB010080B1&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>众多汽车制造商和大型科技公司在很多很长一段时间以来一直在测试无人驾驶汽车技术，但是在2018年，Alphabet旗下的无人驾驶汽车公司 Waymo将会取得更大突破。</p><p>在过去几个月时间里，Waymo一直在亚利桑那州凤凰城的研究无人驾驶工具。Waymo近期曾表示，Waymo将会在几个月时间之内在凤凰城推出无人驾驶汽车服务。这意味着在凤凰城里，Waymo的汽车将会按需接人，在驾驶位没有驾驶员的情况下就能把乘客送到凤凰城内的目的地。这不仅仅是Waymo向前迈进的一大步，也是整个无人驾驶汽车技术向前迈进的一大进步，意味着 Waymo公司将领先竞争对手几年时间。大多汽车制造商和技术公司都表示将于2021年推出自己的无人驾驶汽车服务。</p><h3 id="（8）新太阳能技术的发展。"><a href="#（8）新太阳能技术的发展。" class="headerlink" title="（8）新太阳能技术的发展。"></a>（8）新太阳能技术的发展。</h3><p><img src="https://ss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=4154187801,2648797519&amp;fm=173&amp;s=BAF7E504D7B1B1C814D184830100E083&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>2017年，麻省理工学院的一群科学家开始研究一种名为太阳能热光伏电池（Hot Solar Cells）的新型的太阳能面板技术。和传统的太能能光伏电板而言，太阳能热光伏电池在将太阳能转化为电能上更高效，是传统的太能能光伏电板的两倍，因为它使用的材料更先进，捕捉光线的流程更先进。</p><p>太阳能热光伏电池本质上是将光线导入碳纳米管，然后在将其加热至大概 1000摄氏度后，再将热量转移到聚焦于太阳能电池的光线中。和传统的太阳能面板相比，这个过程将太阳能转化为能源的效率更高，还能吸收并储存一些热能供今后使用。太阳能热光伏电池的研究现在还处在非常早期的阶段，但它可以在未来几年时间内让太阳能面板技术超越传统的太阳能面板</p><h3 id="（9）家用数字虚拟助理将会变得越来越流行。"><a href="#（9）家用数字虚拟助理将会变得越来越流行。" class="headerlink" title="（9）家用数字虚拟助理将会变得越来越流行。"></a>（9）家用数字虚拟助理将会变得越来越流行。</h3><p><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=1971030484,36171777&amp;fm=173&amp;s=E50E37764D23F32858D908D30000E032&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>像Amazon的 Echo（配有Alexa虚拟助手）这样的支持语音的智能音箱其实在2014 年就有了，但是到2017 年年底才得到了更多的关注和普及，相信这个趋势将会延续到 2018 年。这类智能音箱可以回答有关天气咨询、在线订购产品和网页搜索等服务，甚至还可以帮用户叫Uber或线上订Pizza。智能音箱还可以与其他智能家居产品配合起来使用，比如恒温器，这样一来，用户使用语音就能够控制家里的所有设备。</p><p>调研结构Juniper Research发布的数据现实，从2016年到2017年，使用这类智能音箱设备的美国人增加了将近 130%，预计到 2020 年，将会有超过一半的美国家庭都会拥有至少一台智能家用音箱。所有这些数据都意味着，2018年，我们将会看到越来越多的用户将会使用数字虚拟助理。</p><h3 id="（10）量子计算将会越来越普遍。"><a href="#（10）量子计算将会越来越普遍。" class="headerlink" title="（10）量子计算将会越来越普遍。"></a>（10）量子计算将会越来越普遍。</h3><p><img src="https://ss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=4206417327,2728889308&amp;fm=173&amp;s=BA1BA04C8FA4152E41F7F5930300E09A&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>量子计算是一个比较难理解的概念，对于量子计算最简单的解释是：借助量子力学定律，让计算机能够将信息同时处理 为1 和 0，传统的计算机只能将信息处理为 1 或 0。如此一来，计算机就能够一次性计算出多个结果，而不用按顺序处理答案。</p><p>因为量子计算机能够按这种方式处理信息，因此量子计算机远比传统计算机更为强大。2017年年底，Google在量子计算机领域取得了重大进展，目前Google已经为化学家和材料科学家提供免费的、开源量子计算软件。</p><p>虽然目前的量子计算机还处于非常早期的阶段，但是很多科技巨头都相信，量子计算机很快就能用来解答气候变迁问题，并且有助于新的医学发现的出现。Google、IBM、微软等公司已经在量子计算领域加大了投入，Google最新的软件与 IBM 等公司的量子计算机是兼容的，这为量子计算技术在2018年的广泛采用铺平了道路。</p><h3 id="（11）外科手术机器人将会进入越来越多的手术室。"><a href="#（11）外科手术机器人将会进入越来越多的手术室。" class="headerlink" title="（11）外科手术机器人将会进入越来越多的手术室。"></a>（11）外科手术机器人将会进入越来越多的手术室。</h3><p><img src="https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=3106730231,2730983978&amp;fm=173&amp;s=0AD20CCD1AA24A1718B1609E0300C092&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>多年以来，Intuitive Surgical 公司在外科手术机器人市场一直占据主导地位，这家公司的达芬奇手术机器人从2000年开始就已经投入使用了，但是因为这家公司的专利即将到期，这就为其它科技公司和大型医疗公司进入外科手术机器人市场打开了一扇大门。</p><p>外科手术机器人不能自己做手术，需要由外科医生来控制才可以。在有些情况下，医生在外科手术机器人的配合下能够更精确地完成手术。如今，这些外科手术机器人已经变得越来越智能了。最近《经济学人》撰文表示，Google已经开始与Johnson &amp; Johnson合作建立了一家名为 Verb Surgical合资公司，这家公司已经推出了可联网的手术机器人。这种手术机器人能够与其他外科手术机器人共享数据和视频信息，然后再利用机器学习来改善手术的效果。随着越来越多的公司进入这一市场，而且Google进一步扩展了这些机器人协助手术的方式，2018年，我们势必将在这一领域看到更多突破。</p><h3 id="（12）飞行出租车将会出现。"><a href="#（12）飞行出租车将会出现。" class="headerlink" title="（12）飞行出租车将会出现。"></a>（12）飞行出租车将会出现。</h3><p><img src="https://ss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=3428282064,1894424645&amp;fm=173&amp;s=27BA642334BC67802E6138C70100E0E3&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>如今，已经有很多公司吹嘘自己有在大型城市推出无人驾驶飞行器（AAV）的能力，但是一家名为“ 亿航”的中国公司将在2018年率先将这一想法变为现实。这家公司目前已经有一架单人的飞行出租车原型机，公司宣称将在 2018 年开始量产能搭载两名乘客的飞行出租车。</p><p>亿航的 e-184 型看起来就像是一个大型的无人机，它的顶部是一个能够容纳一个人的座舱，它能够以高达 62英里公里/小时的速度飞行到预先设置好的目的地，电池可供支持 25 分钟的航行时间。亿航目前已经签署了一份合同，根据合同，亿航将于2018 年将飞行出租车带到迪拜。目前亿航还在与欧洲的一些城市就引进飞行出租车产品进行洽谈。飞行出租车未来究竟能够变得有多普及，现在还很难预测。但是，2018年将是飞行出租车变成现实的一年。</p><h3 id="（13）Alphabet将用激光为印度提供互联网服务。"><a href="#（13）Alphabet将用激光为印度提供互联网服务。" class="headerlink" title="（13）Alphabet将用激光为印度提供互联网服务。"></a>（13）Alphabet将用激光为印度提供互联网服务。</h3><p><img src="https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=307612307,4260216421&amp;fm=173&amp;s=1387D302BD4C2F7E80CF5983030070CA&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>Alphabet旗下的X研究部门在2018年将向印度政府出售部分高科技激光束技术，这样它就能为数千万印度用户提供高速互联网服务。这种设备使用的是“空间光学技术”，它是由用来相互间发送和接收激光信号的盒子组成，进而来传输互联网信号，而不是采用常用的铜线或光纤材料。最终的互联网信号将会通过Wifi或者蜂窝信号发送给用户。因为它使用光来发送信号，所以它是终极的无线信号。</p><p>路透社近日发表的一篇文章表示，Alphabet将提供 2000 个空间光盒，这些光盒彼此间距离大概20公里，大都位于屋顶和电线杆上。这些设备能以 20G/秒的速度发送网络信号。太空光学技术通常会遭遇与天气有关的问题障碍（它非常不喜欢雨雾天气），但 Alphabet表示公司的技术已解决了这里面的一部分问题。</p><h3 id="（14）物联网继续高歌猛进。"><a href="#（14）物联网继续高歌猛进。" class="headerlink" title="（14）物联网继续高歌猛进。"></a>（14）物联网继续高歌猛进。</h3><p><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2690314636,3187657758&amp;fm=173&amp;s=4FB62EC01631402D19BEFD520300F099&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>物联网（IoT）指的是指那些有传感器来收集数据、并且能够连接到互联网的设备，它们产生的信息可用来分析或者用于自动化。这些设备可以是任何东西，从Apple Watct这类智能手表到能够向计算机发送信号、告知计算机某个机械部件即将失效的工业机械。</p><p>2016年，在使用中的物联网设备有64 亿个；到2018年，物联网设备数量将达112亿。随着物联网设备数量的不断增加，而且随着物联网设备接管越来越多的工业和企业系统，它们就需要变得更加安全、更容易更新。正因为如此，我们很可能会在未来一年里看到更多的物联网平台软件的出现，而且它的安全性也会继续提高。许多物联网设备非常小，也非常便宜，这意味着基于云计算的软件和基于网络的安全可能是日益增长的物联网市场的关键。市场研究公司Gartner表示，从2018年开始，我们将会看到更多市场空间更大、成本更低的物联网设备的出现，这些设备将会用于生活中的各个领域，从互联安全系统到智能LED灯等等。</p><h3 id="（15）AR头戴设备的归来。"><a href="#（15）AR头戴设备的归来。" class="headerlink" title="（15）AR头戴设备的归来。"></a>（15）AR头戴设备的归来。</h3><p><img src="https://ss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=2163013440,1033266509&amp;fm=173&amp;s=9EE3EA0146505C6258988DD20300E090&amp;w=639&amp;h=298&amp;img.JPEG" alt=""></p><p>2012年，Google就已经推出了一款名为Google Glass的增强现实眼镜，但是这款设备最终在大家的广泛关注下失败了，因为它无法为用户带来实际的用处。但是在这之后，Google又发布了一款企业级的AR眼镜。Google投资过的AR公司Magic Leap 也将于2018 年发布“混合现实”眼镜。</p><p>Magic Leap的混合现实眼镜名为“Lightwear”，用于可以用它来浏览互联网、开电话会议或者玩游戏。这款设备的报价目前尚未公布。不过Lightwear的预计销售对象是软件开发者，这样这些开发者就能为这款眼镜开发更多内容和应用。到目前为止，Magic Leap共获得约19亿美元的融资。备受外界期待的 Lightwear眼镜的发布很可能会成为我们将在2018年看到的众多AR眼镜中的第一个。</p><p>当然了，现在还无法知道上面列的这些技术趋势中的哪些趋势将会在2018 年实现真正的腾飞。但是，上面这些技术趋势都是建立在现有的技术创新的基础之上的，这意味着它们已经不仅仅是昙花一现的想法了。如果让我自己选择几个我希望能够在2018年实现的技术趋势的话，我希望无人驾驶汽车、增强现实技术、物联网以及家用虚拟助理在2018年能够实现更大突破。</p><p>原文地址：<a href="https://baijiahao.baidu.com/s?id=1592977883484480025&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1592977883484480025&amp;wfr=spider&amp;for=pc</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据工程师必备技能（转载）</title>
      <link href="/2018/10/22/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E5%A4%87%E6%8A%80%E8%83%BD%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/10/22/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E5%A4%87%E6%8A%80%E8%83%BD%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="市场岗位"><a href="#市场岗位" class="headerlink" title="市场岗位"></a>市场岗位</h3><p>1、大数据的系统架构（掌握计算机体系结构、网络架构、编程范式、文件系统、分布并行处理）</p><p>2、大数据的系统分析（人工智能、机器学习、数理统计、矩阵计算、优化方法）</p><p>3、hadoop开发（大数据存储问题）</p><p>4、数据分析（至少需要熟练SPSS、STATISTIC、Eviews、SAS、大数据魔镜等数据分析软件中的一门，至少能用Acess等进行数据库开发，至少掌握一门数学软件如matalab、mathmatics进行新模型的构建，至少掌握一门编程语言。优秀的分析师可以掌握业务、管理、分析、工具、设计兼顾）</p><p>5、数据挖掘（数学知识要强一些，编程语言要掌握Java、Python）</p><p>6、大数据可视化（有点高级哈哈，涉及D3啊R啊等一些编程语言和Excel以及一些可视化工具，可以自行查阅一下或者看下面提及到的技能图谱）</p><a id="more"></a><p>下图是StuQ所总结的大数据工程师所掌握的技能图谱：<br><img src="../大数据工程师必备技能（转载）/20181008004058740.jpg" alt="大数据工程师必备技能（转载）"></p><h3 id="本科知识体系"><a href="#本科知识体系" class="headerlink" title="本科知识体系"></a>本科知识体系</h3><p>数学基础：</p><p>1、数学分析，华东大的教材，习题是谢惠民版的&lt;数学分析习题课讲义&gt;</p><p>2、高等代数，蓝以中版。</p><p>3、离散数学，”十二五”规划的邓辉文&lt;离散数学第三版&gt;+配套的离散数学习题解答。或是著名屈婉玲老教授的离散数学第三版+配套的离散数学习题解答与学习指导第三版。这个自我感觉自学起来很难学，本人科班，老师自己讲都是一知半解，学透还是别想了，看国外著作感觉也没那么专业，有时间有能力还是可以去看罗森的&lt;离散数学及其应用&gt;，我感觉看看国内的那些指定用书就行了，其他的教材都是抄来抄去的hh。</p><p>4、概率论与数理统计，经典教材浙大第四版。</p><p>计算机基础：</p><p>1、编程语言（一般高校是c/cpp程序设计入门，但java和python基础是必须的，我自己认为javaee和大数据没什么联系，只不过是java语言在web上做的开发，当然作为科班也不得不学，就全当对Java的扩充了。html和json需要了解和看懂）</p><p>2、Linux（科班学的是操作系统原理，非网络专业都没接触过，但感觉这个已经成为了企业标配，学大数据也得会Linux基本命令，操作管理和shell编程）</p><p>3、数据库原理（知其原理还是挺重要的，用mysql操作语句是基础）</p><p>4、数据结构（掌握链、队列、链表散列表、树、图，c和cpp没学好比如我就只能看java版了QAQ）</p><p>5、算法（我感觉王宏志的大数据算法挺不错的，日后有需要还得再补，算法是灵魂）</p><p>然后是一些基本技能/知识。</p><p>6、JDBC    java程序与数据的接口，应该算在Java SE体系里了。</p><p>7、正则表达式    文本模式，动态处理</p><p>8、git，GitHub       版本控制工具   <a href="https://www.zhihu.com/question/20070065/answer/30521531" target="_blank" rel="noopener">https://www.zhihu.com/question/20070065/answer/30521531</a></p><p>9、maven   java开发的项目自动化构建工具</p><p>知识性补充：</p><p>有人说，操作系统、编译与计算机组成原理在三门，你对它们的理解决定了你水平的高低。</p><p>不谈正确性，但足以有一定的重要性。</p><p>计算机网络和汇编，一个的网络知识相关，一个是底层编程原理的理解，我就了解个什么前端后台编程，客户端与服务器的交互，tcp/ip协议，网络安全信息加密知道个什么REA啊DES啊哈希算法Md5密钥啊之类的这些常见的。更多的CS专业补充可参考知乎搜索263445600。</p><p>作者：Facenoboy<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/qq_36042506/article/details/82948473" target="_blank" rel="noopener">https://blog.csdn.net/qq_36042506/article/details/82948473</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>假如有1亿用户的访问量,你的服务器架构是怎样的</title>
      <link href="/2018/09/28/%E5%81%87%E5%A6%82%E6%9C%891%E4%BA%BF%E7%94%A8%E6%88%B7%E7%9A%84%E8%AE%BF%E9%97%AE%E9%87%8F-%E4%BD%A0%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84/"/>
      <url>/2018/09/28/%E5%81%87%E5%A6%82%E6%9C%891%E4%BA%BF%E7%94%A8%E6%88%B7%E7%9A%84%E8%AE%BF%E9%97%AE%E9%87%8F-%E4%BD%A0%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84%E6%98%AF%E6%80%8E%E6%A0%B7%E7%9A%84/</url>
      
        <content type="html"><![CDATA[<p>我们以淘宝架构为例，了解下大型电商项目的服务端架构是怎样的，如图1所示：</p><p>上面是一些安全体系系统，如数据安全体系、应用安全体系、前端安全体系等。中间是业务运营服务系统，如会员服务、商品服务、店铺服务、交易服务等。还有共享业务，如分布式数据层、数据分析服务、配置服务、数据搜索服务等。最下面是中间件服务，如MQS即队列服务，OCS即缓存服务等。</p><p>图中也有一些看不到，例如高可用的体现、实现双机房容灾和异地机房单元化部署，为淘宝业务提供稳定、高效和易于维护的基础架构支撑。</p><p><img src="http://s3.51cto.com/oss/201809/28/fb7aaa44433103f4ce10a5666994b912.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？"><br><a id="more"></a></p><center>图1</center><p>这是一个含金量非常高的架构，也是一个非常复杂而庞大的架构。当然这个架构不是一天两天演进而成，也不是一上来就设计并开发成这么高大上的。</p><p>这边我想说的是，小型公司要怎么做架构呢?对很多创业公司而言，很难在初期就预估到流量十倍、百倍以及千倍以后的网站架构会是一个怎样的状况。同时，如果系统初期就设计一个千万级并发的流量架构，也很难有公司可以支撑这个成本。</p><p>因此，一个大型服务系统都是从一步一步走过来的，在每个阶段，找到对应该阶段网站架构所面临的问题，然后在不断解决这些问题，在这个过程中整个架构会一直演进。</p><p><strong>一、单服务器-俗称all in one</strong></p><p><img src="http://s3.51cto.com/oss/201809/28/c384331f1705fee3287b0f703f38defc.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>从一个小网站说起。一台服务器也就足够了。文件服务器，数据库，还有应用都部署在一台机器，俗称ALL IN ONE。</p><p>随着我们用户越来越多，访问越来越大，硬盘、CPU、内存等都开始吃紧，一台服务器已经满足不了。这时看到下一步演进。</p><p><strong>二、数据服务与应用服务分离</strong></p><p><img src="http://s1.51cto.com/oss/201809/28/8cb357bd58b77d24a94a017f28b51aaa.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？"></p><p><img src="http://s3.51cto.com/oss/201809/28/e7e60c54105b29e4da72b6b039921062.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p><strong>三、使用缓存</strong></p><p>包括本地缓存、远程缓存、远程分布式缓存。</p><p><img src="http://s2.51cto.com/oss/201809/28/03a119c5a3decdb6e5bfbd46e4edab25.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>因为 80% 的业务访问都集中在 20% 的数据上，也就是我们经常说的28法则。如果能将这部分数据缓存下来，性能一下子就上来了。而缓存又分为两种：本地缓存和远程缓存缓存，以及远程分布式缓存，我们这里面的远程缓存图上画的是分布式的缓存集群(Cluster)。</p><p>思考的点</p><ul><li>具有哪种业务特点数据使用缓存?</li><li>具有哪种业务特点的数据使用本地缓存?</li><li>具有哪种务特点的数据使用远程缓存?</li><li>分布式缓存在扩容时会碰到什么问题?如何解决?分布式缓存的算法都有哪几种?各有什么优缺点?</li></ul><p><img src="http://s3.51cto.com/oss/201809/28/a6733109939500f5645fb149a2899d52.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？"></p><p><strong>四、使用负载均衡，进行服务器集群</strong></p><p><img src="http://s2.51cto.com/oss/201809/28/f2eafb0f10cab28d1f78fe3aa5796da6.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？"></p><p>增加了负载均衡、服务器集群之后，我们可以横向扩展服务器，解决了服务器处理能力的瓶颈。</p><p>思考的点</p><ul><li>负载均衡的调度策略都有哪些?</li><li>各有什么优缺点?</li><li>各适合什么场景?</li></ul><p>打个比方，我们有轮询、权重、地址散列，地址散列又分为原ip地址散列hash、目标ip地址散列hash，最少连接，加权最少连接，还有继续升级的很多种策略……我们都来分析一下。</p><p><strong>典型负载均衡策略分析</strong></p><ul><li>轮询：优点-实现简单，缺点-不考虑每台服务器处理能力</li><li>权重：优点-考虑了服务器处理能力的不同</li><li>地址散列：优点-能实现同一个用户访问同一个服务器</li><li>最少连接：优点-使集群中各个服务器负载更加均匀</li><li>加权最少连接：在最少连接的基础上，为每台服务器加上权值。算法为(活动连接数*256+非活动连接数)/权重，计算出来的值小的服务器优先被选择。</li></ul><p><strong>继续引出问题的场景</strong></p><p><img src="http://s2.51cto.com/oss/201809/28/29a13f7139949cfcc797fbbaa368a2a3.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p><strong>Session管理-Session Sticky粘滞会话</strong></p><p><img src="http://s4.51cto.com/oss/201809/28/afcbaf1989859ea6e5689787ae616f3d.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>打个比方，如果我们每次吃饭都要保证用的是自己的碗筷，只要我们在一家饭店里存着自己的碗筷，并且每次去这家饭店吃饭就好了。</p><p>对于同一个连接中的数据包，负载均衡会将其转发至后端固定的服务器进行处理。</p><p>解决了我们session共享的问题，但是它有什么缺点呢?</p><p>一台服务器运行的服务挂掉，或者重启，上面的 session 都没了。</p><p>负载均衡器成了有状态的机器，为以后实现容灾造成了羁绊。</p><p><strong>Session管理-Session 复制</strong></p><p><img src="http://s5.51cto.com/oss/201809/28/9b6fc3e509ee6f6b0d41ac532d83225a.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>就像我们在所有的饭店里都存一份自己的碗筷。这样随意去哪一家饭店吃饭都OK，不适合做大规模集群，适合机器不多的情况。</p><p><strong>解决了我们session共享的问题，但是它有什么缺点呢?</strong></p><p>应用服务器间带宽问题，因为需要不断同步session数据。</p><p>大量用户在线时，服务器占用内存过多。</p><p><strong>Session管理-基于Cookie</strong></p><p><img src="http://s4.51cto.com/oss/201809/28/c749a3c5863b417b8612791a5055c290.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>打个比方，就是我们每次去饭店吃饭，都带着自己的碗筷去。</p><p>解决了我们session共享的问题，但是它有什么缺点呢?</p><p>cookie 的长度限制。</p><p>cookie存于浏览器，安全性是一个问题。</p><p><strong>Session管理-Session 服务器</strong></p><p><img src="http://s5.51cto.com/oss/201809/28/5f677161a6af2b65e53a815bc2071aa7.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>打个比方，就是我们的碗筷都存在了一个庞大的橱柜里，我们去任何一家饭店吃饭，都可以从橱柜中拿到属于我们自己的碗筷。</p><p>解决了我们session共享的问题，这种方案需要思考哪些问题呢?</p><p><strong>保证 session 服务器的可用性，session服务器单点如何解决?</strong></p><p>我们在写应用时需要做调整存储session的业务逻辑。</p><p>打个比方，为了提高session server的可用性，我们可以继续给session server做集群。</p><p><strong>五、中间总结</strong></p><p>所以网站架构在遇到某些指标瓶颈、演进的过程中，都有哪些解决方案?它们都有什么优缺点?业务功能上如何取舍?如何做出选择?这个过程才是最重要的。</p><p>在解决了横向扩展应用服务器之后，我们继续回到目前的架构图：</p><p><img src="http://s2.51cto.com/oss/201809/28/a521c7a94b4c19768ced766e453e6788.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>数据库的读及写操作都还需要经过数据库。当用户量达到一定量，数据库将会成为瓶颈。又该如何解决呢?</p><p><strong>六、数据库读写分离</strong></p><p><img src="http://s2.51cto.com/oss/201809/28/87723ed760a1b81962949b6757e099f4.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">   </p><p><img src="http://s2.51cto.com/oss/201809/28/71e56768bebce092c94726da693e123d.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>思考的点</p><ul><li>如何支持多数据源?</li><li>如何封装对业务没有侵入?</li><li>如何使用目前业务的ORM框架完成主从读写分离?是否需要更4. 换ORM模型?ORM模型之间各有什么优缺点?</li><li>如何取舍?</li></ul><p><strong>数据库读写分离会遇到如下问题：</strong></p><ul><li>在master和slave复制的时候，考虑延时问题、数据库的支持、复制条件的支持。</li><li>当为了提高可用性，将数据库分机房后，跨机房传输同步数据，这个更是问题。</li><li>应用对于数据源的路由问题。</li></ul><p><strong>七、使用反向代理和CDN加速网站响应</strong></p><p><img src="http://s2.51cto.com/oss/201809/28/97271280e93e510abc0717b370740fc8.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p><img src="http://s1.51cto.com/oss/201809/28/0cb238821fabe21429110bc39c1105d0.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p><strong>八、分布式文件系统</strong></p><p><img src="http://s2.51cto.com/oss/201809/28/fb6fe1668eb7c2d5086f1123310da3fa.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>思考的点</p><ul><li>分布式文件系统如何不影响已部署在线上的业务访问?不能让某个图片突然访问不到呀。</li><li>是否需要业务部门清洗数据?</li><li>是否需要重新做域名解析?</li></ul><p>这时数据库又出现了瓶颈。</p><p><strong>九、数据垂直拆分</strong></p><p><img src="http://s4.51cto.com/oss/201809/28/97b39fde63ca219231d9cc14a454c75d.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>数据库专库专用，如图Products、Users、Deal库。解决写数据时并发量大的问题。</p><p>思考的点</p><ul><li>跨业务的事务如何解决?使用分布式事务、去掉事务或不追求强事务。</li><li>应用的配置项多了。</li><li>如何跨库进行数据的join操作?</li></ul><p>这个时候，某个业务的数据表的数据量或者更新量达到了单个数据库的瓶颈。</p><p><strong>十、数据水平拆分</strong></p><p><img src="http://s5.51cto.com/oss/201809/28/881e578e6cd663be59a5bece835581ba.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p>如图，我们把User拆成了User1和User2，将同一个表的数据拆分到两个数据库中，解决了单数据库的瓶颈。</p><p>思考的点</p><ul><li>水平拆分的策略都有哪些?各有什么优缺点?</li><li>水平拆分的时候如何清洗数据?</li><li>SQL的路由问题，需要知道某个User在哪个数据库上。</li><li>主键的策略会有不同。</li></ul><p>假设系统中需要查询2017年4月份已经下单过的用户名的明细，而这些用户分布在user1和user2上，我们后台运营系统在展示时如何分页?</p><p>这个时候，公司对外部做了流量导入，我们应用中的搜索量飙升，继续演进。</p><p><strong>十一、拆分搜索引擎</strong></p><p><img src="http://s1.51cto.com/oss/201809/28/6b8c00b25c53faddc49de029c036ac0f.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">   </p><p><img src="http://s4.51cto.com/oss/201809/28/7c25855b8902747e1b22cc2995e87684.jpeg" alt="假如有1亿用户的访问量，你的服务器架构是怎样的？">  </p><p><strong>总结</strong></p><p>本文只是一个举例演示，各个服务的技术架构需要根据自己业务特点进行优化和演进，所以大家的过程也不完全相同。</p><p>最后的这个示例也不是完美的，例如负载均衡还是一个单点，也需要集群，我们这个架构也只是冰山一角。因为在架构演进的过程中，还要考虑系统的安全性、数据分析、监控、反作弊等，同时往后继续发展，也要考虑到SOA架构、服务化、消息队列、任务调度、多机房等。</p><p>从以上对架构演进的讲解，也可以看出来，所有大型项目的架构和代码，都是一步一步根据实际的业务场景和发展情况发展演变而来，在不同的阶段，会使用的不同的技术，不同的架构来解决实际的问题，所以说，高大上的项目技术架构和开发设计实现不是一蹴而就的。</p><p>正是所谓的万丈高楼平地起。在架构演进的过程中，小到核心模块代码，大到核心架构，都会不断演进的，这个过程值得我们去深入学习和思考。</p><p>原文地址：<a href="http://server.51cto.com/sOS-584263.htm" target="_blank" rel="noopener">http://server.51cto.com/sOS-584263.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从单机到2000万QPS：如何搭建高可用Redis平台</title>
      <link href="/2018/09/27/%E4%BB%8E%E5%8D%95%E6%9C%BA%E5%88%B02000%E4%B8%87QPS%EF%BC%9A%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Redis%E5%B9%B3%E5%8F%B0/"/>
      <url>/2018/09/27/%E4%BB%8E%E5%8D%95%E6%9C%BA%E5%88%B02000%E4%B8%87QPS%EF%BC%9A%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8Redis%E5%B9%B3%E5%8F%B0/</url>
      
        <content type="html"><![CDATA[<p>知乎存储平台团队基于开源 Redis 组件打造的 Redis 平台管理系统，经过不断的研发迭代，目前已经形成了一整套完整自动化运维服务体系，提供一键部署集群，一键自动扩缩容，Redis 超细粒度监控，旁路流量分析等辅助功能。</p><p>目前，Redis 在知乎的规模如下：</p><ul><li>机器内存总量约 70TB，实际使用内存约 40TB。</li><li>平均每秒处理约 1500 万次请求，峰值每秒约 2000 万次请求。</li><li>每天处理约 1 万亿余次请求。</li><li>单集群每秒处理最高每秒约 400 万次请求。</li><li>集群实例与单机实例总共约 800 个。</li><li>实际运行约 16000 个 Redis 实例。</li><li>Redis 使用官方 3.0.7 版本，少部分实例采用 4.0.11 版本。<a id="more"></a><strong>知乎 Redis 平台演进历程</strong></li></ul><p>根据业务的需求，我们将实例区分为如下两种类型：</p><ul><li>单机(Standalone)，单机实例通常用于容量与性能要求不高的小型存储。</li><li>集群(Cluster)，集群则用来应对对性能和容量要求较高的场景。</li></ul><p><strong>单机(Standalone)</strong></p><p>对于单机实例，我们采用原生主从(Master-Slave)模式实现高可用，常规模式下对外仅暴露 Master 节点。由于使用原生 Redis，所以单机实例支持所有 Redis 指令。</p><p>对于单机实例，我们使用 Redis 自带的哨兵(Sentinel)集群对实例进行状态监控与 Failover。</p><p>Sentinel 是 Redis 自带的高可用组件，将 Redis 注册到由多个 Sentinel 组成的 Sentinel 集群后，Sentinel 会对 Redis 实例进行健康检查。</p><p>当 Redis 发生故障后，Sentinel 会通过 Gossip 协议进行故障检测，确认宕机后会通过一个简化的 Raft 协议来提升 Slave 成为新的 Master。</p><p>通常情况我们仅使用 1 个 Slave 节点进行冷备，如果有读写分离请求，可以建立多个 Read only slave 来进行读写分离。</p><p><a href="http://s3.51cto.com/oss/201809/20/5be52b6cdf8a3e4eec2529c304e37443.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201809/20/5be52b6cdf8a3e4eec2529c304e37443.jpg" alt=""></a></p><p>如上图所示，通过向 Sentinel 集群注册 Master 节点实现实例的高可用，当提交 Master 实例的连接信息后，Sentinel 会主动探测所有的 Slave 实例并建立连接，定期检查健康状态。</p><p>客户端通过多种资源发现策略如简单的 DNS 发现 Master 节点，将来有计划迁移到如 Consul 或 etcd 等资源发现组件 。</p><p>当 Master 节点发生宕机时，Sentinel 集群会提升 Slave 节点为新的 Master，同时在自身的 pubsub channel +switch-master 广播切换的消息，具体消息格式为：</p><ol><li>switch-master <master name=""> <oldip> <oldport> <newip> <newport> </newport></newip></oldport></oldip></master></li></ol><p>Watcher 监听到消息后，会去主动更新资源发现策略，将客户端连接指向新的 Master 节点，完成 Failover。</p><p>实际使用中需要注意以下几点：</p><ul><li>只读 Slave 节点可以按照需求设置 slave-priority 参数为 0，防止故障切换时选择了只读节点而不是热备 Slave 节点。</li><li>Sentinel 进行故障切换后会执行 CONFIG REWRITE 命令将 SLAVEOF 配置落地，如果 Redis 配置中禁用了 CONFIG 命令，切换时会发生错误，可以通过修改 Sentinel 代码来替换 CONFIG 命令。</li><li>Sentinel Group 监控的节点不宜过多，实测超过 500 个切换过程偶尔会进入 TILT 模式，导致 Sentinel 工作不正常，推荐部署多个 Sentinel 集群并保证每个集群监控的实例数量小于 300 个。</li><li>Master 节点应与 Slave 节点跨机器部署，有能力的使用方可以跨机架部署，不推荐跨机房部署 Redis 主从实例。</li><li>Sentinel 切换功能主要依赖 down-after-milliseconds 和failover-timeout 两个参数，down-after-milliseconds 决定了Sentinel 判断 Redis 节点宕机的超时，知乎使用 30000 作为阈值。</li></ul><p>而 failover-timeout 则决定了两次切换之间的最短等待时间，如果对于切换成功率要求较高，可以适当缩短 failover-timeout 到秒级保证切换成功。</p><ul><li>单机网络故障等同于机器宕机，但如果机房全网发生大规模故障会造成主从多次切换，此时资源发现服务可能更新不够及时，需要人工介入。</li></ul><p><strong>集群(Cluster)</strong></p><p>当实例需要的容量超过 20G 或要求的吞吐量超过 20 万请求每秒时，我们会使用集群(Cluster)实例来承担流量。</p><p>集群是通过中间件(客户端或中间代理等)将流量分散到多个 Redis 实例上的解决方案。</p><p>知乎的 Redis 集群方案经历了两个阶段：</p><ul><li><strong>客户端分片</strong></li><li><strong>Twemproxy 代理</strong></li></ul><p><strong>客户端分片(before 2015)</strong></p><p>早期知乎使用 redis-shard 进行客户端分片，redis-shard 库内部实现了 CRC32、MD5、SHA1 三种哈希算法，支持绝大部分Redis 命令。使用者只需把 redis-shard 当成原生客户端使用即可，无需关注底层分片。</p><p><a href="http://s1.51cto.com/oss/201809/20/ef730504d150077a1cb7c28a8524adaa.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201809/20/ef730504d150077a1cb7c28a8524adaa.jpg" alt=""></a></p><p>基于客户端的分片模式具有如下优点：</p><ul><li>基于客户端分片的方案是集群方案中最快的，没有中间件，仅需要客户端进行一次哈希计算，不需要经过代理，没有官方集群方案的 MOVED/ASK 转向。</li><li>不需要多余的 Proxy 机器，不用考虑 Proxy 部署与维护。</li><li>可以自定义更适合生产环境的哈希算法。</li></ul><p>但是也存在如下问题：</p><ul><li>需要每种语言都实现一遍客户端逻辑，早期知乎全站使用 Python 进行开发，但是后来业务线增多，使用的语言增加至 Python，Golang，Lua，C/C++，JVM 系(Java，Scala，Kotlin)等，维护成本过高。</li><li>无法正常使用 MSET、MGET 等多种同时操作多个 Key 的命令，需要使用 Hash tag 来保证多个 Key 在同一个分片上。</li><li>升级麻烦，升级客户端需要所有业务升级更新重启，业务规模变大后无法推动。</li><li>扩容困难，存储需要停机使用脚本 Scan 所有的 Key 进行迁移，缓存只能通过传统的翻倍取模方式进行扩容。</li><li>由于每个客户端都要与所有的分片建立池化连接，客户端基数过大时会造成 Redis 端连接数过多，Redis 分片过多时会造成 Python 客户端负载升高。</li><li>早期知乎大部分业务由 Python 构建，Redis 使用的容量波动较小，redis-shard 很好地应对了这个时期的业务需求，在当时是一个较为不错的解决方案。</li></ul><p><strong>Twemproxy 集群 (2015 - Now)</strong></p><p>2015 年开始，业务上涨迅猛，Redis 需求暴增，原有的 redis-shard 模式已经无法满足日益增长的扩容需求，我们开始调研多种集群方案，最终选择了简单高效的 Twemproxy 作为我们的集群方案。</p><p>由 Twitter 开源的 Twemproxy 具有如下优点：</p><ul><li>性能很好且足够稳定，自建内存池实现 Buffer 复用，代码质量很高。</li><li>支持 fnv1a_64、murmur、md5 等多种哈希算法。</li><li>支持一致性哈希(ketama)，取模哈希(modula)和随机(random)三种分布式算法。</li></ul><p>但是缺点也很明显：</p><ul><li>单核模型造成性能瓶颈。</li><li>传统扩容模式仅支持停机扩容。</li></ul><p>对此，我们将集群实例分成两种模式：</p><ul><li>缓存(Cache)</li><li>存储(Storage)</li></ul><p>如果使用方可以接受通过损失一部分少量数据来保证可用性，或使用方可以从其余存储恢复实例中的数据，这种实例即为缓存，其余情况均为存储。我们对缓存和存储采用了不同的策略。</p><p><strong>存储</strong></p><p><a href="http://s1.51cto.com/oss/201809/20/5972d56fb785304a0750a79eb9d8a628.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201809/20/5972d56fb785304a0750a79eb9d8a628.jpg" alt=""></a></p><p>对于存储我们使用 fnv1a_64 算法结合 modula 模式即取模哈希对 Key 进行分片。</p><p>底层 Redis 使用单机模式结合 Sentinel 集群实现高可用，默认使用 1 个 Master 节点和 1 个 Slave 节点提供服务，如果业务有更高的可用性要求，可以拓展 Slave 节点。</p><p>当集群中 Master 节点宕机，按照单机模式下的高可用流程进行切换，Twemproxy 在连接断开后会进行重连。</p><p>对于存储模式下的集群，我们不会设置 auto_eject_hosts，不会剔除节点。</p><p>同时，对于存储实例，我们默认使用 noeviction 策略，在内存使用超过规定的额度时直接返回 OOM 错误，不会主动进行 Key 的删除，保证数据的完整性。</p><p>由于 Twemproxy 仅进行高性能的命令转发，不进行读写分离，所以默认没有读写分离功能。</p><p>而在实际使用过程中，我们也没有遇到集群读写分离的需求，如果要进行读写分离，可以使用资源发现策略在 Slave 节点上架设 Twemproxy 集群，由客户端进行读写分离的路由。</p><p><strong>缓存</strong></p><p>考虑到对于后端(MySQL/HBase/RPC 等)的压力，知乎绝大部分业务都没有针对缓存进行降级，这种情况下对缓存的可用性要求较数据的一致性要求更高。</p><p>但是如果按照存储的主从模式实现高可用，1 个 Slave 节点的部署策略在线上环境只能容忍 1 台物理节点宕机，N 台物理节点宕机高可用就需要至少 N 个 Slave 节点，这无疑是种资源的浪费。</p><p><a href="http://s1.51cto.com/oss/201809/20/554c2563bbda037d43d91d357d9c157d.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201809/20/554c2563bbda037d43d91d357d9c157d.jpg" alt=""></a></p><p>所以我们采用了 Twemproxy 一致性哈希(Consistent Hashing)策略来配合 auto_eject_hosts 自动弹出策略组建 Redis 缓存集群。</p><p>对于缓存我们仍然使用 fnv1a_64 算法进行哈希计算，但是分布算法我们使用了 ketama 即一致性哈希进行 Key 分布。缓存节点没有主从，每个分片仅有 1 个 Master 节点承载流量。</p><p>Twemproxy 配置 auto_eject_hosts 会在实例连接失败超过server_failure_limit 次的情况下剔除节点。</p><p>并在 server_retry_timeout 超时之后进行重试，剔除后配合 ketama 一致性哈希算法重新计算哈希环，恢复正常使用，这样即使一次宕机多个物理节点仍然能保持服务。</p><p><a href="http://s5.51cto.com/oss/201809/20/e29c6268a7be5645935c273fb8df8e37.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201809/20/e29c6268a7be5645935c273fb8df8e37.jpg" alt=""></a></p><p>在实际的生产环境中需要注意以下几点：</p><ul><li>剔除节点后，会造成短时间的命中率下降，后端存储如 MySQL、HBase 等需要做好流量监测。</li><li>线上环境缓存后端分片不宜过大，建议维持在 20G 以内，同时分片调度应尽可能分散，这样即使宕机一部分节点，对后端造成的额外的压力也不会太多。</li><li>机器宕机重启后，缓存实例需要清空数据之后启动，否则原有的缓存数据和新建立的缓存数据会冲突导致脏缓存。</li></ul><p>直接不启动缓存也是一种方法，但是在分片宕机期间会导致周期性 server_failure_limit 次数的连接失败。</p><ul><li>server_retry_timeout 和 server_failure_limit 需要仔细敲定确认，知乎使用 10min 和 3 次作为配置，即连接失败 3 次后剔除节点，10 分钟后重新进行连接。</li></ul><p><strong>Twemproxy 部署</strong></p><p>在方案早期我们使用数量固定的物理机部署 Twemproxy，通过物理机上的 Agent 启动实例，Agent 在运行期间会对 Twemproxy 进行健康检查与故障恢复。</p><p>由于 Twemproxy 仅提供全量的使用计数，所以 Agent 运行时还会进行定时的差值计算来计算 Twemproxy 的 requests_per_second 等指标。</p><p>后来为了更好地故障检测和资源调度，我们引入了 Kubernetes，将 Twemproxy 和 Agent 放入同一个 Pod 的两个容器内，底层 Docker 网段的配置使每个 Pod 都能获得独立的 IP，方便管理。</p><p>最开始，本着简单易用的原则，我们使用 DNS A Record 来进行客户端的资源发现，每个 Twemproxy 采用相同的端口号，一个 DNS A Record 后面挂接多个 IP 地址对应多个 Twemproxy 实例。</p><p>初期，这种方案简单易用，但是到了后期流量日益上涨，单集群 Twemproxy 实例个数很快就超过了 20 个。</p><p>由于 DNS 采用的 UDP 协议有 512 字节的包大小限制，单个 A Record 只能挂接 20 个左右的 IP 地址，超过这个数字就会转换为 TCP 协议，客户端不做处理就会报错，导致客户端启动失败。</p><p>当时由于情况紧急，只能建立多个 Twemproxy Group，提供多个 DNS A Record 给客户端，客户端进行轮询或者随机选择，该方案可用，但是不够优雅。</p><p><strong>如何解决 Twemproxy 单 CPU 计算能力的限制?</strong></p><p>之后我们修改了 Twemproxy 源码， 加入 SO_REUSEPORT 支持。</p><p><a href="http://s3.51cto.com/oss/201809/20/60873c7d6b5bab29ac5a5ac0f7da640f.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201809/20/60873c7d6b5bab29ac5a5ac0f7da640f.jpg" alt=""></a></p><p>Twemproxy with SO_REUSEPORT on Kubernetes</p><p>同一个容器内由 Starter 启动多个 Twemproxy 实例并绑定到同一个端口，由操作系统进行负载均衡，对外仍然暴露一个端口，但是内部已经由系统均摊到了多个 Twemproxy 上。</p><p>同时 Starter 会定时去每个 Twemproxy 的 stats 端口获取 Twemproxy 运行状态进行聚合，此外 Starter 还承载了信号转发的职责。</p><p>原有的Agent 不需要用来启动 Twemproxy 实例，所以 Monitor 调用 Starter 获取聚合后的 stats 信息进行差值计算，最终对外界暴露出实时的运行状态信息。</p><p><strong>为什么没有使用官方 Redis 集群方案?</strong></p><p>我们在 2015 年调研过多种集群方案，综合评估多种方案后，最终选择了看起来较为陈旧的 Twemproxy 而不是官方 Redis 集群方案与 Codis，具体原因如下：</p><p>MIGRATE 造成的阻塞问题：Redis 官方集群方案使用 CRC16 算法计算哈希值并将 Key 分散到 16384 个 Slot 中，由使用方自行分配 Slot 对应到每个分片中。</p><p>扩容时由使用方自行选择 Slot 并对其进行遍历，对 Slot 中每一个 Key 执行 MIGRATE 命令进行迁移。</p><p>调研后发现，MIGRATE 命令实现分为三个阶段：</p><ul><li>DUMP 阶段：由源实例遍历对应 Key 的内存空间，将 Key 对应的 Redis Object 序列化，序列化协议跟 Redis RDB 过程一致。</li><li>RESTORE 阶段：由源实例建立 TCP 连接到对端实例，并将 DUMP 出来的内容使用 RESTORE 命令到对端进行重建，新版本的 Redis 会缓存对端实例的连接。</li><li>DEL 阶段(可选)：如果发生迁移失败，可能会造成同名的 Key 同时存在于两个节点。</li></ul><p>此时 MIGRATE 的 REPLACE 参数决定是否覆盖对端的同名 Key，如果覆盖，对端的 Key 会进行一次删除操作，4.0 版本之后删除可以异步进行，不会阻塞主进程。</p><p>经过调研，我们认为这种模式并不适合知乎的生产环境。Redis 为了保证迁移的一致性， MIGRATE 所有操作都是同步操作，执行 MIGRATE 时，两端的 Redis 均会进入时长不等的 BLOCK 状态。</p><p>对于小 Key，该时间可以忽略不计，但如果一旦 Key 的内存使用过大，一个 MIGRATE 命令轻则导致 P95 尖刺，重则直接触发集群内的 Failover，造成不必要的切换。</p><p>同时，迁移过程中访问到处于迁移中间状态的Slot 的 Key 时，根据进度可能会产生 ASK 转向，此时需要客户端发送 ASKING 命令到 Slot 所在的另一个分片重新请求，请求时延则会变为原来的两倍。</p><p>同样，方案初期时的 Codis 采用的是相同的 MIGRATE 方案，但是使用 Proxy 控制 Redis 进行迁移操作而非第三方脚本(如 redis-trib.rb)，基于同步的类似 MIGRATE 的命令，实际跟 Redis 官方集群方案存在同样的问题。</p><p>对于这种 Huge Key 问题决定权完全在于业务方，有时业务需要不得不产生 Huge Key 时会十分尴尬，如关注列表。</p><p>一旦业务使用不当出现超过 1MB 以上的大 Key 便会导致数十毫秒的延迟，远高于平时 Redis 亚毫秒级的延迟。</p><p>有时，在 Slot 迁移过程中业务不慎同时写入了多个巨大的 Key 到 Slot 迁移的源节点和目标节点，除非写脚本删除这些 Key ，否则迁移会进入进退两难的地步。</p><p>对此，Redis 作者在 Redis 4.2 的 roadmap[5] 中提到了 Non blocking MIGRATE。</p><p>但是截至目前，Redis 5.0 即将正式发布，仍未看到有关改动，社区中已经有相关的 Pull Request [6]，该功能可能会在 5.2 或者 6.0 之后并入 Master 分支，对此我们将持续观望。</p><p>缓存模式下高可用方案不够灵活：还有，官方集群方案的高可用策略仅有主从一种，高可用级别跟 Slave 的数量成正相关。</p><p>如果只有一个 Slave，则只能允许一台物理机器宕机，Redis 4.2 roadmap 提到了 cache-only mode，提供类似于 Twemproxy 的自动剔除后重分片策略，但是截至目前仍未实现。</p><p>内置 Sentinel 造成额外流量负载：另外，官方 Redis 集群方案将 Sentinel 功能内置到 Redis 内，这导致在节点数较多(大于 100)时在 Gossip 阶段会产生大量的 PING/INFO/CLUSTER INFO 流量。</p><p>根据 issue 中提到的情况，200 个使用 3.2.8 版本节点搭建的 Redis 集群，在没有任何客户端请求的情况下，每个节点仍然会产生 40Mb/s 的流量。</p><p>虽然到后期 Redis 官方尝试对其进行压缩修复，但按照 Redis 集群机制，节点较多的情况下无论如何都会产生这部分流量，对于使用大内存机器但是使用千兆网卡的用户这是一个值得注意的地方。</p><p>Slot 存储开销：最后，每个Key 对应的 Slot 的存储开销，在规模较大的时候会占用较多内存，4.x 版本以前甚至会达到实际使用内存的数倍。</p><p>虽然 4.x 版本使用 rax 结构进行存储，但是仍然占据了大量内存，从非官方集群方案迁移到官方集群方案时，需要注意这部分多出来的内存。</p><p>总之，官方 Redis 集群方案与 Codis 方案对于绝大多数场景来说都是非常优秀的解决方案。</p><p>但是我们仔细调研发现并不是很适合集群数量较多且使用方式多样化的我们，场景不同侧重点也会不一样，但在此仍然要感谢开发这些组件的开发者们，感谢你们对 Redis 社区的贡献。</p><p><strong>扩容</strong></p><p><strong>静态扩容</strong></p><p>对于单机实例，如果通过调度器观察到对应的机器仍然有空闲的内存，我们仅需直接调整实例的 maxmemory 配置与报警即可。</p><p>同样，对于集群实例，我们通过调度器观察每个节点所在的机器，如果所有节点所在机器均有空闲内存，我们会像扩容单机实例一样直接更新 maxmemory 与报警。</p><p><strong>动态扩容</strong></p><p>但是当机器空闲内存不够，或单机实例与集群的后端实例过大时，无法直接扩容，需要进行动态扩容：</p><ul><li>对于单机实例，如果单实例超过 30GB 且没有如 sinterstore 之类的多 Key 操作，我们会将其扩容为集群实例。</li><li>对于集群实例，我们会进行横向的重分片，我们称之为 Resharding 过程。</li></ul><p><a href="http://s2.51cto.com/oss/201809/20/896c54e4da6cfb6e9eadaf4a5422837d.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201809/20/896c54e4da6cfb6e9eadaf4a5422837d.jpg" alt=""></a></p><p>Resharding 过程</p><p>原生 Twemproxy 集群方案并不支持扩容，我们开发了数据迁移工具来进行 Twemproxy 的扩容，迁移工具本质上是一个上下游之间的代理，将数据从上游按照新的分片方式搬运到下游。</p><p>原生 Redis 主从同步使用 SYNC/PSYNC 命令建立主从连接，收到 SYNC 命令的 Master 会 fork 出一个进程遍历内存空间生成 RDB 文件并发送给 Slave。</p><p>期间所有发送至 Master 的写命令在执行的同时都会被缓存到内存的缓冲区内，当 RDB 发送完成后，Master 会将缓冲区内的命令及之后的写命令转发给 Slave 节点。</p><p>我们开发的迁移代理会向上游发送 SYNC 命令模拟上游实例的 Slave，代理收到 RDB 后进行解析。</p><p>由于 RDB 中每个 Key 的格式与 RESTORE 命令的格式相同，所以我们使用生成 RESTORE 命令按照下游的 Key 重新计算哈希并使用 Pipeline 批量发送给下游。</p><p>等待 RDB 转发完成后，我们按照新的后端生成新的 Twemproxy 配置，并按照新的 Twemproxy 配置建立 Canary 实例。</p><p>从上游的 Redis 后端中取 Key 进行测试，测试 Resharding 过程是否正确，测试过程中的 Key 按照大小，类型，TTL 进行比较。</p><p>测试通过后，对于集群实例，我们使用生成好的配置替代原有 Twemproxy 配置并 restart/reload Twemproxy 代理。</p><p>我们修改了 Twemproxy 代码，加入了 config reload 功能，但是实际使用中发现直接重启实例更加可控。</p><p>而对于单机实例，由于单机实例和集群实例对于命令的支持不同，通常需要和业务方确定后手动重启切换。</p><p>由于 Twemproxy 部署于 Kubernetes ，我们可以实现细粒度的灰度，如果客户端接入了读写分离，我们可以先将读流量接入新集群，最终接入全部流量。</p><p>这样相对于 Redis 官方集群方案，除在上游进行 BGSAVE 时的 fork 复制页表时造成的尖刺以及重启时造成的连接闪断，其余对于 Redis 上游造成的影响微乎其微。</p><p>这样扩容存在的问题：</p><p>对上游发送 SYNC 后，上游fork 时会造成尖刺：对于存储实例，我们使用Slave 进行数据同步，不会影响到接收请求的 Master 节点。</p><p>对于缓存实例，由于没有 Slave 实例，该尖刺无法避免，如果对于尖刺过于敏感，我们可以跳过 RDB 阶段，直接通过 PSYNC 使用最新的 SET 消息建立下游的缓存。</p><p>切换过程中有可能写到下游，而读在上游：对于接入了读写分离的客户端，我们会先切换读流量到下游实例，再切换写流量。</p><p>一致性问题：两条具有先后顺序的写同一个 Key 命令在切换代理后端时会通过 1)写上游同步到下游 2)直接写到下游两种方式写到下游。</p><p>此时，可能存在应先执行的命令却通过 1)执行落后于通过 2)执行，导致命令先后顺序倒置。</p><p>这个问题在切换过程中无法避免，好在绝大部分应用没有这种问题，如果无法接受，只能通过上游停写排空 Resharding 代理保证先后顺序。</p><p>官方 Redis 集群方案和 Codis 会通过 blocking 的 MIGRATE 命令来保证一致性，不存在这种问题。</p><p>实际使用过程中，如果上游分片安排合理，可实现数千万次每秒的迁移速度，1TB 的实例 Resharding 只需要半小时左右。</p><p>另外，对于实际生产环境来说，提前做好预期规划比遇到问题紧急扩容要快且安全得多。</p><p><strong>旁路分析</strong></p><p>由于生产环境调试需要，有时会需要监控线上 Redis 实例的访问情况，Redis 提供了多种监控手段，如 MONITOR 命令。</p><p>但由于 Redis 单线程的限制，导致自带的 MONITOR 命令在负载过高的情况下会再次跑高 CPU，对于生产环境来说过于危险。</p><p>而其余方式如 Keyspace Notify 只有写事件，没有读事件，无法做到细致的观察。</p><p>对此我们开发了基于 libpcap 的旁路分析工具，系统层面复制流量，对应用层流量进行协议分析，实现旁路 MONITOR，实测对于运行中的实例影响微乎其微。</p><p>同时对于没有 MONITOR 命令的 Twemproxy，旁路分析工具仍能进行分析。</p><p>由于生产环境中绝大部分业务都使用 Kubernetes 部署于 Docker 内 ，每个容器都有对应的独立 IP。</p><p>所以可以使用旁路分析工具反向解析找出客户端所在的应用，分析业务方的使用模式，防止不正常的使用。</p><p><strong>将来的工作</strong></p><p>由于 Redis 5.0 发布在即，4.0 版本趋于稳定，我们将逐步升级实例到 4.0 版本，由此带来的如 MEMORY 命令、Redis Module 、新的 LFU 算法等特性无论对运维方还是业务方都有极大的帮助。</p><p><strong>最后</strong></p><p>知乎架构平台团队是支撑整个知乎业务的基础技术团队，开发和维护着知乎几乎全量的核心基础组件。</p><p>包括容器、Redis、MySQL、Kafka、LB、HBase 等核心基础设施，团队小而精，每个同学都独当一面负责上面提到的某个核心系统。</p><p>随着知乎业务规模的快速增长，以及业务复杂度的持续增加，团队面临的技术挑战也越来越大。</p><p>参考资料：</p><ul><li>Redis Official site</li></ul><p><a href="https://redis.io/" target="_blank" rel="noopener">https://redis.io/</a></p><ul><li>Twemproxy Github Page</li></ul><p><a href="https://github.com/twitter/twemproxy" target="_blank" rel="noopener">https://github.com/twitter/twemproxy</a></p><ul><li>Codis Github Page</li></ul><p><a href="https://github.com/CodisLabs/codis" target="_blank" rel="noopener">https://github.com/CodisLabs/codis</a></p><ul><li>SO_REUSEPORT Man Page</li></ul><p><a href="http://man7.org/linux/man-pages/man7/socket.7.html" target="_blank" rel="noopener">http://man7.org/linux/man-pages/man7/socket.7.html</a></p><ul><li>Kubernetes</li></ul><p><a href="https://kubernetes.io/" target="_blank" rel="noopener">https://kubernetes.io/</a></p><p>作者：陈鹏</p><p>简介：现知乎存储平台组 Redis 平台技术负责人，2014 年加入知乎技术平台组从事基础架构相关系统的开发与运维，从无到有建立了知乎 Redis 平台，承载了知乎高速增长的业务流量。</p><p>原文地址：<a href="http://developer.51cto.com/art/201809/583728.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201809/583728.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>UI动效基本规则全面总结（转载）</title>
      <link href="/2018/09/17/UI%E5%8A%A8%E6%95%88%E5%9F%BA%E6%9C%AC%E8%A7%84%E5%88%99%E5%85%A8%E9%9D%A2%E6%80%BB%E7%BB%93%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/09/17/UI%E5%8A%A8%E6%95%88%E5%9F%BA%E6%9C%AC%E8%A7%84%E5%88%99%E5%85%A8%E9%9D%A2%E6%80%BB%E7%BB%93%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>有了这篇文章，你就不用在其他地方学习基本的动效设计规则了。</p></blockquote><p><img src="http://image.woshipm.com/wp-files/2018/09/rls54QKS21cpJz1RpUqu.jpg" alt=""><br><a id="more"></a></p><h2 id="动效的持续时长和速度"><a href="#动效的持续时长和速度" class="headerlink" title="动效的持续时长和速度"></a>动效的持续时长和速度</h2><p>当元素的位置和状态发生改变的时候，动效的速度应该足够慢，维持足够长的时间，让用户能够注意到变化，但是同时，又不能慢到需要用户去等待。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/pIA7IUn5qT5vq3KJyk44.gif" alt=""></p><p>大量的研究表明，动效的最佳持续时长是200毫秒到500毫秒之间，这个研究数字是基于人脑的认知方式和信息消化速度得出来的。任何低于100毫秒的动效对于人的眼睛而言，几乎都是瞬间，很难被识别出来，而超过1秒的动效会让人有迟滞感。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/ua4rLHfVFBUgdhy08HSe.gif" alt=""></p><p>△ 界面中动效持续时长</p><p>在手机这样的移动端设备上，按照 Material Design 的建议，动效时长应该控制在200~300毫秒之间。在平板电脑上，这个时长应该延长大概30%，也就是说，时长应该在400~450毫秒之间。</p><p>原因很简单，屏幕尺寸越大，元素在发生位移的时候，跨越的距离越长，速度一定的情况下，时长自然越长。相应的，在可穿戴设备的小屏幕上，这个时长应该缩短30%，在150~200毫秒之间。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/8S3hTGI4OVKfx23zCAE6.gif" alt=""></p><p>△ 移动端设备的屏幕尺寸影响动画的持续时长</p><p>网页动效的处理方式也不一样，由于我们习惯在浏览器中直接打开网页，考虑到浏览器性能和大家的使用习惯，用户对于浏览器中动效变化速率的预期还是比较快的。相比于移动端中的动效速度，网页中的速度会快上一倍，换句话说，就是动效的持续时长应该在150~200毫秒之间。如果持续时间太长，用户会忍不住觉得网页卡住了。</p><p>不过，如果你的网页中所用的动效并非功能性的，而是装饰用的，或者用来吸引用户的注意力，那么请忘记这个规则，在这种情况下，动效可以更长。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/j06Mx37CiI5GlQIRF2zW.gif" alt=""></p><p>更大的屏幕=更慢的动效？绝不是如此！</p><p>请务必记住，无论是在什么平台上，动效的持续时长绝不是单纯取决于屏幕尺寸和运动距离，还取决于平台特征、元素大小、功能设定等等。较小的元素或者较小的变化，相应的动效应该更快一点。因此，大而复杂的元素动效持续时间更长，看起来也更舒服一点。</p><p>大小相同的元素，在移动的时候，移动距离最短的元素，是最先停止下来的。</p><p>与较大的元素相比，较小的元素运动速度应该更慢，因为相同的移动距离，对于小元素而言，位移距离和自身大小比例倍数更大，相对偏移更远。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/5N6RzT2XFq8pqWAuJloL.gif" alt=""></p><p>△ 动效的持续时长还和元素大小、运动距离有关</p><p>动效的运动规律要符合物理规律，当元素运动到边界，发生碰撞的时候，碰撞的「能量」最终是要均匀分摊下来的，而弹跳的特效在多数情况下是不适合的，仅在特殊情况下适合使用。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/6Qp05dvD8qTWHjt3hCBh.gif" alt=""></p><p>△ 避免使用弹跳特效，它会分散用户的注意力</p><p>元素的运动过程应该是清晰的，尽量不要在运动中使用模糊的效果（是的，说的就是 AE 的模糊动效爱好者们），模糊的动效不适合在 UI界面中使用。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/E0aTo2GUN0zFxUbGLXqW.gif" alt=""></p><p>△ 不要在动效中使用模糊效果</p><p>列表项（新闻列表、邮件列表等）所使用的动效，在实际运动的过程中，项和项之间应该有轻微的延迟，元素之间的延迟应该控制在20~25毫秒之间，如果持续时间再长，可能会给人一种迟滞的观感。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/itaywheD94d7udwoHiRM.gif" alt=""></p><p>△ 列表项之间的延迟应该在20~25毫秒</p><h2 id="缓动"><a href="#缓动" class="headerlink" title="缓动"></a>缓动</h2><p>缓动指的是物体在物理规则下，渐进加速或减速的现象。在动效中加入缓动的效果能够让运动显得更加自然，这是运动的基本原则之一。对于缓动，迪士尼的两位关键性的动画大师 Ollie Johnston 和 Frank Thomas 在他们的著作《The Illusion of Life: Disney Animation》中有过非常详尽的描述。</p><p>为了不让动效看起来机械或者人工痕迹太明显，元素的运动应该有渐进加速和渐进减速的特征，就像物理世界当中其他的物体这样。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/h055W7qSt4YW4bXY9VIF.gif" alt=""></p><p><strong>匀速直线运动</strong></p><p>不受任何物理力量的影响，匀速直线运动看起来是非常不自然的，尤其是对于人眼而言。</p><p>所有用来设计动画的应用都会使用坐标轴和曲线来阐述动效的运动特征，我将尝试阐述它们的含义，以及如何使用。坐标轴的 X轴是实现，而 Y轴则表示的唯一，换句话来说，如同我们在初中物理中所学到的，坐标轴上的线条描述的是速度和加速度的特征。</p><p>下面所示的直线，表示速度是均匀的，也就是匀速直线运动，物体在相同时间内运动的距离是不变的。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/kQYVlwJLTpnZgEjPMC8U.gif" alt=""></p><p>△ 匀速直线运动的座标图</p><p>均匀的变化通常只会用在色彩的改变或者透明的改变上，一般来说，我们也可以让背景元素均匀运动，而前景元素保持不变，来呈现它的状态，就像上图一样。</p><p><strong>缓动加速曲线</strong></p><p>通过曲线我们可以看到，物体开始时候的初速度比较低，运动缓慢，随后速度逐渐增加，这意味着物体在加速运动。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/Gh9mymksaOiWpKv6IzwA.gif" alt=""></p><p>△ 加速曲线</p><p>当物体加速飞出屏幕的时候，可以使用这种加速曲线，比如界面中被用户使用滑动手势甩出去的卡片。但是请记住，只有当运动对象需要完全离开界面的时候才会这么使用，如果它还需要再回来的话，则不行。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/afFPFGLSoEJCL6RJA5li.gif" alt=""></p><p>△ 以加速运动将卡片扔出屏幕</p><p>动画曲线有助于正确传达讯息，甚至表达情绪和感觉。在下面的案例当中，我们可以看每个元素的运动位移是完全一样的，所消耗的总时长也是一样的，但是运动的速率变化是不同的，这一点也体现在曲线上，所表现出来的情绪也不同。</p><p>当然，通过调整曲线，你能够让物体的运动轨迹尽可能接近现实世界。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/AOUUOLZzGuxetwTlNIEt.gif" alt=""></p><p><strong>缓动减速曲线</strong></p><p>当元素从屏幕外运动到屏幕内的时候，动效应该遵循这类曲线的运动特征。从全速进入屏幕开始，速度降低，直到完全停止。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/oTmkAZe59avlGf2pWo8T.gif" alt=""></p><p>△ 减速曲线</p><p>减速曲线可以适用于多种不同的 UI控件和元素，包括从屏幕外进入屏幕内的的卡片、条目等。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/dvvp7xl87WFw9jpcrc5b.gif" alt=""></p><p>△ 减速曲线案例</p><h2 id="缓动标准曲线"><a href="#缓动标准曲线" class="headerlink" title="缓动标准曲线"></a>缓动标准曲线</h2><p>在这种曲线之下，物体从静止开始加速，到达速度最高点之后开始减速直到静止。这种类型的元素在 UI界面中最为常见，每当你不知道要在动效中使用哪种运动方式的时候，可以试试标准曲线。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/3hj8o4y3YjR64UdGt0DX.gif" alt=""></p><p>△ 标准曲线</p><p>根据 Material Design 的规范，最好使用不那么对称的增速和减速的过程，让动效看起来更加真实。同时大家会更加在意运动的结果，曲线的末端，也就是运动结束的过程最好进行适当的强调。</p><p>换句话说就是减速过程持续的时长最好超过开始加速的时长，用户将会更加清楚地观察到运动的最终结果，从而更好地明白运动的终止状态。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/RTnErJn3HAAruzNC38HY.gif" alt=""></p><p>△ 对称和非对称运动的差异</p><p>当元素从屏幕的一个位置移动到另外一个位置的时候，最好使用这种标准的缓动曲线，这个过程中，尽量不要让动画效果引人注意，不要使用戏剧化的效果。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/RUm2liaFEUQ4xezLSmkH.gif" alt=""></p><p>△ 卡片元素从屏幕上运动的时候，不对称的缓动曲线</p><p>当元素从屏幕上消失的时候，采用了相同的不对称缓动曲线，用户同样可以通过滑动回到之前的位置。这个环节使用了抽屉式导航控件。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/0CrnoTuBxko0ijZySz0U.gif" alt=""></p><p>△ 抽屉式导航随着缓动曲线从屏幕上隐藏</p><p>从这些案例当中，可以看出许多动效的初学者对于运动规则的了解还不足。比如：下面的这个动效，元素随着减速曲线出现，然后使用标准缓动曲线消失。根据 Material Design 的标准，新出现的元素持续的时间应该更长，因为需要吸引更多的注意力。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/5wF4Fzno4qGjBwc3X8ml.gif" alt=""></p><p>△ 导航菜单的出现和消失</p><p>贝赛尔曲线函数 cubic-bezier（）是用来描述曲线的，它的名字前面之所以带有 Cubic 是因为每个贝赛尔曲线的描述都是基于四个不同的参数来确定的。曲线的起点（0，0）和终点（1，1）在坐标轴上的位置是已经事先确定的。</p><p>为了简化你对于贝赛尔曲线的理解，我们推荐两个网站，分别是 easings.net 和 cubic-bezier.com ，前者包含了最常见的曲线的列表，你可以将他们复制到你的原型工具中，第二个网站为你提供了不同曲线的参数，你可以直接在其中查看各种对象的移动方式。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/eR5UwdMdGHGyg2y84NIy.gif" alt=""></p><p>△ 不同类型的 cubic-bezier（）的曲线和参数</p><h2 id="界面动效的编排"><a href="#界面动效的编排" class="headerlink" title="界面动效的编排"></a>界面动效的编排</h2><p>就像芭蕾舞的舞蹈编排一样，动画效果也是需要编排的，它的主要目的是让元素从一个状态切换到下一个状态，自然过渡，让用户的注意力自然地被引导过去。</p><p>编排有两种不同的方式：一种是均等交互，另一种是从属交互。</p><h3 id="均等交互"><a href="#均等交互" class="headerlink" title="均等交互"></a><strong>均等交互</strong></h3><p>均等交互意味着所有的元素和对象都遵循一个特定的编排的规则。</p><p>在这个实例当中，所有的卡片都遵循着一个方向来引导用户的注意力，自上到下地次第加载。相反，没有按照这样清晰的规则来加载，用户的注意力会被分散，元素的外观排布也会显得比较糟糕。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/KwLZHMS7MWrMHRs1ufvS.gif" alt=""></p><p>△ 用户的注意力应该沿着一个流向来引导</p><p>至于表格式的布局，它相对就复杂一点。在这里，用户的视线流向应该是清晰的对角线方向，因此，逐个区块次第出现是一个糟糕的设计。这样的逐个显示，一方面耗时太长，另一方面会让用户觉得元素的加载方式是锯齿状的，这种方式并不合理。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/zqgrHCDinzX1fI4qbejf.gif" alt=""></p><p>△ 沿着对角线加载</p><h3 id="从属交互"><a href="#从属交互" class="headerlink" title="从属交互"></a><strong>从属交互</strong></h3><p>从属交互指的是使用一个中心对象作为主体，来吸引用户的注意力，而其他的元素从属于它来逐步呈现。这样的动画设计能够创造更强的秩序感，让主要的内容更容易引起用户的注意。</p><p>在其他的设计当中，用户很难搞清楚哪个才是主要的，因为注意力被分散了。因此，如果要设置多个动画元素，应该定义清楚谁为主，谁是中心，并且尽量按照从属关系来次第呈现不同的子元素。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/sLeQkHKCt2q6CilDkFOI.gif" alt=""></p><p>如果只有一个中心对象，那么其他的对象的运动方式都要受它制约，否则用户分不清楚主次。</p><p>根据 Material Design 的规定，当元素不成比例地变幻尺寸的时候，它应该沿着弧线运动，而不是直线运动，这样有助于让它看起来更加自然。所谓「不成比例」地变化指的是元素的长和宽的变化并不是按照相同比例来缩放或者变化的，换句话来说，变化的速度也不一样。（比如：方形变成矩形）</p><p><img src="http://image.woshipm.com/wp-files/2018/09/4PDuYPzHHATJ7JDV5jR3.gif" alt=""></p><p>△ 不成比例地改变对象外观的时候，运动轨迹应该是弧线的</p><p>相反，如果元素是按照比例改变大小的时候，应该沿着直线移动，这样不仅操作更加方便，而且更符合均匀变化的特征。看一下真实的案例，你会发现直线的运动轨迹会更加合理。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/Qso8YKr4ri6LaenU1rwg.gif" alt=""></p><p>△ 成比例变化大小的时候，应该沿着直线运动</p><p>当元素不成比例放大的时候，运动轨迹是弧线，而这种弧线运动轨迹有两种不同的呈现一种，一种轨迹是初始方向为垂直方向而运动结束时瞬间运动方向是水平的（Horizontal out），另外一种初始方向是水平方向而运动结束时瞬间运动方向是垂直的（Vertical out）。</p><p><strong>那么怎么选取这个方向呢？</strong></p><p>很简单，元素运动曲线的方向，应该是要向界面的主要运动方向的主轴靠拢重合。举个例子，在下面的动效当中，整个界面的滚动方向是上下滚动，主轴是纵向的。因此，当卡片点击之后被展开的时候，会先向右水平移动，并最终以垂直运动结束，运动的最终方向，切线是垂直的，也就和垂直方向的主轴重合了。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/9wd8ZtJ1PUrry83Z5dAp.gif" alt=""></p><p>△ 元素按照弧线展开的时候，最终方向应该和主轴重合</p><p>如果几个不同的元素的运动轨迹相交，那么他们不能彼此穿越。如果每个元素都必须通过某个交点，抵达另外一个位置，那么应该次第减速，依次通过这个点，给彼此留出足够的空间。另外一种选择，是元素不相交，而是像实体一样在靠近的时候，彼此推开。</p><p>为什么？</p><p>因为我们通常假定界面中所有的元素都位于同一个平面当中。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/LbCENhh2JYZVlYaHL9iX.gif" alt=""></p><p>在运动过程中，元素不应彼此穿越，而应该互相留出空间。</p><p>但是这一点也不是一成不变的。在比较拥挤的界面当中，某个元素可以「越过」其他的元素，它同样没有穿过其他的元素消失，而是单纯的移动。这一点从某种意义上也是延续自我们日常的物理规律，只不过我们会默认界面中的元素在这个情况下拥有了高度这样的属性。</p><p><img src="http://image.woshipm.com/wp-files/2018/09/YxqJUyJTvp83rdp54bMT.gif" alt=""></p><p>△ 元素可以越过其他的元素运动</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>我们总结了这么多动效运动的规则和原则，从某种意义上还是延续自我们对于物理世界的认知，摩擦力和加速度在虚拟界面中以另外的方式续存着。模仿现实世界的界面让我们对于界面的秩序有更清晰的认知，允许我们更轻松的了解和访问界面的内容。</p><p>如果动效按照正确的方式来设计，它应该是不显著，且不会分散用户注意力的。如果不是这样，那么你需要让动效更微妙一点，实在不行甚至需要将它移除。动效不应该成为影响用户操控界面的障碍，或者转移注意力的存在。</p><p>当然，即使是遵循这么多规律，动效的设计依然是一门艺术，而非单纯的科学，多做测试多摸索总是有必要的。</p><p>原文作者：Taras Skytskyi</p><p>原文链接：<a href="https://uxdesign.cc/the-ultimate-guide-to-proper-use-of-animation-in-ux-10bd98614fa9" target="_blank" rel="noopener">https://uxdesign.cc/the-ultimate-guide-to-proper-use-of-animation-in-ux-10bd98614fa9</a></p><p>译文作者：陈子木</p><p>译文链接：<a href="https://www.uisdc.com/ultimate-guide-to-ui-animation" target="_blank" rel="noopener">https://www.uisdc.com/ultimate-guide-to-ui-animation</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 产品经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringCloud五大核心组件（转载）</title>
      <link href="/2018/09/08/SpringCloud%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/09/08/SpringCloud%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>Spring Cloud由众多子项目组成，如Spring Cloud Config、Spring Cloud Netflix、Spring Cloud Consul 等，提供了搭建分布式系统及微服务常用的工具，如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性token、全局锁、选主、分布式会话和集群状态等，满足了构建微服务所需的所有解决方案。</p><p>服务发现——Netflix Eureka</p><p>客服端负载均衡——Netflix Ribbon</p><p>断路器——Netflix Hystrix</p><p>服务网关——Netflix Zuul</p><p>分布式配置——Spring Cloud Config<br><a id="more"></a></p><h2 id="Eureka"><a href="#Eureka" class="headerlink" title="Eureka"></a>Eureka</h2><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151046924-1982054858.png" alt=""></p><p>一个RESTful服务，用来定位运行在AWS地区（Region）中的中间层服务。由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。Netflix在其生产环境中使用的是另外的客户端，它提供基于流量、资源利用率以及出错状态的加权负载均衡。</p><h2 id="Ribbon"><a href="#Ribbon" class="headerlink" title="Ribbon"></a>Ribbon</h2><p>Ribbon，主要提供客户侧的软件负载均衡算法。</p><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151052962-2072470043.png" alt=""><br><img src="https://segmentfault.com/img/bVuYWg" alt="">Ribbon客户端组件提供一系列完善的配置选项，比如连接超时、重试、重试算法等。Ribbon内置可插拔、可定制的负载均衡组件。下面是用到的一些负载均衡策略：</p><ul><li><p>简单轮询负载均衡</p></li><li><p>加权响应时间负载均衡</p></li><li><p>区域感知轮询负载均衡</p></li><li><p>随机负载均衡</p></li></ul><p>Ribbon中还包括以下功能：</p><ul><li><p>易于与服务发现组件（比如Netflix的Eureka）集成</p></li><li><p>使用Archaius完成运行时配置</p></li><li><p>使用JMX暴露运维指标，使用Servo发布</p></li><li><p>多种可插拔的序列化选择</p></li><li><p>异步和批处理操作（即将推出）</p></li><li><p>自动SLA框架（即将推出）</p></li><li><p>系统管理/指标控制台（即将推出）</p></li></ul><h2 id="Hystrix"><a href="#Hystrix" class="headerlink" title="Hystrix"></a>Hystrix</h2><p><img src="https://segmentfault.com/img/bVuZSb" alt=""><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151100735-1093465094.png" alt=""></p><p>断路器可以防止一个应用程序多次试图执行一个操作，即很可能失败，允许它继续而不等待故障恢复或者浪费 CPU 周期，而它确定该故障是持久的。断路器模式也使应用程序能够检测故障是否已经解决。如果问题似乎已经得到纠正​​，应用程序可以尝试调用操作。欢迎大家一起学习研究相关技术愿意了解源码的朋友直接求求交流分享技术：2147775633</p><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151107786-234603606.png" alt=""></p><p><img src="https://segmentfault.com/img/bVuZSk" alt=""></p><p>断路器增加了稳定性和灵活性，以一个系统，提供稳定性，而系统从故障中恢复，并尽量减少此故障的对性能的影响。它可以帮助快速地拒绝对一个操作，即很可能失败，而不是等待操作超时（或者不返回）的请求，以保持系统的响应时间。如果断路器提高每次改变状态的时间的事件，该信息可以被用来监测由断路器保护系统的部件的健康状况，或以提醒管理员当断路器跳闸，以在打开状态。</p><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151114584-270530092.png" alt=""></p><p><img src="https://segmentfault.com/img/bVuYWr" alt=""></p><p>流程图</p><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151122013-721938940.png" alt=""><br><img src="https://segmentfault.com/img/bVuYWl" alt=""></p><h2 id="Zuul"><a href="#Zuul" class="headerlink" title="Zuul"></a>Zuul</h2><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151127916-1134484737.png" alt=""></p><p><img src="https://segmentfault.com/img/bVuYWw" alt="">类似nginx，反向代理的功能，不过netflix自己增加了一些配合其他组件的特性。</p><h2 id="Spring-Cloud-Config"><a href="#Spring-Cloud-Config" class="headerlink" title="Spring Cloud Config"></a>Spring Cloud Config</h2><p><img src="https://images2018.cnblogs.com/blog/1468241/201809/1468241-20180907151133508-574110198.png" alt=""></p><p><img src="https://segmentfault.com/img/bVuYWI" alt="">这个还是静态的，得配合Spring Cloud Bus实现动态的配置更新。</p><p>更多详细源码参考来源：<a href="http://minglisoft.cn/honghu/technology.html" target="_blank" rel="noopener">http://minglisoft.cn/honghu/technology.html</a></p><p>原文地址：<a href="https://www.cnblogs.com/my1903832579/p/9604757.html" target="_blank" rel="noopener">https://www.cnblogs.com/my1903832579/p/9604757.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文读懂RFID技术（转载）</title>
      <link href="/2018/09/08/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82RFID%E6%8A%80%E6%9C%AF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/09/08/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82RFID%E6%8A%80%E6%9C%AF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>为什么我们的快递可以一直准确无误在路线上?为什么学校图书馆里海量的书籍却管理得整齐有序?为什么有些不小心失窃的物品可以迅速追踪回来?而这些都得利用RFID技术，因为在这个物联网的时代，它是数据连接、数据交流的关键技术之一。</p><p><a href="http://s5.51cto.com/oss/201809/07/9acda0ba66a5cc95c98a65c688c8daf6.jpg-wh_651x-s_3709089521.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201809/07/9acda0ba66a5cc95c98a65c688c8daf6.jpg-wh_651x-s_3709089521.jpg" alt="RFID技术" title="RFID技术"></a><br><a id="more"></a><br>什么是RFID技术?RFID又称无线射频识别，通过无线电讯号识别并读写特定目标数据，不需要机械接触或者特定复杂环境就可完成识别与读写数据。如今，大家所讲的RFID技术应用其实就是RFID标签，它已经存在于我们生活中的方方面面。</p><p>它的工作方式有两种情况，一种就是当RFID标签进入解读器有效识别范围内时，接收解读器发出的射频信号，凭借感应电流所获得能量发出存储在芯片中的信息，另一种就是由RFID标签主动发送某一频率的信号，解读器接收信息并解码后，送至中央信息系统进行有关数据处理。</p><p><a href="http://s5.51cto.com/oss/201809/07/8c9096ff6745a0d6def03f09a52237dd.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201809/07/8c9096ff6745a0d6def03f09a52237dd.jpg" alt="RFID技术" title="RFID技术"></a></p><p>在二十世纪中期，基于雷达的改进和应用，射频识别技术就开始奠定基础，此后便开始初步发展，直到今天，RFID技术应用已经有了长达半个世纪的历史，目前，RFID技术在国内外的发展状况良好，尤其是美国、德国、瑞典、日本、南非、英国和瑞士等国家，均有较为成熟和先进的RFID系统，我国在这方面的发展也不甘落后，比较成功的案例的是推出了完全自主研究远距离自动识别系统。</p><p>接下来小编就带着大家读懂RFID技术：</p><p><strong>一、三种类型</strong></p><p>由RFID技术衍生的产品主要有三大类：</p><p><strong>1. 无源RFID产品：</strong></p><p>此类产品需要近距离接触式识别，比如饭卡、银行卡、公交卡和身份证等，这些卡类型都是在工作识别时需要近距离接触，主要工作频率有低频125KHZ、高频13.56MHZ、超高频433MHZ和 915MHZ。这类产品也是我们生活中比较常见，也是发展比较早的产品。</p><p><strong>2. 有源RFID产品：</strong></p><p>这类型的产品则具有远距离自动识别的特性，所以相应地应用到一些大型环境下，比如智能停车场、智慧城市、智慧交通及物联网等领域，它们的主要工作有微波2.45GHZ和5.8GHZ，超高频433MHZ。</p><p><strong>3. 半有源RFID产品：</strong></p><p>顾名思义就是有源RFID产品和无源RFID产品的结合，它结合二者的优点，在低频125KHZ频率的触发下，让微波2.45G发挥优势，解决了有源RFID产品和无源RFID产品不能解决的问题，比如门禁出入管理、区域定位管理及安防报警等方面的应用，近距离激活定位、远距离传输数据。</p><p><strong>二、六个领域</strong></p><p>RFID技术具有抗干扰性强以及无需人工识别的特点，所以常常被应用在一些需要采集或追踪信息的领域上，大致包括但不限于以下七点:</p><p><strong>1. 仓库/运输/物资：</strong></p><p>给货品嵌入RFID芯片，存放在仓库、商场等货品以及物流过程中，货品相关信息被读写器自动采集，管理人员就可以在系统迅速查询货品信息，降低丢弃或者被盗的风险，可以提高货品交接速度，提高准确率，并且防止窜货和防伪。</p><p><strong>2. 门禁/考勤：</strong></p><p>一些公司或者一些大型会议，通过提前录入身份或者指纹信息，就可以通过门口识别系统自行识别签到，中间就省去了很多时间，方便又省力。</p><p><strong>3. 固定资产管理：</strong></p><p>像图书馆、艺术馆及博物馆等资产庞大或者物品贵重的一些场所，就需要有完整的管理程序或者严谨的保护措施，当书籍或者贵重物品的存放信息有异常变动，就会第一时间在系统里提醒管理员，从而处理相关情况。</p><p><strong>4. 火车/汽车识别/行李安检：</strong></p><p>我国铁路的车辆调度系统就是一个典型的案例，自动识别车辆号码、信息输入，省去了大量人工统计的时间，以及提高了精准度。</p><p><strong>5. 医疗信息追踪：</strong></p><p>病例追踪、废弃物品追踪、药品追踪等都是提高医院服务水平和效率的好方法。</p><p><strong>6. 军事/国防/国家安全：</strong></p><p>一些重要军事药品、枪支、弹药或者军事车辆的动态都是需要实时跟踪。</p><p><strong>三、七大优点</strong></p><p><strong>1. 抗干扰性超强</strong></p><p>它有一个最重要的优点就是非接触式识别，它能在急剧恶劣的环境下都可以工作，可以并且穿透力极强，可以快速识别并阅读标签。</p><p><strong>2. RFID标签的数据容量十分庞大</strong></p><p>它可以根据用户的需求扩充到10k，远远高于二维码条形2725个数字的容量。</p><p><strong>3. 可以动态操作</strong></p><p>它的标签数据可以利用编程进行动态修改，并且可以动态追踪和监控，只要RFID标签所附着的物体出现在解读器的有效识别范围内。</p><p><strong>4. 使用寿命长</strong></p><p>因为其抗干扰性强，所以RFID标签不易被破坏，使用寿命很长。</p><p><strong>5. 防冲突</strong></p><p>在解读器的有效识别范围内，它可以同时读取多个RFID标签。</p><p><strong>6. 安全性高</strong></p><p>RFID标签可以以任何形式附着在产品上，可以为标签数据进行密码加密，提高安全性。</p><p><strong>7. 识别速度快</strong></p><p>只要RFID标签一进入解读器的有效识别范围内，就马上开始读取数据，一般情况下不到100毫秒就可完成识别。</p><p><strong>四、共同进退</strong></p><p>当然，每一项技术都是有利有弊的，RFID技术发展到今天，也会存在缺陷，那就是超高频频段的技术应用还不够广泛，技术不够成熟，相关产品价格昂贵，稳定性不高，国际上也没有制定统一的标准。</p><p>目前，RFID技术已经和我们的日常生活息息相关了，在现在的物联网时代，假如RFID技术得到完善，RFID超高频技术成熟，RFID超高频市场发展得到广泛应用，那么物联网的发展也会推向一个新高度。</p><p><strong>五、延伸阅读：RFID安全十大问题与威胁</strong></p><p>和其它安全设备一样，RFID设备的安全性并不完美。尽管RFID设备得到了广泛的应用，但其带来的安全威胁需要我们在设备部署前解决。下文将主要介绍几个RFID相关的安全问题。</p><p><strong>1. RFID伪造</strong></p><p>根据计算能力，RFID可以分为三类：</p><ul><li>普通标签(tag)</li><li>使用对称密钥的标签</li><li>使用非对称密钥的标签</li></ul><p>其中，普通标签不做任何加密操作，很容易进行伪造。但普通标签却广泛应用在物流管理和旅游业中，攻击者可以轻易将信息写入一张空白的RFID标签中或者修改一张现有的标签，以获取使用RFID标签进行认证系统对应的访问权限。对于普通标签攻击者可以进行如下三件事：</p><p>根据计算能力，RFID可以分为三类：</p><ul><li>修改现有标签中的数据，使一张无效标签变为有效的，或者相反，将有效的标签变为无效。例如，可以通过修改商品的标签内容，然后以一个较低的价格购买一件昂贵的商品。</li><li>同样还是修改标签，不过是将一个标签内容修改为另一个标签的内容，就是狸猫换太子。</li><li>根据获取到的别人标签内容来制造一张自己的标签。</li></ul><p>所以，当想在一些处理如身份证这种包含敏感信息的系统中使用RFID标签时，一定要使用加密技术。但如果不得不使用普通标签的话，一定要确保配有相应的安全规范、监控和审计程序，以检测RFID系统中任何的异常行为。</p><p><strong>2. RFID嗅探</strong></p><p>RFID嗅探是RFID系统中一个主要的问题。RFID阅读器总是向标签发送请求认证的信息，当阅读器收到标签发送的认证信息时，它会利用后端数据库验证标签认证信息的合法性。</p><p>但不幸的是，大部分的RFID标签并不认证RFID阅读器的合法性。那么攻击者可以使用自己的阅读器去套取标签的内容。</p><p><strong>3. 跟踪</strong></p><p>通过读取标签上的内容，攻击者可以跟踪一个对象或人的运动轨迹。当一个标签进入到了阅读器可读取的范围内时，阅读器可以识别标签并记录下标签当前的位置。</p><p>无论是否对标签和阅读器之间的通信进行了加密，都无法逃避标签被追踪的事实。攻击者可以使用移动机器人来跟踪标签的位置。</p><p><strong>4. 拒绝服务</strong></p><p>当阅读器收到来自标签的认证信息时，它会将认证信息与后端数据库内的信息进行比对。阅读器和后端数据库都很容易遭受拒绝服务攻击。</p><p>当出现拒绝服务攻击时，阅读器将无法完成对标签的认证，并导致其他相应服务的中断。所以，必须确保阅读器和后端数据库之间有相应防范拒绝服务攻击的机制。</p><p><strong>5. 欺骗</strong></p><p>在欺骗攻击中，攻击中常常将自己伪造成为一个合法的用户。有时，攻击者会把自己伪造成后端数据库的管理员，如果伪造成功，那么攻击者就可以随心所欲的做任何事，例如：相应无效的请求，更改RFID标识，拒绝正常的服务或者干脆直接在系统中植入恶意代码。</p><p><strong>6. 否认</strong></p><p>所谓否认就是当一个用户在进行了某个操作后拒绝承认他曾做过，当否认发送时，系统没有办法能够验证该用户究竟有没有进行这项操作。</p><p>在使用RFID中，存在两种可能的否认：一种是发送者或接收者可能否认进行过一项操作，如发出一个RFID请求，此时我们没任何证据可以证明发送者或接收者是否发出过RFID请求;另一种是数据库的拥有者可能否认他们给予过某件物品或人任何标签。</p><p><strong>7. 插入攻击</strong></p><p>在这种攻击中，攻击者试图向RFID系统发送一段系统命令而不是原本正常的数据内容。一个最简单的例子就是，攻击者将攻击命令插入到标签存储的正常数据中。</p><p><strong>8. 重传攻击</strong></p><p>攻击者通过截获标签与阅读器之间的通信，记录下标签对阅读器认证请求的回复信息，并在之后将这个信息重传给阅读器。重传攻击的一个例子就是，攻击者记录下标签和阅读器之间用于认证的信息。</p><p><strong>9. 物理攻击</strong></p><p>物理攻击发送在攻击者能够在物理上接触到标签并篡改标签的信息。物理攻击有多种方式，例如：使用微探针读取修改标签内容，使用X射线或者其他射线去破坏标签内容，使用电磁干扰破坏标签与阅读器之间的通信。</p><p>另外，任何人都可以轻易的使用小刀或其他工具人为的破坏标签，这样阅读器就无法识别标签了。</p><p><strong>10. 病毒</strong></p><p>同其他信息系统一样，RFID系统很容易遭受病毒的攻击。多数情况下，病毒的目标都是后端数据库。 RFID病毒可以破坏或泄露后端数据库中存储的标签内容，拒绝或干扰阅读器与后端数据库之间的通信。为了保护后端数据库，一定要及时修补数据库漏洞和其他风险。</p><p>虽然RFID系统常常成为被攻击的目标，但是由于RFID系统低廉的成本，使得其在很多领域还是得到了广泛的应用。所以当准备部署RFID系统时，一定要更多的关注其安全问题，特别是本文描述的前四种攻击：伪造、嗅探、跟踪和拒绝服务攻击。</p><p>原文地址：<a href="http://network.51cto.com/art/201809/582929.htm" target="_blank" rel="noopener">http://network.51cto.com/art/201809/582929.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 物联网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>国产数据库发展现状分析（转载）</title>
      <link href="/2018/08/23/%E5%9B%BD%E4%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%91%E5%B1%95%E7%8E%B0%E7%8A%B6%E5%88%86%E6%9E%90%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/08/23/%E5%9B%BD%E4%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8F%91%E5%B1%95%E7%8E%B0%E7%8A%B6%E5%88%86%E6%9E%90%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>从上世纪90年代开始，国产数据库的开发就不断被人提起，国内已有不少企业、团体在这个方面做了不少的投入。在此，我们与大家分享一下对几个数据库国产化途径的看法。我们主要讨论自主研发、引进代码和互联网厂商提供的云上数据库。</p><p><img src="https://img-blog.csdn.net/2018060418045593" alt=""><br><a id="more"></a></p><h2 id="自主研发"><a href="#自主研发" class="headerlink" title="自主研发"></a>自主研发</h2><p>国内自主研发关系型数据库的企业、单位基本上都是发源于上世纪90年代的，而且都是以大学、科研机构为主。到今天，有代表性的厂商有：</p><ul><li><p>达梦 – 由华中理工冯玉才教授创办，完全自主研发。以Oracle为参照、追赶对象。</p></li><li><p>人大金仓 – 由人民大学王珊教授创办，自主研发。普通的关系型数据库。</p></li><li><p>神舟通用 – 神舟集团与南大通用合作开发的关系型数据库。更多地用于数据分析领域。</p></li><li><p>南大通用（Gbase 8a）- 南开大学的背景，2010年左右自主研发的，基于列式存储的，面向数据分析、数据仓库的数据库系统。</p></li></ul><p>其他没有列出的，属于在下孤陋寡闻，绝无贬低的意思。</p><p>这些公司的发展分为两个阶段，以2007年作为分界线。前一个阶段集中在20世纪90年代，公司的出发点就是开发一款通用的，主要面向OLTP的关系型数据库。在那个年代，中国的人工成本还是比较低的，国外厂商的数据库，如：Oracle，Sybase，Informix，DB2都算是成本较高的产品。很多人认为，只要做出功能、性能、稳定性合适的国产数据库，就能有一定的市场，至少价格能够有优势；即便市场不成功，作为科研教学也有一定价值，至少申请科研经费和政府补贴是个好题材。</p><p>但是，这么些年下来，这些产品在市场上并没有产生大影响力。他们发展的这些年，也正好是本人在相应的国外知名厂商从业的这些年。从微观的角度看，国产自主研发的数据库败在仅仅追求大而全，而技术创新不足、没有特点，产品的稳定性一直上不去，也不敢做有挑战性的性能测试。所以，稳定性、性能都无法让市场信服。结果就是：稍微重要一点的系统根本没人敢用。</p><p>中国数据库软件这个市场，从80年代末一开始，就是伴随改革开放的自由竞争市场，政府基本没有对国产品牌的扶植和保护。因此，在国产产品的研发思路上，就不能够简单地模仿Oracle。Oracle的数据库软件，功能极为丰富，支持的语句非常全面，超过ANSI SQL的标准，代码量堪称浩瀚。而一个国产的小厂商，要模仿Oracle开发一个数据库系统，开发一个与Oracle全兼容的数据库（而我们很多政府客户恰恰就在标书上要求这个，也不知道他们是支持国产数据库还是打压国产数据库），就好比乞丐与龙王比阔：这注定是一条艰辛且难以成功的道路。</p><p><img src="https://img-blog.csdn.net/20180604180507793" alt="">  </p><p>那么，看看同期国外的小厂商又是怎么做的？</p><p>Sybase采用多线程代替多进程开发了服务器，一切围绕client/server架构，成为当年的新锐；一家叫做Express Way的小公司开发了位图索引，被Syabse收购，发展出Sybase IQ和列式存储，直到今天依然是Sybase的重要产品；90年代初，加州大学伯克利的Michael Stonebraker教授提出了对象关系数据库理论，并以此成立了Illustra公司，然后被Informix收购，推出Informix Universal Server；数据仓库大师Ralph Kimball以多维关系模型理论，构建了RedBrick数据库公司，在星型连接环节提供特别的索引技术，后来被Informix收购，成为单节点数据仓库产品的主力；Netezza则是采用了可编程门阵列FPGA参与磁盘数据的扫描，最后被IBM收购，成为IBM数据仓库的主力产品……</p><p>因此，那些年间，我有一个感慨：每当我们认为关系数据库的技术已经到头了，已经是一个夕阳产业的时候，总会有一家极富创新的小公司跳出来，他们带着令人耳目一新的想法赢得市场的尊敬与成功。而可惜的是，这一切都没有出现在我们这几个自主研发的国产数据库“龙头企业”身上。</p><p>这样的话，这些国产自主研发的数据库产品就会典型地遭遇到：</p><ul><li><p>没有特别的技术亮点和优势；</p></li><li><p>产品的稳定性差：客户不敢用它承担关键业务（甚至是普通业务）；</p></li><li><p>生态环境差：成型应用少，合作开发商少；</p></li><li><p>产品技术发展滞后，与国外同类产品差距巨大；</p></li></ul><p>这实际上是一个恶性循环，跳不出这个圈子，产品和公司是没办法发展的。不过，这些公司有能力生存至今，除了人工便宜，还因为每年能得到政府一定程度上的补贴；尽管从政府的角度看，这些补贴是打水漂的。</p><p><img src="https://img-blog.csdn.net/2018060418052165" alt="">  </p><p>今天，我们对于这一类的产品的未来总体是不看好的，这是因为：20年对于一个高科技企业来说不是一个短暂的时间段。在90年代，即便是Oracle在整个市场普及的情况下，Sybase、Informix也还是在短短的几年间发展起来，并占据了一部分市场。如果一个高科技企业20年都没能发展起来，一定有一些关键的内在因素制约，其实已经说明他们不可能发展起来了。这些因素改变了吗？我们认为近两年来，仅仅是外部舆论的环境，似乎更加注重了国产化，但其它内在的因素都没有任何改变。与此同时，负面的因素也不少：其中最重要的是关系数据库软件大发展的年代已经过去了，今天我们面临的是经济发展减速、开源软件普及和云计算的转型。</p><p>依靠这一类“自主研发”的数据库产品去支撑中国数据库国产化基本是不可能的。世界的顶尖产品依然高速发展。今年2月，Oracle已经正式发布18c这个版本了。我们的自主研发，也许还在以Oracle 9i为目标，而且还没有令人信服的稳定性。</p><p>在2007年以后，一些国内的数据库公司意识到数据库软件应该跟随数据管理市场的发展。而数据分析（OLAP）被认为是未来具有发展潜力的一块（相对于传统OLTP数据库应用）。而且，数据仓库类的平台，对可靠性、时效性要求比较低。直觉是更适合于国产软件进行率先突破。这样，以神州通用、南大通用为代表的公司在分布式并行处理和列式存储数据库方面做了新的投入。</p><p>然而，这一块是一个发展迅速且技术路线存在争议的领域。从历史上看，大型的数据仓库，采用Teradata的客户占据一大部分，Oracle在那个领域的投入并不大。因此，在数据仓库上的国产化发展，并不能解政府的“自主可控”、替代、摆脱Oracle的燃眉之急。数据仓库领域在大数据概念被炒作之前一直不怎么受待见。另一方面，这个领域遇到的市场竞争也不小：数据仓库规模大了，有Teradata和DB2的压制；规模小了，有Greenplum的竞争；如果遇到界限模糊的，客户就直接延用Oracle了。</p><p>而今天，这个领域最大的竞争则是来自大数据的解决方案。本来，国产的数据仓库平台开始就是主攻中低端的（高端有Teradata的各种压制）。但是，之前主要的对手Greenplum竟然宣布开源了，背景是其东家Pivotal公司将重点转向虚拟化和大数据了。这样的话，专注于数据仓库的国产数据库有多了一个竞争对手，市场被进一步瓜分。要想进入良性循环就更加困难。</p><h2 id="引进源代码"><a href="#引进源代码" class="headerlink" title="引进源代码"></a>引进源代码</h2><p>引进数据库源代码发展国产数据库，这在上个世纪对于国人来讲还是不敢想的。如今，经济发展了，有钱了；而且IBM也愿意迎合国人对于国产化的诉求，将搁置多年的Informix源代码拿出来“一马（码）多吃”，发挥余热。2015年以来，与IBM签订源代码授权的公司有华胜天成、南大通用（Gbase 8t）和星瑞格。这三个公司成为以引进Informix源代码发展国产数据库的代表。</p><p><img src="https://img-blog.csdn.net/20180604180536425" alt="">  </p><p>在此，我们分析一下引进Informix源代码到底解决了什么问题？其实，数据库技术发展到今天，国人并非不知道数据库底层该怎么开发，而是自己写的代码在市场上磨练比较少，对稳定性信心不足，而这种信心的不足同样来自客户方面。所以，引进Informix的源代码，一个核心的作用就是利用Informix以往在中国市场上的声誉使客户获得信心。所以，这些拿着Informix源代码的国产数据库公司在做销售工作的时候往往需要反复向客户说明：“这就是Informix，这就是世界级的数据库产品！”，否则就体现不出与自主研发或抄袭开源的区别了。</p><p>对于Informix而言，国内的客户大致可以分为三类：</p><p>第一类是Informix的老客户，今天还在使用的。这些客户之前是认可Informix，而且坚持到了今天。这些客户中有一部分其实已经不愿意继续使用Informix（企业中的其它系统早已采用Oracle），仅仅是因为有老的应用跑在这个数据库上；一旦应用升级换代，Informix数据库也自然淘汰。还有一些客户虽然可以使用Informix，但是否换成国产的版本还不确定，他们还可能等待IBM对于Informix的继续支持、乃至升级。这部分客户其实是国产Informix厂商最适合作为起步的客户，但所剩的数量实在太少了。更严峻的是，IBM其实并没有彻底出售Informix，IBM还在继续发布Informix的后续版本。虽然非常缓慢，但还是有可能比这几个国产的厂商快一些，因为国产的厂商可是要从头读源代码的，要等到他们也能出新版本，时间不会短。但是，国内的这部分客户，一旦升级到IBM提供的Informix新版本，他们就绝难回到国产的Informix版本了，因为那就相当于降级了。这部分客户留给国产Informix的时间已经不多。</p><p>第二类是曾经采用过Informix的客户，如今已经转到Oracle上。这一类用户或许还是认可Informix技术的，但要让他们再回到国产的Informix上则非常难。对于这些客户而言，回到Informix，不管是IBM的还是国产的，都如同走回头路 - 很少有人愿意走回头路的。</p><p>第三类则从来就不是Informix的客户，对于这些人来讲，当年就没有认可Informix。今天，即便你说Informix是世界级的，你是中国的Informix，对于这类客户的感受依然是遥远和陌生的。</p><p>因此，我们并不认为花钱引进Informix源代码是国产数据库发展的最佳途径。这套源代码的市场效应其实是有限的，在技术上的帮助也是有限的。在技术发展日新月异的年代，经过10多年的搁置，Informix的代码、产品和创新已经算不得先进，技术的细节暂不在此赘述了。</p><p>需要指出的是：依靠代码的引进的另一个风险就是二次落后，这一点很多人可能还没有意识到。从目前的情况看，国产Informix数据库的代码在不远未来就会与IBM自己的Informix分叉。即便基于Informix的源代码，国产数据库到底能走多远，与世界先进水平之间的差距是扩大还是缩小，最终还是要靠自己。</p><p>类似的案例就是国产战斗机的发展：在抗美援朝年代，我们得到了苏联的米格15以及后来的米格17。那时候是中国空军与美国空军在飞机装备上差距最小的时期。我们在此基础上发展出自己的歼5、歼6。但是，几十年以后的科索沃战争、伊拉克战争的时候，我们的差距却又变得那么悬殊。因此，即便是今天的“世界级”，如果离开了强大的研发和创新实力，也会失去未来。</p><p>今天，拥有Informix源代码的国产数据库厂商，如果真的想全面掌控Informix技术，研发的投入绝对是不小的。Informix的主要模块的源代码就有2000多万行，Informix当年在美国的核心研发队伍就有超过200人，加上测试和周边团队，不下500人。要快速追赶世界先进水平，研发的规模不能小于这个（读别人的代码其实比自己写代码还要累！）。但目前的市场容量又不支持这样的投入。这是一个现实的困境，是不容易突破的。无论是华胜天成、南大通用，还是星瑞格，都没有像样的、有规模的研发队伍投入到Informix的源代码掌握中。他们对Informix的研发投入，都不超过40人。</p><p>2017年5月发生的事情彻底断绝了Informix成为中国国产数据库的希望：2017年5月，IBM把整个Informix业务卖给了印度公司HCL。印度的HCL是全面接管IBM Informix，不仅仅是代码，而是全部的团队、公司、办公场所。IBM Informix在美国的办公楼直接换了Logo，员工全部签了IBM的离职协议，同时签印度HCL的入职。印度的“数据库国产化力度”似乎更“大手笔”一些。这样，国内的几个半吊子Informix代码的厂商 - 华胜天成、南大通用、星瑞格，从此基本都断了根！这几年来，南大通用到处给国人宣传的口号“让中国人用上世界领先的数据库”，恐怕要改成“让中国人用上三哥的数据库”。</p><p><img src="https://img-blog.csdn.net/20180604180550442" alt="">  </p><p>这已经表明，在Informix代码上发展国产数据库这条路，基本已经走不通：国内的这几个厂商面对十年前的巨量代码一筹莫展，要弄通又得花很多年，而且还是在资金、人员充足的情况下。即便到了那个时候（公司还存在），且不说他们与世界先进水平差多远，就是比邻国印度，都难以望其项背！</p><p>在引进源代码这类公司中，还需要一提的是浪潮。浪潮的K-DB数据库实际上是从韩国引进的。但浪潮并没有说明是否引进了源代码。所以，我们把它归为引进产品而非引进代码。从某种程度上讲，还不能算是“自主可控”，或者“国产数据库”，因为我们不能确信浪潮掌握的相关技术。我们知道，浪潮前些年采用了HP的安腾芯片推出自己的服务器。但不幸的是，除了HP自身走下坡路，Oracle更是在后来宣布Oracle数据库不再支持安腾。因此，浪潮的安腾服务器便陷入了无法提供Oracle数据库软件的窘境。一个不能运行Oracle数据库的服务器，市场份额的损失得有多大？为此，浪潮必须寻找一个Oracle的替代品，而且与Oracle越像越好，最好全兼容。这样，浪潮从韩国找到了K-DB。K-DB号称是与Oracle全兼容的数据库。但是业界有传闻说是K-DB是来源于早年Oracle源代码的泄漏。这个虽然很难核实。但我们可以考察一下韩国市场的情况：如果K-DB真有浪潮说的那么强，那它应该在韩国红遍天才对，因为韩国每年在Oracle上的支出是非常大的（Oracle在韩国的销售额近年来出现过好几个季度是超过中国的！）。可是，我们并没有发现K-DB在韩国很火。那么原因很可能是K-DB真的有不方便的地方：韩国是需要美国来保护的，韩国对于美国软件的知识产权上是不敢造次的。</p><p>同样，未来遇到某个国产数据库号称全部兼容Oracle的时候一定要注意。因为Oracle最有特色的地方就是功能非常繁多，语句极端丰富，即便大部分都不常用。其它公司要是追求这个，在研发上就是乞丐与龙王爷比富！在2010年前后，IBM启动了一个“破甲计划”，就是在DB2数据库中增加一个选项，使得DB2可以支持Oracle的各种语法。这个计划花费了大量的投资，结果依然有很多Oracle的语句与用法不能够支持。</p><h2 id="互联网企业的云数据库"><a href="#互联网企业的云数据库" class="headerlink" title="互联网企业的云数据库"></a>互联网企业的云数据库</h2><p>除了上述的传统国产数据库之外，近年来，互联网公司的云计算在市场上非常红火。尤其是阿里云，在中国的云计算市场上占据了主导地位。互联网公司拥有雄厚的资金实力，阿里巴巴在数据库研发上有大量的投入，并在阿里云上提供了自己的数据库。那么，云计算厂商在其云计算平台上提供的数据库能否承担起数据库国产化的重任？</p><p><img src="https://img-blog.csdn.net/20180604180601737" alt="">  </p><p>在此，我们认为这是存在局限的：</p><p>1、云计算厂商主要的业务是提供云服务。因此，云计算供应商在云上一般会提供多种数据库平台，如：MySQL，PostgreSQL，自研的数据库。云厂商自研的数据库只是其中一种，并不见得有绝对的优势。只是提供给客户更多的选择。</p><p>2、云计算厂商开发的数据库系统，市场目标是在自家的云上提供服务，而不是作为传统软件卖许可证。因此，很多打算摆脱Oracle，采用国产数据库，但又还没打算上云的用户，就很难享受到这类产品。</p><p>3、每个云计算供应商开发的数据库只在自家的云上提供服务，其它云平台当然无法运行。这样，对于独立软件商来说，他们当然希望自己的软件能够运行在多个云平台上。在这个时候，他们的应用软件如果采用某个云计算公司提供的数据库，这就有些不方便了。所以，这些独立软件商需要一个类似传统的、独立的关系数据库，这个数据库可以在各个云计算平台上运行。同样的情况也发生在数据库的用户身上。</p><p>4、目前的云计算公司提供的数据库，其实也处于起步阶段。只是在资金、支持服务上因为云的关系，存在一定的优势。未来的发展还要看市场。</p><p>因此，云计算虽然在近年来发展迅速，而且也是未来的趋势。但云计算平台上的数据库系统还不能取代市场上对传统国产数据库的需求。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>今天，国产数据库确实处于一个非常尴尬的局面。很多政府相关的数据库软件招标都指明了要求国产数据库。在这个过程中，政府实际上承受着不小的压力：美国就此指责我们违反商业公平原则，从而在美国本土对于中国企业，如：华为、阿里，等等，进行限制与施压。造成中国企业的损失。而在国内，真正做基层工作的人员，无论是出于对国产软件的信心缺失，还是由于自身的保守，抑或是收取了Oracle的好处，又纷纷诉苦，声称他们的系统离不开Oracle。从而造成的一种局面，就是管理层似乎不懂技术，只讲政治；而基层人员要为业务负责，又必须采购国外产品。同时，还有一帮同为政府部门的猪队友，给Oracle出具证书，证明他们在中国有实验室，Oracle产品可以算作是在中国生产的……</p><p>从另一个角度看，但凡有国产数据库参与的投标，只要是传统的关系数据库，无非就是人大金仓、达梦以及南大通用（Gbase 8t）。但这几款产品，各自确存在这样或那样的硬伤，对于承载中国数据库软件国产化的重任力不从心。因此，我们必须寻求新的出路。</p><p>原帖地址：<a href="https://blog.csdn.net/dataondemand0514/article/details/80570834" target="_blank" rel="noopener">https://blog.csdn.net/dataondemand0514/article/details/80570834</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于爬虫，这里有一份《中国焦虑图鉴》</title>
      <link href="/2018/08/21/%E5%85%B3%E4%BA%8E%E7%88%AC%E8%99%AB%EF%BC%8C%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%80%E4%BB%BD%E3%80%8A%E4%B8%AD%E5%9B%BD%E7%84%A6%E8%99%91%E5%9B%BE%E9%89%B4%E3%80%8B/"/>
      <url>/2018/08/21/%E5%85%B3%E4%BA%8E%E7%88%AC%E8%99%AB%EF%BC%8C%E8%BF%99%E9%87%8C%E6%9C%89%E4%B8%80%E4%BB%BD%E3%80%8A%E4%B8%AD%E5%9B%BD%E7%84%A6%E8%99%91%E5%9B%BE%E9%89%B4%E3%80%8B/</url>
      
        <content type="html"><![CDATA[<p>就在我们身边的网络上，已经密密麻麻爬满了各种网络爬虫，它们善恶不同，各怀心思。而越是每个人切身利益所在的地方，就越是爬满了爬虫。</p><p><a href="http://s3.51cto.com/oss/201807/30/3ec1424e0271c7cfee5f14083d307af4.jpg-wh_651x-s_2288948059.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/3ec1424e0271c7cfee5f14083d307af4.jpg-wh_651x-s_2288948059.jpg" alt=""></a></p><p>来源浅黑科技（ID：qianheikeji），经授权转载</p><p>来不及了，快上车。上车前，中哥先问你三个问题：</p><ul><li>你以为你在大众点评上找到的馆子，真的是几百个人给了好评，然后才出现在你的推荐里的吗？</li><li>你以为你在百度上搜索到的信息，真的是百度想让你看到的吗？（注意体会这句话的意思，不是日常黑百度）</li><li>你以为在微博上看到的热搜话题大V互动，真的都是真实发生的吗？<a id="more"></a>好，确认过眼神，我们准备发车了。</li></ul><p>最近北京下暴雨，人们寸步难行。我和幺哥坐在窗边喝茶，他看着窗外的阴霾，联想起了辛酸往事。</p><p>每年总有那么几天，幺哥会心情焦虑，坐立不安，腰膝乏力，湿身盗汗。那是因为，他又要准备抢回家的火车票了。</p><p>幺哥家在湖南，离北京上千公里。他是家里的独子，每年买到火车票准时出现在家门口是他的“义务”。</p><p>这两年，他的救命稻草是一个叫做“智行火车票”的抢票软件。他在打折的时候买了会员。</p><p>据说会员是有特权的：哪怕只抢到一张票，都会优先给他。（起码幺哥是这样安慰自己的。）</p><p>从技术上说，幺哥的救命稻草不是抢票软件，而是抢票软件背后，无数个叫做“爬虫”的东西。</p><p>说到这，中哥就得给你介绍今天的新朋友：爬虫。</p><p><a href="http://s2.51cto.com/oss/201807/30/6dc84e79085cacef5eba7d77fae751b1.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201807/30/6dc84e79085cacef5eba7d77fae751b1.jpg" alt=""></a></p><p>等等，图片错了，应该是这样的爬虫：</p><p><a href="http://s4.51cto.com/oss/201807/30/ea5c78508cc08cd7d3155e29241ebfa8.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201807/30/ea5c78508cc08cd7d3155e29241ebfa8.jpg" alt=""></a></p><p>爬虫就是一个探测机器，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。</p><p>你可以简单地想象：每个爬虫都是你的“分身”。就像孙悟空拔了一撮汗毛，吹出一堆猴子一样。</p><p>你每天使用的百度，其实就是利用了这种爬虫技术：每天放出无数爬虫到各个网站，把他们的信息抓回来，然后化好淡妆排着小队等你来检索。</p><p>抢票软件，就相当于撒出去无数个分身，每一个分身都帮助你不断刷新 12306 网站的火车余票。一旦发现有票，就马上拍下来，然后对你喊：土豪快来付款。</p><p>正好在上周末，一位黑客盆友御风神秘兮兮地给我发来一份《中国爬虫图鉴》，这哥们在腾讯云鼎实验室主要负责加班，顺便和同事们开发了很多黑科技。</p><p>比如他们搞了一个威胁情报系统，号称能探测到全世界的“爬虫”都在做什么。</p><p>我吹着口哨打开《图鉴》，但一分钟以后，我整个人都不好了。</p><p>我看到了另一个“平行世界”：就在我们身边的网络上，已经密密麻麻爬满了各种网络爬虫，它们善恶不同，各怀心思。</p><p>而越是每个人切身利益所在的地方，就越是爬满了爬虫。</p><p>看到最后，我发现这哪里是《中国爬虫图鉴》，这分明是一份《中国焦虑图鉴》。</p><p><a href="http://s3.51cto.com/oss/201807/30/b1b9048e9f14ace36a4d80f25bcd0b40.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/b1b9048e9f14ace36a4d80f25bcd0b40.jpg" alt=""></a></p><p>我们今天要说的，就和这些 App 有关。</p><p><strong>爬虫的“骚操作”</strong></p><p>爬虫也分善恶。像谷歌这样的搜索引擎爬虫，每隔几天对全网的网页扫一遍，供大家查阅，各个被扫的网站大都很开心。这种就被定义为“善意爬虫”。</p><p>但是，像抢票软件这样的爬虫，对着 12306 每秒钟恨不得撸几万次。铁总并不觉得很开心。这种就被定义为“恶意爬虫”。（注意，抢票的你觉得开心没用，被扫描的网站觉得不开心，它就是恶意的。）</p><p>给你看一张图：</p><p><a href="http://s3.51cto.com/oss/201807/30/b34d3e0325a3f530104ba8637ba0d367.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/b34d3e0325a3f530104ba8637ba0d367.jpg" alt=""></a></p><p>这张图里显示的，就是各行各业被爬虫“叨扰”的比例。（注意，这张图显示的是全世界，不是全中国。）而每一个色块背后，都是一条真实而强大的利益链条。</p><p>接下来，中哥就给你科普一下里面的骚操作。</p><p><strong>排名第一的是出行</strong></p><p>出行行业中爬虫的占比最高（20.87%）。在出行的爬虫中，有 89.02% 的流量都是冲着 12306 去的。这不意外，全中国卖火车票的独此一家别无分号。</p><p>你还记得当年 12306 上线王珞丹和白百何的“史上最坑图片验证码”么？</p><p><a href="http://s3.51cto.com/oss/201807/30/4bf79b8f102d899746b61213e9e5f114.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/4bf79b8f102d899746b61213e9e5f114.jpg" alt=""></a></p><p>这些东西不是为了故意难为老老实实买票的人的，而恰恰是为了阻止爬虫（也就是抢票软件）的点击。</p><p>刚才说了，爬虫只会简单的机械点击，它不认识白百何，所以很大一部分爬虫就被挡在了门外。</p><p>你可能会说，不对啊，我现在还可以用抢票软件抢到票啊。没错。抢票软件也不是吃素的。</p><p>它们在和铁总搞“对抗”。有一种东西叫做“打码平台”，你可以了解一下。</p><p>打码平台雇佣了很多叔叔阿姨，他们在电脑屏幕前不做别的事情，专门帮人识别验证码。</p><p>那边抢票软件遇到了验证码，系统就会自动把这些验证码传到叔叔阿姨面前，他们手工选好哪个是白百何哪个是王珞丹，然后再把结果传回去。总共的过程用不了几秒时间。</p><p>当然，这样的打码平台还有记忆功能。如果叔叔阿姨已经标记了这张图是“锅铲”，那么下次这张图片再出现的时候，系统就直接判断它是“锅铲”。</p><p>时间一长，12306 系统里的图片就被标记完了，机器自己都能认识，叔叔阿姨们可以坐在一边斗地主了。</p><p><a href="http://s1.51cto.com/oss/201807/30/c8a718f1a96ae9201678ba0cf46c6b55.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201807/30/c8a718f1a96ae9201678ba0cf46c6b55.jpg" alt=""></a></p><p>你可能会问：为什么 12306 这么抠呢？它大方地让爬虫随意爬会死吗？</p><p>答：会死。</p><p>你知道每年过年之前，12306 被点成什么样了吗？公开数据是这么说的：“最高峰时 1 天内页面浏览量达 813.4 亿次，1小时最高点击量 59.3 亿次，平均每秒 164.8 万次。”</p><p>这还是加上验证码防护之后的数据。可想而知被拦截在外面的爬虫还有多少。</p><p>况且这里还没有讨论，被抢票软件把票抢走，对我们父母那样的不会抢票的人来说，是不是公平呢？</p><p>铁路被爬虫“点鸡”成这样已经够惨了，但它还有个难兄难弟，就是航空。</p><p>而航空里，被搞得最惨的不是国航，不是海航，也不是东航。而是亚航。</p><p><a href="http://s2.51cto.com/oss/201807/30/c6a1a66600a50596b2106daadafb33b7.jpg" target="_blank" rel="noopener"><img src="http://s2.51cto.com/oss/201807/30/c6a1a66600a50596b2106daadafb33b7.jpg" alt=""></a></p><p>航空类爬虫的分布比例</p><p>很多人可能都没坐过亚洲航空。这是一家马来西亚的廉价航空公司，航线基本都是从中国各地飞往东南亚的旅游胜地，飞机上连矿泉水都得自费买，是屌丝穷X度假之首选。</p><p>为什么爬虫这么青睐亚航呢？因为它便宜。确切地说，因为它经常放出便宜的票。</p><p>本来，亚航的初衷只是随机放出一些便宜的票来吸引游客，但这里面黄牛党是有利可图的。</p><p>据我所知，他们是这样玩的：技术宅黄牛党们利用爬虫，不断刷新亚航的票务接口，一旦出现便宜的票，不管三七二十一先拍下来再说。</p><p>亚航有规定，你拍下来半小时（具体时间记不清了）不付款票就自动回到票池，继续卖。</p><p>但是黄牛党们在爬虫脚本里写好了精确的时间，到了半小时，一毫秒都不多，他又把票拍下来，如此循环。</p><p>直到有人从黄牛党这里定了这个票，黄牛党就接着利用程序，在亚航系统里放弃这张票，然后 0.00001 秒之后，就帮你用你的名字预定了这张票。</p><p>“我是中间商，我就要赚差价！”这波骚操作，堪称完美。</p><p><strong>排名第二的是社交</strong></p><p>社交的爬虫重灾区，就是你们喜闻乐见的微博。</p><p>给你看张图：</p><p><a href="http://s3.51cto.com/oss/201807/30/6b4cf48429a785bca5bf7443a1530e79.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/6b4cf48429a785bca5bf7443a1530e79.jpg" alt=""></a></p><p>这是爬虫经常光顾的微博地址。这里的代码其实指向了微博的一个接口。它可以用来获取某个人的微博列表、微博的状态、索引等等等等。</p><p>获得这些，能搞出什么骚操作呢？</p><p>你想想看，如果我能随心所欲地指挥一帮机器人，打开某人的微博，然后刷到某一条，然后疯狂关注、点赞或者留言，这不就是标准的僵尸粉上班儿的流程么。。。</p><p>其实，僵尸粉都只是爬虫的常规操作，更骚的来了：</p><ul><li>我是一个路人甲，我的微博没人关注，我用大量的爬虫，给自己做了十万人的僵尸粉，一群僵尸在我的微博下面点赞评论，不亦乐乎。</li><li>我去找一个游戏厂商，跟他说：你看我有这么多粉丝，你在我这投广告吧。我帮你发一条游戏的注册链接，每有一个人通过我的链接注册了游戏，你就给我一毛钱。广告主说，不错，就这么办。</li><li>我发出注册链接，然后没人点。。。</li><li>不慌，我让十万爬虫继续前赴后继地点击注册链接，然后自动去完成注册动作。</li><li>我躺在床上，数着赚来的一万块钱。</li></ul><p>（以上数据不一定和现实吻合，只是展现一个逻辑。具体操作也会更复杂。）</p><p>还有更骚的么？有的。</p><p>你家爱豆不是经常在微博上发红包么？好的，我率十万僵尸粉去抢。</p><p><a href="http://s3.51cto.com/oss/201807/30/8d157e0f70c19bc901f48851eb2eceb3.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/8d157e0f70c19bc901f48851eb2eceb3.jpg" alt=""></a></p><p>凭本事抢来的红包，就问你有什么不妥吗？</p><p><strong>排名第三的是电商</strong></p><p>你回忆一下，有几种东西叫做“比价平台”“聚合电商”和“返利平台”。</p><p>他们大体都是一个原理：你搜索一样商品，这类聚合平台就会自动把各个电商的商品都放在你面前供你选择。有淘宝、京东，还有唯品会苏宁易购。</p><p>这就是爬虫的功劳。它们去淘宝上，把胖次袜子杜蕾斯的图片和价格统统扒下来，然后在自己这里展示。</p><p><a href="http://s1.51cto.com/oss/201807/30/cff70e67d9591e375395556dd3eacff5.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201807/30/cff70e67d9591e375395556dd3eacff5.jpg" alt=""></a></p><p>这个原理和谷歌差不多。只不过他们展示的不是网页而是商品。但是被放在一起比价，淘宝是拒绝的，京东也是拒绝的啊。。。</p><p>然鹅，由于机器爬虫模拟的是人的点击，电商很难阻止这类事情发生。他们甚至都不能向 12306 学习。</p><p>你想想看，如果你每点开一个商品详情，淘宝都让你先分辨一次白百何和王珞丹，你肯定没心情剁手，没准还要提刀去剁马云呢。。。</p><p>当然，电商对抗爬虫有另外的方法，那就是“Web 应用防火墙”，简称 WAF。这个我们后面再单独说。</p><p>说到这，有童鞋会有个疑问：那些聚合平台，自己写爬虫，然后帮助淘宝京东卖货，他们的名字叫雷锋么？</p><p>醒醒啊同学，雷锋叔叔已经走了很多年了。我随便给你说一下这种聚合电商平台的盈利模式：</p><ul><li>假设几家店铺都卖杜蕾斯，但是用户在我这里搜索“杜蕾斯”的时候，我是有权利决定谁的店铺在前面谁在后面的啊。谁给的钱多，我就让谁在搜索的前面呗。</li></ul><p>@百度君，你说说是不是这个道理？（注意，每个店铺和淘宝平台可不是一致行动人。淘宝平台不希望自己的内容被聚合平台抓取，但每个店铺可是很乐意多一个渠道帮他们卖货的。）</p><ul><li>如果你觉得搞竞价排名良心会痛，也可以用更简单的方式——在网页上展示独立的广告。</li></ul><p>访问你网站的用户，看到页面上的广告，也有可能会点击。每点击一次，你就赚一次钱。</p><p>你还可以作为中间商，收点中介费。我帮你店家卖货了，你是不是要给我意思意思。</p><p>除了给我意思意思，你还得给来买东西的用户意思意思。这种套路，就是“返利网”这类平台的玩法。</p><p><strong>接下来是 O2O 和搜索引擎</strong></p><p>你还记得上车之前，我问了你一个问题吗？你在大众点评上看到的信息，真是吃货们点评的吗？</p><p>答：大部分时候是，但有时候不是。这里面的影响因素还是爬虫。</p><p>御风告诉我，这些爬虫很可能被用来做两件事：</p><ul><li>大众点评毕竟是最好的点评网站。很多网站都会爬取大众点评的数据，用来丰富自己的信息。</li><li>很多刚上点评的商户，信誉值不高，可以用爬虫来模拟留言、点赞，刷高自己的信誉值。</li></ul><p>所以，理论上讲一旦大众点评对这些爬虫对抗出现松懈，就会有一些不三不四的店铺被“刷”到顶部。</p><p>而与之相似的，是爬虫针对搜索引擎的进攻。</p><p>你可能了解，搜索引擎决定哪个网页排名靠前，（除了广告以外）主要一个指标就是看哪个搜索结果被人点击的次数更多。</p><p>既然这样，那么我就派出爬虫，搜索某个特定的“关键词”，然后在结果里拼命地点击某个链接，那么这个网站在搜索引擎的权重里自然就会上升。这个过程就叫做 SEO（搜索引擎优化）。</p><p>举个例子：我随意搜索一个关键词。</p><p><a href="http://s3.51cto.com/oss/201807/30/c81dfa994d9e04b2574924983ec1ad00.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/c81dfa994d9e04b2574924983ec1ad00.jpg" alt=""></a></p><p>它排在前面的网址，有可能就是经过 SEO 的。作为任何一个搜索引擎，都肯定不允许外人对于自己的搜索结果动手动脚，否则就会丧失公立性。它们会通过不定期调整算法来对抗 SEO。</p><p>尤其是很多赌博、黄色网站，搜索引擎如果敢收广告费让他们排到前面，那就离倒闭不远了。</p><p>所以黄赌毒网站只能利用黑色 SEO，强行把自己刷到前面。直到被搜索引擎发现，赶紧对它们“降权”处理。</p><p>不过御风算了算，这些黄色网站如果能把自己刷到前几位一两个小时，赚来的钱就远远超过 SEO 的费用。</p><p>这也就解释了为什么有时我们“众里寻他千百度”，蓦然回首，却看到“有人正在脱裤裤”了。</p><p><strong>最后再说说政府部门</strong></p><p>你看这张图，全是爬虫针对政府信息的爬取。</p><p><a href="http://s1.51cto.com/oss/201807/30/acd9b0ea6be1814bab0e15e1b1d8d2b4.jpg" target="_blank" rel="noopener"><img src="http://s1.51cto.com/oss/201807/30/acd9b0ea6be1814bab0e15e1b1d8d2b4.jpg" alt=""></a></p><p>第二名，北京市预约挂号统一平台。这个锅，板上钉钉要号贩子来背。</p><p>其他的，例如法院公告、信用中国、信用安徽，为什么爬虫要爬这些信息呢？</p><p>因为有些信息，是只有政府部门才掌握的。比如，谁被告过，哪家公司曾经被行政处罚，哪个人曾经进入了失信名单。这些信息综合起来，可以用来做一个公司或者个人的信誉记录。</p><p>我试着打开了一下排名第四位的“信用中国”。</p><p><a href="http://s3.51cto.com/oss/201807/30/1871bdefb33f8b7855c38afbe44bb87d.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/1871bdefb33f8b7855c38afbe44bb87d.jpg" alt=""></a></p><p>在这个平台上，你只要输入一个身份证号或者手机号，就可以查询到一个人的信用情况。拉到最底下一看，这个网站果然是是根红苗正的。</p><p><a href="http://s5.51cto.com/oss/201807/30/7265753a19dc35115a4db5b74a6d9fad.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201807/30/7265753a19dc35115a4db5b74a6d9fad.jpg" alt=""></a></p><p>如果一家公司要对外做信誉库的服务，它必须先把信用中国的信息下载到自己的库里，然后才能和其他数据进行综合运算。</p><p>如此，信用中国被爬，也就很容易解释了。</p><p>不过刚才那张表格里，排名第七的是四川住建厅。这又是什么骚操作？</p><p>根据御风的推测，这很可能是某些公司提供的一项“特殊服务”：</p><p>他们把四川省各个地区的招标情况汇总起来，然后实时提醒那些房地产公司：别睡了，起来投标了。</p><p><strong>爬虫战争</strong></p><p>说了这么多，我猜你会有几个疑问。</p><p><strong>问题 1：爬虫搞出这么多姿势，它究竟是不是违法呢？</strong></p><p>这个问题还真的不简单。</p><p>我打开中国网安第一大法《网络安全法》仔细看了半小时，在里面没有发现“爬取网络公开信息被认定为违法”的条款。</p><p>于是我又继续搜索，发现了几条司法解释：未经授权爬取用户手机通讯录超过 50 条记录；未经授权抓取用户淘宝交易记录超过 500 条；未经授权读取用户运营商网站通话记录超过 500 条；未经授权读取用户公积金社保记录的超过 50000 条的。以上这些情况可以入刑。</p><p>但是仔细看看，如果我只是用机器代替了人的手点击鼠标敲击键盘，接触的都是公开信息，并不触犯这些司法解释。（这只是我简单查询后的结果，不代表任何官方意见）</p><p>但是，对企业来说，爬虫却着实伤害了自己。有句话说：“主救自救者。”他们得组织“民兵”自己保卫自己。</p><p><strong>问题 2：爬虫战争谁会赢？</strong></p><p>爬虫和被爬企业越来越势不两立。</p><p>说白了，他们的对抗都是在阻挡对方的财路。所以下手都挺重。</p><p>企业经典的对抗方式，大概有几种：图片验证码、滑块验证、封禁 IP、给访问者增加一些加解密运算，耗费爬虫的程序资源等等。。。</p><p><a href="http://s4.51cto.com/oss/201807/30/835932c80580c6f352bb62126f20037d.jpg" target="_blank" rel="noopener"><img src="http://s4.51cto.com/oss/201807/30/835932c80580c6f352bb62126f20037d.jpg" alt=""></a></p><p>极验验证的滑块验证技术</p><p>除了刚才这些小模块，企业还可以通过 WAF（Web 应用防火墙）来防护，WAF 的功能就是通过设置一些规则，拦截掉那些不符合规则的请求。</p><p>但是，爬虫的请求，和真人的请求真的太像了。</p><p>我觉得，对这种战争一个形象的比喻就是抗癌。癌细胞的目的就是拼命躲过免疫细胞的识别，而免疫细胞的目标就是拼命分辨哪个是好细胞哪个是癌细胞。</p><p>在我看来，这场对抗爬虫的常规战眼看就要升级为“智能战”，而且战线会向云端转移。</p><p>比如某产品，听说最近就要通过人工智能的方法来识别爬虫。这里就不帮他们打广告了。还有很多其他的云安全厂商，也开始主推反爬虫的技术。</p><p>不过，就像人类目前难以消灭癌症一样，企业也难以完全消灭爬虫。但是我相信，在对抗中这条战线会达到一个精妙的平衡。这个战线每向前推进一步，都需要安全研究员付出艰辛的努力。</p><p><strong>《中国焦虑图鉴》</strong></p><p>最后，中哥帮你搞到了一张秘密表格。</p><p>这是被监测到的受爬虫侵扰最多的 Top50（采样数据，仅供参考）：</p><p><a href="http://s3.51cto.com/oss/201807/30/9eb8a8e799dcbc3b8bf3f74a5a5e2935.jpg" target="_blank" rel="noopener"><img src="http://s3.51cto.com/oss/201807/30/9eb8a8e799dcbc3b8bf3f74a5a5e2935.jpg" alt=""></a></p><p>这张表里，除了 Google、Youtube、ASK、亚洲航空这四家企业之外，应该全是中国企业（或机关）。正是从这些名字背后，我体会到了很多人的辛酸和焦虑。</p><p>爬虫是趋利的，它们永远会向有利益的地方爬行。而爬虫觉得有利益的地方，往往是我们不忍提及的隐痛。</p><p><strong>你看，排名第 1 的“中国铁路客户服务中心”</strong></p><p>无数像幺哥一样的游子，他们奋斗在一个远离家乡的城市，为了让家人有更幸福的生活。正是他们难以买到过年回家车票的事实，才把 12306 推上了爬虫榜的第一名。</p><p><strong>你看，排名第8的“最高人民法院公告查询”</strong></p><p>在中国，我们的信用体系还很不完善，骗子和老赖还可以继续蒙骗新人。所以才催生了爬虫收集法院公告，形成民间信用记录的服务。</p><p><strong>你看，排名第 15 的“北京市预约挂号统一平台”</strong></p><p>我们的医疗改革在进行，但像你我一样的普通人仍然看病难，看病贵。又便宜又好的医疗资源需要争夺，这才有了“一号难求”的现实，才有了黄牛用爬虫拼命抢号的现象。</p><p>自不用说那些神坑的虚假广告，冲榜刷量，背后都有爬虫的影子。</p><p><a href="http://s5.51cto.com/oss/201807/30/24200f78b2834001d1edaf229a91d4b7.jpg" target="_blank" rel="noopener"><img src="http://s5.51cto.com/oss/201807/30/24200f78b2834001d1edaf229a91d4b7.jpg" alt=""></a></p><p>有人说技术有罪，有人说技术无罪。</p><p>我不知道技术是否有罪，我只知道，这些盘踞在我们广袤版图上数以十亿计的爬虫，无时无刻不在提醒着我们：抱怨不会让这个世界变得更好，你想生活在一个怎样的世界，就要用自己的双手去创造它。</p><p>原文地址：<a href="http://developer.51cto.com/art/201807/579966.htm" target="_blank" rel="noopener">http://developer.51cto.com/art/201807/579966.htm</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>系统设计入门（转载）</title>
      <link href="/2018/08/20/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E5%85%A5%E9%97%A8%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/08/20/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E5%85%A5%E9%97%A8%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="系统设计主题的索引"><a href="#系统设计主题的索引" class="headerlink" title="系统设计主题的索引"></a>系统设计主题的索引</h2><blockquote><p>各种系统设计主题的摘要，包括优点和缺点。<strong>每一个主题都面临着取舍和权衡</strong>。</p><p>每个章节都包含着更多的资源的链接。</p></blockquote><p><a href="https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67" alt=""></a><br><a id="more"></a></p><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%BB%E9%A2%98%E4%BB%8E%E8%BF%99%E9%87%8C%E5%BC%80%E5%A7%8B" target="_blank" rel="noopener">系统设计主题：从这里开始</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%B8%80%E6%AD%A5%E5%9B%9E%E9%A1%BE%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7scalability%E7%9A%84%E8%A7%86%E9%A2%91%E8%AE%B2%E5%BA%A7" target="_blank" rel="noopener">第一步：回顾可扩展性的视频讲座</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%9B%9E%E9%A1%BE%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E6%96%87%E7%AB%A0" target="_blank" rel="noopener">第二步：回顾可扩展性的文章</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9A%84%E6%AD%A5%E9%AA%A4" target="_blank" rel="noopener">接下来的步骤</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%80%A7%E8%83%BD%E4%B8%8E%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7" target="_blank" rel="noopener">性能与拓展性</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BB%B6%E8%BF%9F%E4%B8%8E%E5%90%9E%E5%90%90%E9%87%8F" target="_blank" rel="noopener">延迟与吞吐量</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%AF%E7%94%A8%E6%80%A7%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener">可用性与一致性</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cap-%E7%90%86%E8%AE%BA" target="_blank" rel="noopener">CAP 理论</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cp--%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99%E6%80%A7" target="_blank" rel="noopener">CP - 一致性和分区容错性</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#ap--%E5%8F%AF%E7%94%A8%E6%80%A7%E4%B8%8E%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99%E6%80%A7" target="_blank" rel="noopener">AP - 可用性和分区容错性</a></li></ul></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">一致模式</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%B1%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener">弱一致性</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener">最终一致性</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener">强一致性</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%AF%E7%94%A8%E6%80%A7%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">可用模式</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%85%E9%9A%9C%E5%88%87%E6%8D%A2" target="_blank" rel="noopener">故障切换</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">复制</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9F%9F%E5%90%8D%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener">域名系统</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%86%85%E5%AE%B9%E5%88%86%E5%8F%91%E7%BD%91%E7%BB%9Ccdn" target="_blank" rel="noopener">CDN</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-%E6%8E%A8%E9%80%81push" target="_blank" rel="noopener">CDN 推送</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-%E6%8B%89%E5%8F%96pull" target="_blank" rel="noopener">CDN 拉取</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8" target="_blank" rel="noopener">负载均衡器</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%B7%A5%E4%BD%9C%E5%88%B0%E5%A4%87%E7%94%A8%E5%88%87%E6%8D%A2active-passive" target="_blank" rel="noopener">工作到备用切换（Active-passive）</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8C%E5%B7%A5%E4%BD%9C%E5%88%87%E6%8D%A2active-active" target="_blank" rel="noopener">双工作切换（Active-active）</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%9B%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1" target="_blank" rel="noopener">四层负载均衡</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8" target="_blank" rel="noopener">七层负载均衡</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95" target="_blank" rel="noopener">水平扩展</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86web-%E6%9C%8D%E5%8A%A1%E5%99%A8" target="_blank" rel="noopener">反向代理（web 服务器）</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86" target="_blank" rel="noopener">负载均衡与反向代理</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BA%94%E7%94%A8%E5%B1%82" target="_blank" rel="noopener">应用层</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BE%AE%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener">微服务</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0" target="_blank" rel="noopener">服务发现</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener">数据库</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9Frdbms" target="_blank" rel="noopener">关系型数据库管理系统（RDBMS）</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">Master-slave 复制集</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%B8%BB%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">Master-master 复制集</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%81%94%E5%90%88" target="_blank" rel="noopener">联合</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%86%E7%89%87" target="_blank" rel="noopener">分片</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%9D%9E%E8%A7%84%E8%8C%83%E5%8C%96" target="_blank" rel="noopener">非规范化</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-%E8%B0%83%E4%BC%98" target="_blank" rel="noopener">SQL 调优</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#nosql" target="_blank" rel="noopener">NoSQL</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%94%AE-%E5%80%BC%E5%AD%98%E5%82%A8" target="_blank" rel="noopener">Key-value 存储</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%96%87%E6%A1%A3%E7%B1%BB%E5%9E%8B%E5%AD%98%E5%82%A8" target="_blank" rel="noopener">文档存储</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%97%E5%9E%8B%E5%AD%98%E5%82%A8" target="_blank" rel="noopener">宽列存储</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener">图数据库</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-%E8%BF%98%E6%98%AF-nosql" target="_blank" rel="noopener">SQL 还是 NoSQL</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">缓存</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">客户端缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">CDN 缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#web-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">Web 服务器缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">数据库缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BA%94%E7%94%A8%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">应用缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E7%BA%A7%E5%88%AB%E7%9A%84%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">数据库查询级别的缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AF%B9%E8%B1%A1%E7%BA%A7%E5%88%AB%E7%9A%84%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">对象级别的缓存</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BD%95%E6%97%B6%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">何时更新缓存</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">缓存模式</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B4%E5%86%99%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">直写模式</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%9E%E5%86%99%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener">回写模式</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%B7%E6%96%B0" target="_blank" rel="noopener">刷新</a></li></ul></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%82%E6%AD%A5" target="_blank" rel="noopener">异步</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97" target="_blank" rel="noopener">消息队列</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97" target="_blank" rel="noopener">任务队列</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%83%8C%E5%8E%8B" target="_blank" rel="noopener">背压机制</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%80%9A%E8%AE%AF" target="_blank" rel="noopener">通讯</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AEtcp" target="_blank" rel="noopener">传输控制协议（TCP）</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%94%A8%E6%88%B7%E6%95%B0%E6%8D%AE%E6%8A%A5%E5%8D%8F%E8%AE%AEudp" target="_blank" rel="noopener">用户数据报协议（UDP）</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%BF%9C%E7%A8%8B%E8%BF%87%E7%A8%8B%E8%B0%83%E7%94%A8%E5%8D%8F%E8%AE%AErpc" target="_blank" rel="noopener">远程控制调用协议（RPC）</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%A1%A8%E8%BF%B0%E6%80%A7%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BBrest" target="_blank" rel="noopener">表述性状态转移（REST）</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AE%89%E5%85%A8" target="_blank" rel="noopener">安全</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%99%84%E5%BD%95" target="_blank" rel="noopener">附录</a><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#2-%E7%9A%84%E6%AC%A1%E6%96%B9%E8%A1%A8" target="_blank" rel="noopener">2 的次方表</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%AF%8F%E4%B8%AA%E7%A8%8B%E5%BA%8F%E5%91%98%E9%83%BD%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84%E5%BB%B6%E8%BF%9F%E6%95%B0" target="_blank" rel="noopener">每个程序员都应该知道的延迟数</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%B6%E5%AE%83%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98" target="_blank" rel="noopener">其它的系统设计面试题</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9C%9F%E5%AE%9E%E6%9E%B6%E6%9E%84" target="_blank" rel="noopener">真实架构</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%AC%E5%8F%B8%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84" target="_blank" rel="noopener">公司的系统架构</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%AC%E5%8F%B8%E5%B7%A5%E7%A8%8B%E5%8D%9A%E5%AE%A2" target="_blank" rel="noopener">公司工程博客</a></li></ul></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%AD%A3%E5%9C%A8%E5%AE%8C%E5%96%84%E4%B8%AD" target="_blank" rel="noopener">正在完善中</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%87%B4%E8%B0%A2" target="_blank" rel="noopener">致谢</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%81%94%E7%B3%BB%E6%96%B9%E5%BC%8F" target="_blank" rel="noopener">联系方式</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%AE%B8%E5%8F%AF" target="_blank" rel="noopener">许可</a></li></ul><h2 id="学习指引"><a href="#学习指引" class="headerlink" title="学习指引"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%BC%95" target="_blank" rel="noopener"></a>学习指引</h2><blockquote><p>基于你面试的时间线（短、中、长）去复习那些推荐的主题。</p></blockquote><p><a href="https://camo.githubusercontent.com/eb92600aa3bb1314b33edd0204da8428d4d3a493/687474703a2f2f692e696d6775722e636f6d2f4f66566c6c65782e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/eb92600aa3bb1314b33edd0204da8428d4d3a493/687474703a2f2f692e696d6775722e636f6d2f4f66566c6c65782e706e67" alt="Imgur"></a></p><p><strong>问：对于面试来说，我需要知道这里的所有知识点吗？</strong></p><p><strong>答：不，如果只是为了准备面试的话，你并不需要知道所有的知识点。</strong></p><p>在一场面试中你会被问到什么取决于下面这些因素：</p><ul><li>你的经验</li><li>你的技术背景</li><li>你面试的职位</li><li>你面试的公司</li><li>运气</li></ul><p>那些有经验的候选人通常会被期望了解更多的系统设计的知识。架构师或者团队负责人则会被期望了解更多除了个人贡献之外的知识。顶级的科技公司通常也会有一次或者更多的系统设计面试。</p><p>面试会很宽泛的展开并在几个领域深入。这会帮助你了解一些关于系统设计的不同的主题。基于你的时间线，经验，面试的职位和面试的公司对下面的指导做出适当的调整。</p><ul><li><strong>短期</strong>  - 以系统设计主题的<strong>广度</strong>为目标。通过解决<strong>一些</strong>面试题来练习。</li><li><strong>中期</strong>  - 以系统设计主题的<strong>广度</strong>和<strong>初级深度</strong>为目标。通过解决<strong>很多</strong>面试题来练习。</li><li><strong>长期</strong>  - 以系统设计主题的<strong>广度</strong>和<strong>高级深度</strong>为目标。通过解决<strong>大部分</strong>面试题来联系。</li></ul><table><thead><tr><th style="text-align:left">-</th><th style="text-align:left">短期</th><th style="text-align:left">中期</th><th>长期</th></tr></thead><tbody><tr><td style="text-align:left">阅读  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%BB%E9%A2%98%E7%9A%84%E7%B4%A2%E5%BC%95" target="_blank" rel="noopener">系统设计主题</a>  以获得一个关于系统如何工作的宽泛的认识</td><td style="text-align:left">👍</td><td style="text-align:left">👍</td><td>👍</td></tr><tr><td style="text-align:left">阅读一些你要面试的<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%AC%E5%8F%B8%E5%B7%A5%E7%A8%8B%E5%8D%9A%E5%AE%A2" target="_blank" rel="noopener">公司工程博客</a>的文章</td><td style="text-align:left">👍</td><td style="text-align:left">👍</td><td>👍</td></tr><tr><td style="text-align:left">阅读  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9C%9F%E5%AE%9E%E6%9E%B6%E6%9E%84" target="_blank" rel="noopener">真实架构</a></td><td style="text-align:left">👍</td><td style="text-align:left">👍</td><td>👍</td></tr><tr><td style="text-align:left">复习  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E4%B8%80%E4%B8%AA%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98" target="_blank" rel="noopener">如何处理一个系统设计面试题</a></td><td style="text-align:left">👍</td><td style="text-align:left">👍</td><td>👍</td></tr><tr><td style="text-align:left">完成  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%E5%92%8C%E8%A7%A3%E7%AD%94" target="_blank" rel="noopener">系统设计的面试题和解答</a></td><td style="text-align:left">一些</td><td style="text-align:left">很多</td><td>大部分</td></tr><tr><td style="text-align:left">完成  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E7%AD%94" target="_blank" rel="noopener">面向对象设计的面试题和解答</a></td><td style="text-align:left">一些</td><td style="text-align:left">很多</td><td>大部分</td></tr><tr><td style="text-align:left">复习  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%B6%E5%AE%83%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98" target="_blank" rel="noopener">其它的系统设计面试题</a></td><td style="text-align:left">一些</td><td style="text-align:left">很多</td><td>大部分</td></tr></tbody></table><h2 id="如何处理一个系统设计的面试题"><a href="#如何处理一个系统设计的面试题" class="headerlink" title="如何处理一个系统设计的面试题"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E4%B8%80%E4%B8%AA%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98" target="_blank" rel="noopener"></a>如何处理一个系统设计的面试题</h2><p>系统设计面试是一个<strong>开放式的对话</strong>。他们期望你去主导这个对话。</p><p>你可以使用下面的步骤来指引讨论。为了巩固这个过程，请使用下面的步骤完成<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%E5%92%8C%E8%A7%A3%E7%AD%94" target="_blank" rel="noopener">系统设计的面试题和解答</a>这个章节。</p><h3 id="第一步：描述使用场景，约束和假设"><a href="#第一步：描述使用场景，约束和假设" class="headerlink" title="第一步：描述使用场景，约束和假设"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%B8%80%E6%AD%A5%E6%8F%8F%E8%BF%B0%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E7%BA%A6%E6%9D%9F%E5%92%8C%E5%81%87%E8%AE%BE" target="_blank" rel="noopener"></a>第一步：描述使用场景，约束和假设</h3><p>把所有需要的东西聚集在一起，审视问题。不停的提问，以至于我们可以明确使用场景和约束。讨论假设。</p><ul><li>谁会使用它？</li><li>他们会怎样使用它？</li><li>有多少用户？</li><li>系统的作用是什么？</li><li>系统的输入输出分别是什么？</li><li>我们希望处理多少数据？</li><li>我们希望每秒钟处理多少请求？</li><li>我们希望的读写比率？</li></ul><h3 id="第二步：创造一个高层级的设计"><a href="#第二步：创造一个高层级的设计" class="headerlink" title="第二步：创造一个高层级的设计"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%88%9B%E9%80%A0%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B1%82%E7%BA%A7%E7%9A%84%E8%AE%BE%E8%AE%A1" target="_blank" rel="noopener"></a>第二步：创造一个高层级的设计</h3><p>使用所有重要的组件来描绘出一个高层级的设计。</p><ul><li>画出主要的组件和连接</li><li>证明你的想法</li></ul><h3 id="第三步：设计核心组件"><a href="#第三步：设计核心组件" class="headerlink" title="第三步：设计核心组件"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%B8%89%E6%AD%A5%E8%AE%BE%E8%AE%A1%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6" target="_blank" rel="noopener"></a>第三步：设计核心组件</h3><p>对每一个核心组件进行详细深入的分析。举例来说，如果你被问到<a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md" target="_blank" rel="noopener">设计一个 url 缩写服务</a>，开始讨论：</p><ul><li>生成并储存一个完整 url 的 hash<ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md" target="_blank" rel="noopener">MD5</a>  和  <a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md" target="_blank" rel="noopener">Base62</a></li><li>Hash 碰撞</li><li>SQL 还是 NoSQL</li><li>数据库模型</li></ul></li><li>将一个 hashed url 翻译成完整的 url<ul><li>数据库查找</li></ul></li><li>API 和面向对象设计</li></ul><h3 id="第四步：度量设计"><a href="#第四步：度量设计" class="headerlink" title="第四步：度量设计"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E5%9B%9B%E6%AD%A5%E5%BA%A6%E9%87%8F%E8%AE%BE%E8%AE%A1" target="_blank" rel="noopener"></a>第四步：度量设计</h3><p>确认和处理瓶颈以及一些限制。举例来说就是你需要下面的这些来完成拓展性的议题吗？</p><ul><li>负载均衡</li><li>水平拓展</li><li>缓存</li><li>数据库分片</li></ul><p>论述可能的解决办法和代价。每件事情需要取舍。可以使用<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%BB%E9%A2%98%E7%9A%84%E7%B4%A2%E5%BC%95" target="_blank" rel="noopener">可拓展系统的设计原则</a>来处理瓶颈。</p><h3 id="预估计算量"><a href="#预估计算量" class="headerlink" title="预估计算量"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%A2%84%E4%BC%B0%E8%AE%A1%E7%AE%97%E9%87%8F" target="_blank" rel="noopener"></a>预估计算量</h3><p>你或许会被要求通过手算进行一些估算。涉及到的<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%99%84%E5%BD%95" target="_blank" rel="noopener">附录</a>涉及到的是下面的这些资源：</p><ul><li><a href="http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html" target="_blank" rel="noopener">使用预估计算量</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#2-%E7%9A%84%E6%AC%A1%E6%96%B9%E8%A1%A8" target="_blank" rel="noopener">2 的次方表</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%AF%8F%E4%B8%AA%E7%A8%8B%E5%BA%8F%E5%91%98%E9%83%BD%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84%E5%BB%B6%E8%BF%9F%E6%95%B0" target="_blank" rel="noopener">每个程序员都应该知道的延迟数</a></li></ul><h3 id="相关资源和延伸阅读"><a href="#相关资源和延伸阅读" class="headerlink" title="相关资源和延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB" target="_blank" rel="noopener"></a>相关资源和延伸阅读</h3><p>查看下面的链接以获得我们期望的更好的想法：</p><ul><li><a href="https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/" target="_blank" rel="noopener">怎样通过一个系统设计的面试</a></li><li><a href="http://www.hiredintech.com/system-design" target="_blank" rel="noopener">系统设计的面试</a></li><li><a href="https://www.youtube.com/watch?v=ZgdS0EUmn70" target="_blank" rel="noopener">系统架构与设计的面试简介</a></li></ul><h2 id="系统设计的面试题和解答"><a href="#系统设计的面试题和解答" class="headerlink" title="系统设计的面试题和解答"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%E5%92%8C%E8%A7%A3%E7%AD%94" target="_blank" rel="noopener"></a>系统设计的面试题和解答</h2><blockquote><p>普通的系统设计面试题和相关事例的论述，代码和图表。</p></blockquote><blockquote><p>与内容有关的解答在  <code>solutions/</code>  文件夹中。</p></blockquote><p>问题</p><p>设计 Pastebin.com (或者 Bit.ly)</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md" target="_blank" rel="noopener">解答</a></p><p>设计 Twitter 时间线和搜索 (或者 Facebook feed 和搜索)</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/twitter/README.md" target="_blank" rel="noopener">解答</a></p><p>设计一个网页爬虫</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/web_crawler/README.md" target="_blank" rel="noopener">解答</a></p><p>设计 Mint.com</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/mint/README.md" target="_blank" rel="noopener">解答</a></p><p>为一个社交网络设计数据结构</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/social_graph/README.md" target="_blank" rel="noopener">解答</a></p><p>为搜索引擎设计一个 key-value 储存</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/query_cache/README.md" target="_blank" rel="noopener">解答</a></p><p>通过分类特性设计 Amazon 的销售排名</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/sales_rank/README.md" target="_blank" rel="noopener">解答</a></p><p>在 AWS 上设计一个百万用户级别的系统</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/scaling_aws/README.md" target="_blank" rel="noopener">解答</a></p><p>添加一个系统设计问题</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%A1%E7%8C%AE" target="_blank" rel="noopener">贡献</a></p><h3 id="设计-Pastebin-com-或者-Bit-ly"><a href="#设计-Pastebin-com-或者-Bit-ly" class="headerlink" title="设计 Pastebin.com (或者 Bit.ly)"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%AE%BE%E8%AE%A1-pastebincom-%E6%88%96%E8%80%85-bitly" target="_blank" rel="noopener"></a>设计 Pastebin.com (或者 Bit.ly)</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/4aee2d26ebedc20e7fa07a2c30780e332fa29f2c/687474703a2f2f692e696d6775722e636f6d2f346564584730542e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/4aee2d26ebedc20e7fa07a2c30780e332fa29f2c/687474703a2f2f692e696d6775722e636f6d2f346564584730542e706e67" alt="Imgur"></a></p><h3 id="设计-Twitter-时间线和搜索-或者-Facebook-feed-和搜索"><a href="#设计-Twitter-时间线和搜索-或者-Facebook-feed-和搜索" class="headerlink" title="设计 Twitter 时间线和搜索 (或者 Facebook feed 和搜索)"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%AE%BE%E8%AE%A1-twitter-%E6%97%B6%E9%97%B4%E7%BA%BF%E5%92%8C%E6%90%9C%E7%B4%A2-%E6%88%96%E8%80%85-facebook-feed-%E5%92%8C%E6%90%9C%E7%B4%A2" target="_blank" rel="noopener"></a>设计 Twitter 时间线和搜索 (或者 Facebook feed 和搜索)</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/twitter/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67" alt="Imgur"></a></p><h3 id="设计一个网页爬虫"><a href="#设计一个网页爬虫" class="headerlink" title="设计一个网页爬虫"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB" target="_blank" rel="noopener"></a>设计一个网页爬虫</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/web_crawler/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/ba21a95852d1cf7bb64c8c4622a79d1d5a20d344/687474703a2f2f692e696d6775722e636f6d2f625778507451412e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/ba21a95852d1cf7bb64c8c4622a79d1d5a20d344/687474703a2f2f692e696d6775722e636f6d2f625778507451412e706e67" alt="Imgur"></a></p><h3 id="设计-Mint-com"><a href="#设计-Mint-com" class="headerlink" title="设计 Mint.com"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%AE%BE%E8%AE%A1-mintcom" target="_blank" rel="noopener"></a>设计 Mint.com</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/mint/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/12fea5f9324f74189a9cd983b02239c68615b67e/687474703a2f2f692e696d6775722e636f6d2f563571353776552e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/12fea5f9324f74189a9cd983b02239c68615b67e/687474703a2f2f692e696d6775722e636f6d2f563571353776552e706e67" alt="Imgur"></a></p><h3 id="为一个社交网络设计数据结构"><a href="#为一个社交网络设计数据结构" class="headerlink" title="为一个社交网络设计数据结构"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BA%E4%B8%80%E4%B8%AA%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84" target="_blank" rel="noopener"></a>为一个社交网络设计数据结构</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/social_graph/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/16d78e51c2e2949e23122f4c26afe5886f82a96f/687474703a2f2f692e696d6775722e636f6d2f636443763567372e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/16d78e51c2e2949e23122f4c26afe5886f82a96f/687474703a2f2f692e696d6775722e636f6d2f636443763567372e706e67" alt="Imgur"></a></p><h3 id="为搜索引擎设计一个-key-value-储存"><a href="#为搜索引擎设计一个-key-value-储存" class="headerlink" title="为搜索引擎设计一个 key-value 储存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BA%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA-key-value-%E5%82%A8%E5%AD%98" target="_blank" rel="noopener"></a>为搜索引擎设计一个 key-value 储存</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/query_cache/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/b6439861687b9a0fc62d0149a364082643ebaf86/687474703a2f2f692e696d6775722e636f6d2f346a39396d68652e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/b6439861687b9a0fc62d0149a364082643ebaf86/687474703a2f2f692e696d6775722e636f6d2f346a39396d68652e706e67" alt="Imgur"></a></p><h3 id="设计按类别分类的-Amazon-销售排名"><a href="#设计按类别分类的-Amazon-销售排名" class="headerlink" title="设计按类别分类的 Amazon 销售排名"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%AE%BE%E8%AE%A1%E6%8C%89%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%E7%9A%84-amazon-%E9%94%80%E5%94%AE%E6%8E%92%E5%90%8D" target="_blank" rel="noopener"></a>设计按类别分类的 Amazon 销售排名</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/sales_rank/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/a56f5600f7ae29dc0c2e436b8e4e4b55c44d6894/687474703a2f2f692e696d6775722e636f6d2f4d7a45785030362e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/a56f5600f7ae29dc0c2e436b8e4e4b55c44d6894/687474703a2f2f692e696d6775722e636f6d2f4d7a45785030362e706e67" alt="Imgur"></a></p><h3 id="在-AWS-上设计一个百万用户级别的系统"><a href="#在-AWS-上设计一个百万用户级别的系统" class="headerlink" title="在 AWS 上设计一个百万用户级别的系统"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9C%A8-aws-%E4%B8%8A%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E7%99%BE%E4%B8%87%E7%94%A8%E6%88%B7%E7%BA%A7%E5%88%AB%E7%9A%84%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener"></a>在 AWS 上设计一个百万用户级别的系统</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/scaling_aws/README.md" target="_blank" rel="noopener">查看实践与解答</a></p><p><a href="https://camo.githubusercontent.com/e45e39c36eebcc4c66e1aecd4e4145112d8e88e3/687474703a2f2f692e696d6775722e636f6d2f6a6a3341354e382e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/e45e39c36eebcc4c66e1aecd4e4145112d8e88e3/687474703a2f2f692e696d6775722e636f6d2f6a6a3341354e382e706e67" alt="Imgur"></a></p><h2 id="面向对象设计的面试问题及解答"><a href="#面向对象设计的面试问题及解答" class="headerlink" title="面向对象设计的面试问题及解答"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1%E7%9A%84%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E7%AD%94" target="_blank" rel="noopener"></a>面向对象设计的面试问题及解答</h2><blockquote><p>常见面向对象设计面试问题及实例讨论，代码和图表演示。</p><p>与内容相关的解决方案在  <code>solutions/</code>  文件夹中。</p></blockquote><blockquote><p><strong>注：此节还在完善中</strong></p></blockquote><p>问题</p><p>设计 hash map</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/hash_table/hash_map.ipynb" target="_blank" rel="noopener">解决方案</a></p><p>设计 LRU 缓存</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/lru_cache/lru_cache.ipynb" target="_blank" rel="noopener">解决方案</a></p><p>设计一个呼叫中心</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/call_center/call_center.ipynb" target="_blank" rel="noopener">解决方案</a></p><p>设计一副牌</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb" target="_blank" rel="noopener">解决方案</a></p><p>设计一个停车场</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/parking_lot/parking_lot.ipynb" target="_blank" rel="noopener">解决方案</a></p><p>设计一个聊天服务</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/solutions/object_oriented_design/online_chat/online_chat.ipynb" target="_blank" rel="noopener">解决方案</a></p><p>设计一个环形数组</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%A1%E7%8C%AE" target="_blank" rel="noopener">待解决</a></p><p>添加一个面向对象设计问题</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%A1%E7%8C%AE" target="_blank" rel="noopener">待解决</a></p><h2 id="系统设计主题：从这里开始"><a href="#系统设计主题：从这里开始" class="headerlink" title="系统设计主题：从这里开始"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%BB%E9%A2%98%E4%BB%8E%E8%BF%99%E9%87%8C%E5%BC%80%E5%A7%8B" target="_blank" rel="noopener"></a>系统设计主题：从这里开始</h2><p>不熟悉系统设计？</p><p>首先，你需要对一般性原则有一个基本的认识，知道它们是什么，怎样使用以及利弊。</p><h3 id="第一步：回顾可扩展性（scalability）的视频讲座"><a href="#第一步：回顾可扩展性（scalability）的视频讲座" class="headerlink" title="第一步：回顾可扩展性（scalability）的视频讲座"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%B8%80%E6%AD%A5%E5%9B%9E%E9%A1%BE%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7scalability%E7%9A%84%E8%A7%86%E9%A2%91%E8%AE%B2%E5%BA%A7" target="_blank" rel="noopener"></a>第一步：回顾可扩展性（scalability）的视频讲座</h3><p><a href="https://www.youtube.com/watch?v=-W9F__D3oY4" target="_blank" rel="noopener">哈佛大学可扩展性讲座</a></p><ul><li>主题涵盖<ul><li>垂直扩展（Vertical scaling）</li><li>水平扩展（Horizontal scaling）</li><li>缓存</li><li>负载均衡</li><li>数据库复制</li><li>数据库分区</li></ul></li></ul><h3 id="第二步：回顾可扩展性文章"><a href="#第二步：回顾可扩展性文章" class="headerlink" title="第二步：回顾可扩展性文章"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E5%9B%9E%E9%A1%BE%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E6%96%87%E7%AB%A0" target="_blank" rel="noopener"></a>第二步：回顾可扩展性文章</h3><p><a href="http://www.lecloud.net/tagged/scalability" target="_blank" rel="noopener">可扩展性</a></p><ul><li>主题涵盖：<ul><li><a href="http://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones" target="_blank" rel="noopener">Clones</a></li><li><a href="http://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database" target="_blank" rel="noopener">数据库</a></li><li><a href="http://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache" target="_blank" rel="noopener">缓存</a></li><li><a href="http://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism" target="_blank" rel="noopener">异步</a></li></ul></li></ul><h3 id="接下来的步骤"><a href="#接下来的步骤" class="headerlink" title="接下来的步骤"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%8E%A5%E4%B8%8B%E6%9D%A5%E7%9A%84%E6%AD%A5%E9%AA%A4" target="_blank" rel="noopener"></a>接下来的步骤</h3><p>接下来，我们将看看高阶的权衡和取舍:</p><ul><li><strong>性能</strong>与<strong>可扩展性</strong></li><li><strong>延迟</strong>与<strong>吞吐量</strong></li><li><strong>可用性</strong>与<strong>一致性</strong></li></ul><p>记住<strong>每个方面都面临取舍和权衡</strong>。</p><p>然后，我们将深入更具体的主题，如 DNS、CDN 和负载均衡器。</p><h2 id="性能与可扩展性"><a href="#性能与可扩展性" class="headerlink" title="性能与可扩展性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%80%A7%E8%83%BD%E4%B8%8E%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7" target="_blank" rel="noopener"></a>性能与可扩展性</h2><p>如果服务<strong>性能</strong>的增长与资源的增加是成比例的，服务就是可扩展的。通常，提高性能意味着服务于更多的工作单元，另一方面，当数据集增长时，同样也可以处理更大的工作单位。<a href="http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html" target="_blank" rel="noopener">1</a></p><p>另一个角度来看待性能与可扩展性:</p><ul><li>如果你的系统有<strong>性能</strong>问题，对于单个用户来说是缓慢的。</li><li>如果你的系统有<strong>可扩展性</strong>问题，单个用户较快但在高负载下会变慢。</li></ul><h3 id="来源及延伸阅读"><a href="#来源及延伸阅读" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html" target="_blank" rel="noopener">简单谈谈可扩展性</a></li><li><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">可扩展性，可用性，稳定性和模式</a></li></ul><h2 id="延迟与吞吐量"><a href="#延迟与吞吐量" class="headerlink" title="延迟与吞吐量"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BB%B6%E8%BF%9F%E4%B8%8E%E5%90%9E%E5%90%90%E9%87%8F" target="_blank" rel="noopener"></a>延迟与吞吐量</h2><p><strong>延迟</strong>是执行操作或运算结果所花费的时间。</p><p><strong>吞吐量</strong>是单位时间内（执行）此类操作或运算的数量。</p><p>通常，你应该以<strong>可接受级延迟</strong>下<strong>最大化吞吐量</strong>为目标。</p><h3 id="来源及延伸阅读-1"><a href="#来源及延伸阅读-1" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-1" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="https://community.cadence.com/cadence_blogs_8/b/sd/archive/2010/09/13/understanding-latency-vs-throughput" target="_blank" rel="noopener">理解延迟与吞吐量</a></li></ul><h2 id="可用性与一致性"><a href="#可用性与一致性" class="headerlink" title="可用性与一致性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%AF%E7%94%A8%E6%80%A7%E4%B8%8E%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener"></a>可用性与一致性</h2><h3 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="CAP 理论"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cap-%E7%90%86%E8%AE%BA" target="_blank" rel="noopener"></a>CAP 理论</h3><p><a href="https://camo.githubusercontent.com/13719354da7dcd34cd79ff5f8b6306a67bc18261/687474703a2f2f692e696d6775722e636f6d2f62674c4d4932752e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/13719354da7dcd34cd79ff5f8b6306a67bc18261/687474703a2f2f692e696d6775722e636f6d2f62674c4d4932752e706e67" alt=""></a><br><strong><a href="http://robertgreiner.com/2014/08/cap-theorem-revisited" target="_blank" rel="noopener">来源：再看 CAP 理论</a></strong></p><p>在一个分布式计算系统中，只能同时满足下列的两点:</p><ul><li><strong>一致性</strong>  ─ 每次访问都能获得最新数据但可能会收到错误响应</li><li><strong>可用性</strong>  ─ 每次访问都能收到非错响应，但不保证获取到最新数据</li><li><strong>分区容错性</strong>  ─ 在任意分区网络故障的情况下系统仍能继续运行</li></ul><p><strong>网络并不可靠，所以你应要支持分区容错性，并需要在软件可用性和一致性间做出取舍。</strong></p><h4 id="CP-─-一致性和分区容错性"><a href="#CP-─-一致性和分区容错性" class="headerlink" title="CP ─ 一致性和分区容错性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cp--%E4%B8%80%E8%87%B4%E6%80%A7%E5%92%8C%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99%E6%80%A7" target="_blank" rel="noopener"></a>CP ─ 一致性和分区容错性</h4><p>等待分区节点的响应可能会导致延时错误。如果你的业务需求需要原子读写，CP 是一个不错的选择。</p><h4 id="AP-─-可用性与分区容错性"><a href="#AP-─-可用性与分区容错性" class="headerlink" title="AP ─ 可用性与分区容错性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#ap--%E5%8F%AF%E7%94%A8%E6%80%A7%E4%B8%8E%E5%88%86%E5%8C%BA%E5%AE%B9%E9%94%99%E6%80%A7" target="_blank" rel="noopener"></a>AP ─ 可用性与分区容错性</h4><p>响应节点上可用数据的最近版本可能并不是最新的。当分区解析完后，写入（操作）可能需要一些时间来传播。</p><p>如果业务需求允许<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener">最终一致性</a>，或当有外部故障时要求系统继续运行，AP 是一个不错的选择。</p><h3 id="来源及延伸阅读-2"><a href="#来源及延伸阅读-2" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-2" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="http://robertgreiner.com/2014/08/cap-theorem-revisited/" target="_blank" rel="noopener">再看 CAP 理论</a></li><li><a href="http://ksat.me/a-plain-english-introduction-to-cap-theorem/" target="_blank" rel="noopener">通俗易懂地介绍 CAP 理论</a></li><li><a href="https://github.com/henryr/cap-faq" target="_blank" rel="noopener">CAP FAQ</a></li></ul><h2 id="一致性模式"><a href="#一致性模式" class="headerlink" title="一致性模式"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener"></a>一致性模式</h2><p>有同一份数据的多份副本，我们面临着怎样同步它们的选择，以便让客户端有一致的显示数据。回想  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cap-%E7%90%86%E8%AE%BA" target="_blank" rel="noopener">CAP 理论</a>中的一致性定义 ─ 每次访问都能获得最新数据但可能会收到错误响应</p><h3 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%B1%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener"></a>弱一致性</h3><p>在写入之后，访问可能看到，也可能看不到（写入数据）。尽力优化之让其能访问最新数据。</p><p>这种方式可以 memcached 等系统中看到。弱一致性在 VoIP，视频聊天和实时多人游戏等真实用例中表现不错。打个比方，如果你在通话中丢失信号几秒钟时间，当重新连接时你是听不到这几秒钟所说的话的。</p><h3 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener"></a>最终一致性</h3><p>在写入后，访问最终能看到写入数据（通常在数毫秒内）。数据被异步复制。</p><p>DNS 和 email 等系统使用的是此种方式。最终一致性在高可用性系统中效果不错。</p><h3 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener"></a>强一致性</h3><p>在写入后，访问立即可见。数据被同步复制。</p><p>文件系统和关系型数据库（RDBMS）中使用的是此种方式。强一致性在需要记录的系统中运作良好。</p><h3 id="来源及延伸阅读-3"><a href="#来源及延伸阅读-3" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-3" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="http://snarfed.org/transactions_across_datacenters_io.html" target="_blank" rel="noopener">Transactions across data centers</a></li></ul><h2 id="可用性模式"><a href="#可用性模式" class="headerlink" title="可用性模式"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%AF%E7%94%A8%E6%80%A7%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener"></a>可用性模式</h2><p>有两种支持高可用性的模式:  <strong>故障切换（fail-over）</strong>和<strong>复制（replication）</strong>。</p><h3 id="故障切换"><a href="#故障切换" class="headerlink" title="故障切换"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%85%E9%9A%9C%E5%88%87%E6%8D%A2" target="_blank" rel="noopener"></a>故障切换</h3><h4 id="工作到备用切换（Active-passive）"><a href="#工作到备用切换（Active-passive）" class="headerlink" title="工作到备用切换（Active-passive）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%B7%A5%E4%BD%9C%E5%88%B0%E5%A4%87%E7%94%A8%E5%88%87%E6%8D%A2active-passive" target="_blank" rel="noopener"></a>工作到备用切换（Active-passive）</h4><p>关于工作到备用的故障切换流程是，工作服务器发送周期信号给待机中的备用服务器。如果周期信号中断，备用服务器切换成工作服务器的 IP 地址并恢复服务。</p><p>宕机时间取决于备用服务器处于“热”待机状态还是需要从“冷”待机状态进行启动。只有工作服务器处理流量。</p><p>工作到备用的故障切换也被称为主从切换。</p><h4 id="双工作切换（Active-active）"><a href="#双工作切换（Active-active）" class="headerlink" title="双工作切换（Active-active）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8C%E5%B7%A5%E4%BD%9C%E5%88%87%E6%8D%A2active-active" target="_blank" rel="noopener"></a>双工作切换（Active-active）</h4><p>在双工作切换中，双方都在管控流量，在它们之间分散负载。</p><p>如果是外网服务器，DNS 将需要对两方都了解。如果是内网服务器，应用程序逻辑将需要对两方都了解。</p><p>双工作切换也可以称为主主切换。</p><h3 id="缺陷：故障切换"><a href="#缺陷：故障切换" class="headerlink" title="缺陷：故障切换"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E9%99%B7%E6%95%85%E9%9A%9C%E5%88%87%E6%8D%A2" target="_blank" rel="noopener"></a>缺陷：故障切换</h3><ul><li>故障切换需要添加额外硬件并增加复杂性。</li><li>如果新写入数据在能被复制到备用系统之前，工作系统出现了故障，则有可能会丢失数据。</li></ul><h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>复制</h3><h4 id="主─从复制和主─主复制"><a href="#主─从复制和主─主复制" class="headerlink" title="主─从复制和主─主复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E5%92%8C%E4%B8%BB%E4%B8%BB%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>主─从复制和主─主复制</h4><p>这个主题进一步探讨了<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener">数据库</a>部分:</p><ul><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">主─从复制</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%B8%BB%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">主─主复制</a></li></ul><h2 id="域名系统"><a href="#域名系统" class="headerlink" title="域名系统"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9F%9F%E5%90%8D%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener"></a>域名系统</h2><p><a href="https://camo.githubusercontent.com/fae27d1291ed38dd120595d692eacd2505cd3a9c/687474703a2f2f692e696d6775722e636f6d2f494f794c6a34692e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/fae27d1291ed38dd120595d692eacd2505cd3a9c/687474703a2f2f692e696d6775722e636f6d2f494f794c6a34692e6a7067" alt=""></a><br><strong><a href="http://www.slideshare.net/srikrupa5/dns-security-presentation-issa" target="_blank" rel="noopener">来源：DNS 安全介绍</a></strong></p><p>域名系统是把  <a href="http://www.example.com/" target="_blank" rel="noopener">www.example.com</a>  等域名转换成 IP 地址。</p><p>域名系统是分层次的，一些 DNS 服务器位于顶层。当查询（域名） IP 时，路由或 ISP 提供连接 DNS 服务器的信息。较底层的 DNS 服务器缓存映射，它可能会因为 DNS 传播延时而失效。DNS 结果可以缓存在浏览器或操作系统中一段时间，时间长短取决于<a href="https://en.wikipedia.org/wiki/Time_to_live" target="_blank" rel="noopener">存活时间 TTL</a>。</p><ul><li><strong>NS 记录（域名服务）</strong>  ─ 指定解析域名或子域名的 DNS 服务器。</li><li><strong>MX 记录（邮件交换）</strong>  ─ 指定接收信息的邮件服务器。</li><li><strong>A 记录（地址）</strong>  ─ 指定域名对应的 IP 地址记录。</li><li><strong>CNAME（规范）</strong>  ─ 一个域名映射到另一个域名或  <code>CNAME</code>  记录（ example.com 指向  <a href="http://www.example.com/" target="_blank" rel="noopener">www.example.com</a>  ）或映射到一个  <code>A</code>  记录。</li></ul><p><a href="https://www.cloudflare.com/dns/" target="_blank" rel="noopener">CloudFlare</a>  和  <a href="https://aws.amazon.com/route53/" target="_blank" rel="noopener">Route 53</a>  等平台提供管理 DNS 的功能。某些 DNS 服务通过集中方式来路由流量:</p><ul><li><a href="http://g33kinfo.com/info/archives/2657" target="_blank" rel="noopener">加权轮询调度</a><ul><li>防止流量进入维护中的服务器</li><li>在不同大小集群间负载均衡</li><li>A/B 测试</li></ul></li><li>基于延迟路由</li><li>基于地理位置路由</li></ul><h3 id="缺陷-DNS"><a href="#缺陷-DNS" class="headerlink" title="缺陷:DNS"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E9%99%B7dns" target="_blank" rel="noopener"></a>缺陷:DNS</h3><ul><li>虽说缓存可以减轻 DNS 延迟，但连接 DNS 服务器还是带来了轻微的延迟。</li><li>虽然它们通常由<a href="http://superuser.com/questions/472695/who-controls-the-dns-servers/472729" target="_blank" rel="noopener">政府，网络服务提供商和大公司</a>管理，但 DNS 服务管理仍可能是复杂的。</li><li>DNS 服务最近遭受  <a href="http://dyn.com/blog/dyn-analysis-summary-of-friday-october-21-attack/" target="_blank" rel="noopener">DDoS 攻击</a>，阻止不知道 Twitter IP 地址的用户访问 Twitter。</li></ul><h3 id="来源及延伸阅读-4"><a href="#来源及延伸阅读-4" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-4" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="https://technet.microsoft.com/en-us/library/dd197427(v=ws.10" target="_blank" rel="noopener">DNS 架构</a>.aspx)</li><li><a href="https://en.wikipedia.org/wiki/Domain_Name_System" target="_blank" rel="noopener">Wikipedia</a></li><li><a href="https://support.dnsimple.com/categories/dns/" target="_blank" rel="noopener">关于 DNS 的文章</a></li></ul><h2 id="内容分发网络（CDN）"><a href="#内容分发网络（CDN）" class="headerlink" title="内容分发网络（CDN）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%86%85%E5%AE%B9%E5%88%86%E5%8F%91%E7%BD%91%E7%BB%9Ccdn" target="_blank" rel="noopener"></a>内容分发网络（CDN）</h2><p><a href="https://camo.githubusercontent.com/853a8603651149c686bf3c504769fc594ff08849/687474703a2f2f692e696d6775722e636f6d2f683954417547492e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/853a8603651149c686bf3c504769fc594ff08849/687474703a2f2f692e696d6775722e636f6d2f683954417547492e6a7067" alt=""></a><br><strong><a href="https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/" target="_blank" rel="noopener">来源：为什么使用 CDN</a></strong></p><p>内容分发网络（CDN）是一个全球性的代理服务器分布式网络，它从靠近用户的位置提供内容。通常，HTML/CSS/JS，图片和视频等静态内容由 CDN 提供，虽然亚马逊 CloudFront 等也支持动态内容。CDN 的 DNS 解析会告知客户端连接哪台服务器。</p><p>将内容存储在 CDN 上可以从两个方面来提供性能:</p><ul><li>从靠近用户的数据中心提供资源</li><li>通过 CDN 你的服务器不必真的处理请求</li></ul><h3 id="CDN-推送（push）"><a href="#CDN-推送（push）" class="headerlink" title="CDN 推送（push）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-%E6%8E%A8%E9%80%81push" target="_blank" rel="noopener"></a>CDN 推送（push）</h3><p>当你服务器上内容发生变动时，推送 CDN 接受新内容。直接推送给 CDN 并重写 URL 地址以指向你的内容的 CDN 地址。你可以配置内容到期时间及何时更新。内容只有在更改或新增是才推送，流量最小化，但储存最大化。</p><h3 id="CDN-拉取（pull）"><a href="#CDN-拉取（pull）" class="headerlink" title="CDN 拉取（pull）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-%E6%8B%89%E5%8F%96pull" target="_blank" rel="noopener"></a>CDN 拉取（pull）</h3><p>CDN 拉取是当第一个用户请求该资源时，从服务器上拉取资源。你将内容留在自己的服务器上并重写 URL 指向 CDN 地址。直到内容被缓存在 CDN 上为止，这样请求只会更慢，</p><p><a href="https://en.wikipedia.org/wiki/Time_to_live" target="_blank" rel="noopener">存活时间（TTL）</a>决定缓存多久时间。CDN 拉取方式最小化 CDN 上的储存空间，但如果过期文件并在实际更改之前被拉取，则会导致冗余的流量。</p><p>高流量站点使用 CDN 拉取效果不错，因为只有最近请求的内容保存在 CDN 中，流量才能更平衡地分散。</p><h3 id="缺陷：CDN"><a href="#缺陷：CDN" class="headerlink" title="缺陷：CDN"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E9%99%B7cdn" target="_blank" rel="noopener"></a>缺陷：CDN</h3><ul><li>CDN 成本可能因流量而异，可能在权衡之后你将不会使用 CDN。</li><li>如果在 TTL 过期之前更新内容，CDN 缓存内容可能会过时。</li><li>CDN 需要更改静态内容的 URL 地址以指向 CDN。</li></ul><h3 id="来源及延伸阅读-5"><a href="#来源及延伸阅读-5" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-5" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2112&amp;context=compsci" target="_blank" rel="noopener">全球性内容分发网络</a></li><li><a href="http://www.travelblogadvice.com/technical/the-differences-between-push-and-pull-cdns/" target="_blank" rel="noopener">CDN 拉取和 CDN 推送的区别</a></li><li><a href="https://en.wikipedia.org/wiki/Content_delivery_network" target="_blank" rel="noopener">Wikipedia</a></li></ul><h2 id="负载均衡器"><a href="#负载均衡器" class="headerlink" title="负载均衡器"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8" target="_blank" rel="noopener"></a>负载均衡器</h2><p><a href="https://camo.githubusercontent.com/21caea3d7f67f451630012f657ae59a56709365c/687474703a2f2f692e696d6775722e636f6d2f6838316e39694b2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/21caea3d7f67f451630012f657ae59a56709365c/687474703a2f2f692e696d6775722e636f6d2f6838316e39694b2e706e67" alt=""></a><br><strong><a href="http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html" target="_blank" rel="noopener">来源：可扩展的系统设计模式</a></strong></p><p>负载均衡器将传入的请求分发到应用服务器和数据库等计算资源。无论哪种情况，负载均衡器将从计算资源来的响应返回给恰当的客户端。负载均衡器的效用在于:</p><ul><li>防止请求进入不好的服务器</li><li>防止资源过载</li><li>帮助消除单一的故障点</li></ul><p>负载均衡器可以通过硬件（昂贵）或 HAProxy 等软件来实现。 增加的好处包括:</p><ul><li><strong>SSL 终结</strong>  ─ 解密传入的请求并加密服务器响应，这样的话后端服务器就不必再执行这些潜在高消耗运算了。<ul><li>不需要再每台服务器上安装  <a href="https://en.wikipedia.org/wiki/X.509" target="_blank" rel="noopener">X.509 证书</a>。</li></ul></li><li><strong>Session 留存</strong>  ─ 如果 Web 应用程序不追踪会话，发出 cookie 并将特定客户端的请求路由到同一实例。</li></ul><p>通常会设置采用<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%B7%A5%E4%BD%9C%E5%88%B0%E5%A4%87%E7%94%A8%E5%88%87%E6%8D%A2active-passive" target="_blank" rel="noopener">工作─备用</a>  或  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8C%E5%B7%A5%E4%BD%9C%E5%88%87%E6%8D%A2active-active" target="_blank" rel="noopener">双工作</a>  模式的多个负载均衡器，以免发生故障。</p><p>负载均衡器能基于多种方式来路由流量:</p><ul><li>随机</li><li>最少负载</li><li>Session/cookie</li><li><a href="http://g33kinfo.com/info/archives/2657" target="_blank" rel="noopener">轮询调度或加权轮询调度算法</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%9B%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1" target="_blank" rel="noopener">四层负载均衡</a></li><li><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1" target="_blank" rel="noopener">七层负载均衡</a></li></ul><h3 id="四层负载均衡"><a href="#四层负载均衡" class="headerlink" title="四层负载均衡"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%9B%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1" target="_blank" rel="noopener"></a>四层负载均衡</h3><p>四层负载均衡根据监看<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%80%9A%E8%AE%AF" target="_blank" rel="noopener">传输层</a>的信息来决定如何分发请求。通常，这会涉及来源，目标 IP 地址和请求头中的端口，但不包括数据包（报文）内容。四层负载均衡执行<a href="https://www.nginx.com/resources/glossary/layer-4-load-balancing/" target="_blank" rel="noopener">网络地址转换（NAT）</a>来向上游服务器转发网络数据包。</p><h3 id="七层负载均衡器"><a href="#七层负载均衡器" class="headerlink" title="七层负载均衡器"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8" target="_blank" rel="noopener"></a>七层负载均衡器</h3><p>七层负载均衡器根据监控<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%80%9A%E8%AE%AF" target="_blank" rel="noopener">应用层</a>来决定怎样分发请求。这会涉及请求头的内容，消息和 cookie。七层负载均衡器终结网络流量，读取消息，做出负载均衡判定，然后传送给特定服务器。比如，一个七层负载均衡器能直接将视频流量连接到托管视频的服务器，同时将更敏感的用户账单流量引导到安全性更强的服务器。</p><p>以损失灵活性为代价，四层负载均衡比七层负载均衡花费更少时间和计算资源，虽然这对现代商用硬件的性能影响甚微。</p><h3 id="水平扩展"><a href="#水平扩展" class="headerlink" title="水平扩展"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95" target="_blank" rel="noopener"></a>水平扩展</h3><p>负载均衡器还能帮助水平扩展，提高性能和可用性。使用商业硬件的性价比更高，并且比在单台硬件上<strong>垂直扩展</strong>更贵的硬件具有更高的可用性。相比招聘特定企业系统人才，招聘商业硬件方面的人才更加容易。</p><h4 id="缺陷：水平扩展"><a href="#缺陷：水平扩展" class="headerlink" title="缺陷：水平扩展"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E9%99%B7%E6%B0%B4%E5%B9%B3%E6%89%A9%E5%B1%95" target="_blank" rel="noopener"></a>缺陷：水平扩展</h4><ul><li>水平扩展引入了复杂度并涉及服务器复制<ul><li>服务器应该是无状态的:它们也不该包含像 session 或资料图片等与用户关联的数据。</li><li>session 可以集中存储在数据库或持久化<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98" target="_blank" rel="noopener">缓存</a>（Redis、Memcached）的数据存储区中。</li></ul></li><li>缓存和数据库等下游服务器需要随着上游服务器进行扩展，以处理更多的并发连接。</li></ul><h3 id="缺陷：负载均衡器"><a href="#缺陷：负载均衡器" class="headerlink" title="缺陷：负载均衡器"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E9%99%B7%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8" target="_blank" rel="noopener"></a>缺陷：负载均衡器</h3><ul><li>如果没有足够的资源配置或配置错误，负载均衡器会变成一个性能瓶颈。</li><li>引入负载均衡器以帮助消除单点故障但导致了额外的复杂性。</li><li>单个负载均衡器会导致单点故障，但配置多个负载均衡器会进一步增加复杂性。</li></ul><h3 id="来源及延伸阅读-6"><a href="#来源及延伸阅读-6" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-6" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/" target="_blank" rel="noopener">NGINX 架构</a></li><li><a href="http://www.haproxy.org/download/1.2/doc/architecture.txt" target="_blank" rel="noopener">HAProxy 架构指南</a></li><li><a href="http://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones" target="_blank" rel="noopener">可扩展性</a></li><li><a href="https://en.wikipedia.org/wiki/Load_balancing_(computing" target="_blank" rel="noopener">Wikipedia</a>)</li><li><a href="https://www.nginx.com/resources/glossary/layer-4-load-balancing/" target="_blank" rel="noopener">四层负载平衡</a></li><li><a href="https://www.nginx.com/resources/glossary/layer-7-load-balancing/" target="_blank" rel="noopener">七层负载平衡</a></li><li><a href="http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html" target="_blank" rel="noopener">ELB 监听器配置</a></li></ul><h2 id="反向代理（web-服务器）"><a href="#反向代理（web-服务器）" class="headerlink" title="反向代理（web 服务器）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86web-%E6%9C%8D%E5%8A%A1%E5%99%A8" target="_blank" rel="noopener"></a>反向代理（web 服务器）</h2><p><a href="https://camo.githubusercontent.com/e88216d0999853426f72b28e41223f43977d22b7/687474703a2f2f692e696d6775722e636f6d2f6e3431417a66662e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/e88216d0999853426f72b28e41223f43977d22b7/687474703a2f2f692e696d6775722e636f6d2f6e3431417a66662e706e67" alt=""></a><br><strong><a href="https://upload.wikimedia.org/wikipedia/commons/6/67/Reverse_proxy_h2g2bob.svg" target="_blank" rel="noopener">资料来源：维基百科</a></strong>  </p><p>反向代理是一种可以集中地调用内部服务，并提供统一接口给公共客户的 web 服务器。来自客户端的请求先被反向代理服务器转发到可响应请求的服务器，然后代理再把服务器的响应结果返回给客户端。</p><p>带来的好处包括：</p><ul><li><strong>增加安全性</strong>  - 隐藏后端服务器的信息，屏蔽黑名单中的 IP，限制每个客户端的连接数。</li><li><strong>提高可扩展性和灵活性</strong>  - 客户端只能看到反向代理服务器的 IP，这使你可以增减服务器或者修改它们的配置。</li><li><strong>本地终结 SSL 会话</strong>  - 解密传入请求，加密服务器响应，这样后端服务器就不必完成这些潜在的高成本的操作。<ul><li>免除了在每个服务器上安装  <a href="https://en.wikipedia.org/wiki/X.509" target="_blank" rel="noopener">X.509</a>  证书的需要</li></ul></li><li><strong>压缩</strong>  - 压缩服务器响应</li><li><strong>缓存</strong>  - 直接返回命中的缓存结果</li><li><strong>静态内容</strong>  - 直接提供静态内容<ul><li>HTML/CSS/JS</li><li>图片</li><li>视频</li><li>等等</li></ul></li></ul><h3 id="负载均衡器与反向代理"><a href="#负载均衡器与反向代理" class="headerlink" title="负载均衡器与反向代理"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86" target="_blank" rel="noopener"></a>负载均衡器与反向代理</h3><ul><li>当你有多个服务器时，部署负载均衡器非常有用。通常，负载均衡器将流量路由给一组功能相同的服务器上。</li><li>即使只有一台 web 服务器或者应用服务器时，反向代理也有用，可以参考上一节介绍的好处。</li><li>NGINX 和 HAProxy 等解决方案可以同时支持第七层反向代理和负载均衡。</li></ul><h3 id="不利之处：反向代理"><a href="#不利之处：反向代理" class="headerlink" title="不利之处：反向代理"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86" target="_blank" rel="noopener"></a>不利之处：反向代理</h3><ul><li>引入反向代理会增加系统的复杂度。</li><li>单独一个反向代理服务器仍可能发生单点故障，配置多台反向代理服务器（如<a href="https://en.wikipedia.org/wiki/Failover" target="_blank" rel="noopener">故障转移</a>）会进一步增加复杂度。</li></ul><h3 id="来源及延伸阅读-7"><a href="#来源及延伸阅读-7" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-7" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="https://www.nginx.com/resources/glossary/reverse-proxy-vs-load-balancer/" target="_blank" rel="noopener">反向代理与负载均衡</a></li><li><a href="https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/" target="_blank" rel="noopener">NGINX 架构</a></li><li><a href="http://www.haproxy.org/download/1.2/doc/architecture.txt" target="_blank" rel="noopener">HAProxy 架构指南</a></li><li><a href="https://en.wikipedia.org/wiki/Reverse_proxy" target="_blank" rel="noopener">Wikipedia</a></li></ul><h2 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BA%94%E7%94%A8%E5%B1%82" target="_blank" rel="noopener"></a>应用层</h2><p><a href="https://camo.githubusercontent.com/feeb549c5b6e94f65c613635f7166dc26e0c7de7/687474703a2f2f692e696d6775722e636f6d2f7942355359776d2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/feeb549c5b6e94f65c613635f7166dc26e0c7de7/687474703a2f2f692e696d6775722e636f6d2f7942355359776d2e706e67" alt=""></a><br><strong><a href="http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer" target="_blank" rel="noopener">资料来源：可缩放系统构架介绍</a></strong></p><p>将 Web 服务层与应用层（也被称作平台层）分离，可以独立缩放和配置这两层。添加新的 API 只需要添加应用服务器，而不必添加额外的 web 服务器。</p><p><strong>单一职责原则</strong>提倡小型的，自治的服务共同合作。小团队通过提供小型的服务，可以更激进地计划增长。</p><p>应用层中的工作进程也有可以实现<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%82%E6%AD%A5" target="_blank" rel="noopener">异步化</a>。</p><h3 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BE%AE%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener"></a>微服务</h3><p>与此讨论相关的话题是  <a href="https://en.wikipedia.org/wiki/Microservices" target="_blank" rel="noopener">微服务</a>，可以被描述为一系列可以独立部署的小型的，模块化服务。每个服务运行在一个独立的线程中，通过明确定义的轻量级机制通讯，共同实现业务目标。<a href="https://smartbear.com/learn/api-design/what-are-microservices" target="_blank" rel="noopener">1</a></p><p>例如，Pinterest 可能有这些微服务： 用户资料、关注者、Feed 流、搜索、照片上传等。</p><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0" target="_blank" rel="noopener"></a>服务发现</h3><p>像  <a href="https://www.consul.io/docs/index.html" target="_blank" rel="noopener">Consul</a>，<a href="https://coreos.com/etcd/docs/latest" target="_blank" rel="noopener">Etcd</a>  和  <a href="http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper" target="_blank" rel="noopener">Zookeeper</a>  这样的系统可以通过追踪注册名、地址、端口等信息来帮助服务互相发现对方。<a href="https://www.consul.io/intro/getting-started/checks.html" target="_blank" rel="noopener">Health checks</a>可以帮助确认服务的完整性和是否经常使用一个  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AEhttp" target="_blank" rel="noopener">HTTP</a>  路径。Consul 和 Etcd 都有一个内建的  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%94%AE-%E5%80%BC%E5%AD%98%E5%82%A8" target="_blank" rel="noopener">key-value 存储</a>  用来存储配置信息和其他的共享信息。</p><h3 id="不利之处：应用层"><a href="#不利之处：应用层" class="headerlink" title="不利之处：应用层"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E5%BA%94%E7%94%A8%E5%B1%82" target="_blank" rel="noopener"></a>不利之处：应用层</h3><ul><li>添加由多个松耦合服务组成的应用层，从架构、运营、流程等层面来讲将非常不同（相对于单体系统）。</li><li>微服务会增加部署和运营的复杂度。</li></ul><h3 id="来源及延伸阅读-8"><a href="#来源及延伸阅读-8" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-8" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="http://lethain.com/introduction-to-architecting-systems-for-scale" target="_blank" rel="noopener">可缩放系统构架介绍</a></li><li><a href="http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview" target="_blank" rel="noopener">破解系统设计面试</a></li><li><a href="https://en.wikipedia.org/wiki/Service-oriented_architecture" target="_blank" rel="noopener">面向服务架构</a></li><li><a href="http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper" target="_blank" rel="noopener">Zookeeper 介绍</a></li><li><a href="https://cloudncode.wordpress.com/2016/07/22/msa-getting-started/" target="_blank" rel="noopener">构建微服务，你所需要知道的一切</a></li></ul><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener"></a>数据库</h2><p><a href="https://camo.githubusercontent.com/15a7553727e6da98d0de5e9ca3792f6d2b5e92d4/687474703a2f2f692e696d6775722e636f6d2f586b6d3543587a2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/15a7553727e6da98d0de5e9ca3792f6d2b5e92d4/687474703a2f2f692e696d6775722e636f6d2f586b6d3543587a2e706e67" alt=""></a><br><strong><a href="https://www.youtube.com/watch?v=w95murBkYmU" target="_blank" rel="noopener">资料来源：扩展你的用户数到第一个一千万</a></strong></p><h3 id="关系型数据库管理系统（RDBMS）"><a href="#关系型数据库管理系统（RDBMS）" class="headerlink" title="关系型数据库管理系统（RDBMS）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9Frdbms" target="_blank" rel="noopener"></a>关系型数据库管理系统（RDBMS）</h3><p>像 SQL 这样的关系型数据库是一系列以表的形式组织的数据项集合。</p><blockquote><p>校对注：这里作者 SQL 可能指的是 MySQL</p></blockquote><p><strong>ACID</strong>  用来描述关系型数据库<a href="https://en.wikipedia.org/wiki/Database_transaction" target="_blank" rel="noopener">事务</a>的特性。</p><ul><li><strong>原子性</strong>  - 每个事务内部所有操作要么全部完成，要么全部不完成。</li><li><strong>一致性</strong>  - 任何事务都使数据库从一个有效的状态转换到另一个有效状态。</li><li><strong>隔离性</strong>  - 并发执行事务的结果与顺序执行事务的结果相同。</li><li><strong>持久性</strong>  - 事务提交后，对系统的影响是永久的。</li></ul><p>关系型数据库扩展包括许多技术：<strong>主从复制</strong>、<strong>主主复制</strong>、<strong>联合</strong>、<strong>分片</strong>、<strong>非规范化</strong>和  <strong>SQL调优</strong>。</p><p><a href="https://camo.githubusercontent.com/6a097809b9690236258747d969b1d3e0d93bb8ca/687474703a2f2f692e696d6775722e636f6d2f4339696f47746e2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/6a097809b9690236258747d969b1d3e0d93bb8ca/687474703a2f2f692e696d6775722e636f6d2f4339696f47746e2e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">资料来源：可扩展性、可用性、稳定性、模式</a></strong></p><h4 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>主从复制</h4><p>主库同时负责读取和写入操作，并复制写入到一个或多个从库中，从库只负责读操作。树状形式的从库再将写入复制到更多的从库中去。如果主库离线，系统可以以只读模式运行，直到某个从库被提升为主库或有新的主库出现。</p><h5 id="不利之处：主从复制"><a href="#不利之处：主从复制" class="headerlink" title="不利之处：主从复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>不利之处：主从复制</h5><ul><li>将从库提升为主库需要额外的逻辑。</li><li>参考<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">不利之处：复制</a>中，主从复制和主主复制<strong>共同</strong>的问题。</li></ul><p><a href="https://camo.githubusercontent.com/5862604b102ee97d85f86f89edda44bde85a5b7f/687474703a2f2f692e696d6775722e636f6d2f6b7241484c47672e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/5862604b102ee97d85f86f89edda44bde85a5b7f/687474703a2f2f692e696d6775722e636f6d2f6b7241484c47672e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">资料来源：可扩展性、可用性、稳定性、模式</a></strong></p><h4 id="主主复制"><a href="#主主复制" class="headerlink" title="主主复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%BB%E4%B8%BB%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>主主复制</h4><p>两个主库都负责读操作和写操作，写入操作时互相协调。如果其中一个主库挂机，系统可以继续读取和写入。</p><h5 id="不利之处：-主主复制"><a href="#不利之处：-主主复制" class="headerlink" title="不利之处： 主主复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84-%E4%B8%BB%E4%B8%BB%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>不利之处： 主主复制</h5><ul><li>你需要添加负载均衡器或者在应用逻辑中做改动，来确定写入哪一个数据库。</li><li>多数主-主系统要么不能保证一致性（违反 ACID），要么因为同步产生了写入延迟。</li><li>随着更多写入节点的加入和延迟的提高，如何解决冲突显得越发重要。</li><li>参考<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener">不利之处：复制</a>中，主从复制和主主复制<strong>共同</strong>的问题。</li></ul><h5 id="不利之处：复制"><a href="#不利之处：复制" class="headerlink" title="不利之处：复制"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E5%A4%8D%E5%88%B6" target="_blank" rel="noopener"></a>不利之处：复制</h5><ul><li>如果主库在将新写入的数据复制到其他节点前挂掉，则有数据丢失的可能。</li><li>写入会被重放到负责读取操作的副本。副本可能因为过多写操作阻塞住，导致读取功能异常。</li><li>读取从库越多，需要复制的写入数据就越多，导致更严重的复制延迟。</li><li>在某些数据库系统中，写入主库的操作可以用多个线程并行写入，但读取副本只支持单线程顺序地写入。</li><li>复制意味着更多的硬件和额外的复杂度。</li></ul><h5 id="来源及延伸阅读-9"><a href="#来源及延伸阅读-9" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-9" target="_blank" rel="noopener"></a>来源及延伸阅读</h5><ul><li><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">扩展性，可用性，稳定性模式</a></li><li><a href="https://en.wikipedia.org/wiki/Multi-master_replication" target="_blank" rel="noopener">多主复制</a></li></ul><h4 id="联合"><a href="#联合" class="headerlink" title="联合"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%81%94%E5%90%88" target="_blank" rel="noopener"></a>联合</h4><p><a href="https://camo.githubusercontent.com/6eb6570a8b6b4e1d52e3d7cc07e7959ea5dac75f/687474703a2f2f692e696d6775722e636f6d2f553371563333652e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/6eb6570a8b6b4e1d52e3d7cc07e7959ea5dac75f/687474703a2f2f692e696d6775722e636f6d2f553371563333652e706e67" alt=""></a><br><strong><a href="https://www.youtube.com/watch?v=w95murBkYmU" target="_blank" rel="noopener">资料来源：扩展你的用户数到第一个一千万</a></strong></p><p>联合（或按功能划分）将数据库按对应功能分割。例如，你可以有三个数据库：<strong>论坛</strong>、<strong>用户</strong>和<strong>产品</strong>，而不仅是一个单体数据库，从而减少每个数据库的读取和写入流量，减少复制延迟。较小的数据库意味着更多适合放入内存的数据，进而意味着更高的缓存命中几率。没有只能串行写入的中心化主库，你可以并行写入，提高负载能力。</p><h5 id="不利之处：联合"><a href="#不利之处：联合" class="headerlink" title="不利之处：联合"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E8%81%94%E5%90%88" target="_blank" rel="noopener"></a>不利之处：联合</h5><ul><li>如果你的数据库模式需要大量的功能和数据表，联合的效率并不好。</li><li>你需要更新应用程序的逻辑来确定要读取和写入哪个数据库。</li><li>用  <a href="http://stackoverflow.com/questions/5145637/querying-data-by-joining-two-tables-in-two-database-on-different-servers" target="_blank" rel="noopener">server link</a>  从两个库联结数据更复杂。</li><li>联合需要更多的硬件和额外的复杂度。</li></ul><h5 id="来源及延伸阅读：联合"><a href="#来源及延伸阅读：联合" class="headerlink" title="来源及延伸阅读：联合"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%E8%81%94%E5%90%88" target="_blank" rel="noopener"></a>来源及延伸阅读：联合</h5><ul><li><a href="https://www.youtube.com/watch?v=w95murBkYmU" target="_blank" rel="noopener">扩展你的用户数到第一个一千万</a></li></ul><h4 id="分片"><a href="#分片" class="headerlink" title="分片"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%86%E7%89%87" target="_blank" rel="noopener"></a>分片</h4><p><a href="https://camo.githubusercontent.com/1df78be67b749171569a0e11a51aa76b3b678d4f/687474703a2f2f692e696d6775722e636f6d2f775538783549642e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1df78be67b749171569a0e11a51aa76b3b678d4f/687474703a2f2f692e696d6775722e636f6d2f775538783549642e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">资料来源：可扩展性、可用性、稳定性、模式</a></strong></p><p>分片将数据分配在不同的数据库上，使得每个数据库仅管理整个数据集的一个子集。以用户数据库为例，随着用户数量的增加，越来越多的分片会被添加到集群中。</p><p>类似<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%81%94%E5%90%88" target="_blank" rel="noopener">联合</a>的优点，分片可以减少读取和写入流量，减少复制并提高缓存命中率。也减少了索引，通常意味着查询更快，性能更好。如果一个分片出问题，其他的仍能运行，你可以使用某种形式的冗余来防止数据丢失。类似联合，没有只能串行写入的中心化主库，你可以并行写入，提高负载能力。</p><p>常见的做法是用户姓氏的首字母或者用户的地理位置来分隔用户表。</p><h5 id="不利之处：分片"><a href="#不利之处：分片" class="headerlink" title="不利之处：分片"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E5%88%86%E7%89%87" target="_blank" rel="noopener"></a>不利之处：分片</h5><ul><li>你需要修改应用程序的逻辑来实现分片，这会带来复杂的 SQL 查询。</li><li>分片不合理可能导致数据负载不均衡。例如，被频繁访问的用户数据会导致其所在分片的负载相对其他分片高。<ul><li>再平衡会引入额外的复杂度。基于<a href="http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html" target="_blank" rel="noopener">一致性哈希</a>的分片算法可以减少这种情况。</li></ul></li><li>联结多个分片的数据操作更复杂。</li><li>分片需要更多的硬件和额外的复杂度。</li></ul><h4 id="来源及延伸阅读：分片"><a href="#来源及延伸阅读：分片" class="headerlink" title="来源及延伸阅读：分片"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%E5%88%86%E7%89%87" target="_blank" rel="noopener"></a>来源及延伸阅读：分片</h4><ul><li><a href="http://highscalability.com/blog/2009/8/6/an-unorthodox-approach-to-database-design-the-coming-of-the.html" target="_blank" rel="noopener">分片时代来临</a></li><li><a href="https://en.wikipedia.org/wiki/Shard_(database_architecture" target="_blank" rel="noopener">数据库分片架构</a>)</li><li><a href="http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html" target="_blank" rel="noopener">一致性哈希</a></li></ul><h4 id="非规范化"><a href="#非规范化" class="headerlink" title="非规范化"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%9D%9E%E8%A7%84%E8%8C%83%E5%8C%96" target="_blank" rel="noopener"></a>非规范化</h4><p>非规范化试图以写入性能为代价来换取读取性能。在多个表中冗余数据副本，以避免高成本的联结操作。一些关系型数据库，比如  <a href="https://en.wikipedia.org/wiki/PostgreSQL" target="_blank" rel="noopener">PostgreSQL</a>  和 Oracle 支持<a href="https://en.wikipedia.org/wiki/Materialized_view" target="_blank" rel="noopener">物化视图</a>，可以处理冗余信息存储和保证冗余副本一致。</p><p>当数据使用诸如<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%81%94%E5%90%88" target="_blank" rel="noopener">联合</a>和<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%86%E7%89%87" target="_blank" rel="noopener">分片</a>等技术被分割，进一步提高了处理跨数据中心的联结操作复杂度。非规范化可以规避这种复杂的联结操作。</p><p>在多数系统中，读取操作的频率远高于写入操作，比例可达到 100:1，甚至 1000:1。需要复杂的数据库联结的读取操作成本非常高，在磁盘操作上消耗了大量时间。</p><h5 id="不利之处：非规范化"><a href="#不利之处：非规范化" class="headerlink" title="不利之处：非规范化"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%B8%8D%E5%88%A9%E4%B9%8B%E5%A4%84%E9%9D%9E%E8%A7%84%E8%8C%83%E5%8C%96" target="_blank" rel="noopener"></a>不利之处：非规范化</h5><ul><li>数据会冗余。</li><li>约束可以帮助冗余的信息副本保持同步，但这样会增加数据库设计的复杂度。</li><li>非规范化的数据库在高写入负载下性能可能比规范化的数据库差。</li></ul><h5 id="来源及延伸阅读：非规范化"><a href="#来源及延伸阅读：非规范化" class="headerlink" title="来源及延伸阅读：非规范化"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%E9%9D%9E%E8%A7%84%E8%8C%83%E5%8C%96" target="_blank" rel="noopener"></a>来源及延伸阅读：非规范化</h5><ul><li><a href="https://en.wikipedia.org/wiki/Denormalization" target="_blank" rel="noopener">非规范化</a></li></ul><h4 id="SQL-调优"><a href="#SQL-调优" class="headerlink" title="SQL 调优"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-%E8%B0%83%E4%BC%98" target="_blank" rel="noopener"></a>SQL 调优</h4><p>SQL 调优是一个范围很广的话题，有很多相关的<a href="https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&amp;field-keywords=sql+tuning" target="_blank" rel="noopener">书</a>可以作为参考。</p><p>利用<strong>基准测试</strong>和<strong>性能分析</strong>来模拟和发现系统瓶颈很重要。</p><ul><li><strong>基准测试</strong>  - 用  <a href="http://httpd.apache.org/docs/2.2/programs/ab.html" target="_blank" rel="noopener">ab</a>  等工具模拟高负载情况。</li><li><strong>性能分析</strong>  - 通过启用如<a href="http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html" target="_blank" rel="noopener">慢查询日志</a>等工具来辅助追踪性能问题。</li></ul><p>基准测试和性能分析可能会指引你到以下优化方案。</p><h5 id="改进模式"><a href="#改进模式" class="headerlink" title="改进模式"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%94%B9%E8%BF%9B%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener"></a>改进模式</h5><ul><li>为了实现快速访问，MySQL 在磁盘上用连续的块存储数据。</li><li>使用  <code>CHAR</code>  类型存储固定长度的字段，不要用  <code>VARCHAR</code>。<ul><li><code>CHAR</code>  在快速、随机访问时效率很高。如果使用  <code>VARCHAR</code>，如果你想读取下一个字符串，不得不先读取到当前字符串的末尾。</li></ul></li><li>使用  <code>TEXT</code>  类型存储大块的文本，例如博客正文。<code>TEXT</code>  还允许布尔搜索。使用  <code>TEXT</code>  字段需要在磁盘上存储一个用于定位文本块的指针。</li><li>使用  <code>INT</code>  类型存储高达 2^32 或 40 亿的较大数字。</li><li>使用  <code>DECIMAL</code>  类型存储货币可以避免浮点数表示错误。</li><li>避免使用  <code>BLOBS</code>  存储对象，存储存放对象的位置。</li><li><code>VARCHAR(255)</code>  是以 8 位数字存储的最大字符数，在某些关系型数据库中，最大限度地利用字节。</li><li>在适用场景中设置  <code>NOT NULL</code>  约束来<a href="http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search" target="_blank" rel="noopener">提高搜索性能</a>。</li></ul><h5 id="使用正确的索引"><a href="#使用正确的索引" class="headerlink" title="使用正确的索引"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BD%BF%E7%94%A8%E6%AD%A3%E7%A1%AE%E7%9A%84%E7%B4%A2%E5%BC%95" target="_blank" rel="noopener"></a>使用正确的索引</h5><ul><li>你正查询（<code>SELECT</code>、<code>GROUP BY</code>、<code>ORDER BY</code>、<code>JOIN</code>）的列如果用了索引会更快。</li><li>索引通常表示为自平衡的  <a href="https://en.wikipedia.org/wiki/B-tree" target="_blank" rel="noopener">B 树</a>，可以保持数据有序，并允许在对数时间内进行搜索，顺序访问，插入，删除操作。</li><li>设置索引，会将数据存在内存中，占用了更多内存空间。</li><li>写入操作会变慢，因为索引需要被更新。</li><li>加载大量数据时，禁用索引再加载数据，然后重建索引，这样也许会更快。</li></ul><h5 id="避免高成本的联结操作"><a href="#避免高成本的联结操作" class="headerlink" title="避免高成本的联结操作"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%81%BF%E5%85%8D%E9%AB%98%E6%88%90%E6%9C%AC%E7%9A%84%E8%81%94%E7%BB%93%E6%93%8D%E4%BD%9C" target="_blank" rel="noopener"></a>避免高成本的联结操作</h5><ul><li>有性能需要，可以进行非规范化。</li></ul><h5 id="分割数据表"><a href="#分割数据表" class="headerlink" title="分割数据表"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E8%A1%A8" target="_blank" rel="noopener"></a>分割数据表</h5><ul><li>将热点数据拆分到单独的数据表中，可以有助于缓存。</li></ul><h5 id="调优查询缓存"><a href="#调优查询缓存" class="headerlink" title="调优查询缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B0%83%E4%BC%98%E6%9F%A5%E8%AF%A2%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>调优查询缓存</h5><ul><li>在某些情况下，<a href="http://dev.mysql.com/doc/refman/5.7/en/query-cache" target="_blank" rel="noopener">查询缓存</a>可能会导致<a href="https://www.percona.com/blog/2014/01/28/10-mysql-performance-tuning-settings-after-installation/" target="_blank" rel="noopener">性能问题</a>。</li></ul><h5 id="来源及延伸阅读-10"><a href="#来源及延伸阅读-10" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-10" target="_blank" rel="noopener"></a>来源及延伸阅读</h5><ul><li><a href="http://20bits.com/article/10-tips-for-optimizing-mysql-queries-that-dont-suck" target="_blank" rel="noopener">MySQL 查询优化小贴士</a></li><li><a href="http://stackoverflow.com/questions/1217466/is-there-a-good-reason-i-see-varchar255-used-so-often-as-opposed-to-another-l" target="_blank" rel="noopener">为什么 VARCHAR(255) 很常见？</a></li><li><a href="http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search" target="_blank" rel="noopener">Null 值是如何影响数据库性能的？</a></li><li><a href="http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html" target="_blank" rel="noopener">慢查询日志</a></li></ul><h3 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#nosql" target="_blank" rel="noopener"></a>NoSQL</h3><p>NoSQL 是<strong>键-值数据库</strong>、<strong>文档型数据库</strong>、<strong>列型数据库</strong>或<strong>图数据库</strong>的统称。数据库是非规范化的，表联结大多在应用程序代码中完成。大多数 NoSQL 无法实现真正符合 ACID 的事务，支持<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7" target="_blank" rel="noopener">最终一致</a>。</p><p><strong>BASE</strong>  通常被用于描述 NoSQL 数据库的特性。相比  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cap-%E7%90%86%E8%AE%BA" target="_blank" rel="noopener">CAP 理论</a>，BASE 强调可用性超过一致性。</p><ul><li><strong>基本可用</strong>  - 系统保证可用性。</li><li><strong>软状态</strong>  - 即使没有输入，系统状态也可能随着时间变化。</li><li><strong>最终一致性</strong>  - 经过一段时间之后，系统最终会变一致，因为系统在此期间没有收到任何输入。</li></ul><p>除了在  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-%E8%BF%98%E6%98%AF-nosql" target="_blank" rel="noopener">SQL 还是 NoSQL</a>  之间做选择，了解哪种类型的 NoSQL 数据库最适合你的用例也是非常有帮助的。我们将在下一节中快速了解下  <strong>键-值存储</strong>、<strong>文档型存储</strong>、<strong>列型存储</strong>和<strong>图存储</strong>数据库。</p><h4 id="键-值存储"><a href="#键-值存储" class="headerlink" title="键-值存储"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%94%AE-%E5%80%BC%E5%AD%98%E5%82%A8" target="_blank" rel="noopener"></a>键-值存储</h4><blockquote><p>抽象模型：哈希表</p></blockquote><p>键-值存储通常可以实现 O(1) 时间读写，用内存或 SSD 存储数据。数据存储可以按<a href="https://en.wikipedia.org/wiki/Lexicographical_order" target="_blank" rel="noopener">字典顺序</a>维护键，从而实现键的高效检索。键-值存储可以用于存储元数据。</p><p>键-值存储性能很高，通常用于存储简单数据模型或频繁修改的数据，如存放在内存中的缓存。键-值存储提供的操作有限，如果需要更多操作，复杂度将转嫁到应用程序层面。</p><p>键-值存储是如文档存储，在某些情况下，甚至是图存储等更复杂的存储系统的基础。</p><h4 id="来源及延伸阅读-11"><a href="#来源及延伸阅读-11" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-11" target="_blank" rel="noopener"></a>来源及延伸阅读</h4><ul><li><a href="https://en.wikipedia.org/wiki/Key-value_database" target="_blank" rel="noopener">键-值数据库</a></li><li><a href="http://stackoverflow.com/questions/4056093/what-are-the-disadvantages-of-using-a-key-value-table-over-nullable-columns-or" target="_blank" rel="noopener">键-值存储的劣势</a></li><li><a href="http://qnimate.com/overview-of-redis-architecture/" target="_blank" rel="noopener">Redis 架构</a></li><li><a href="https://www.adayinthelifeof.nl/2011/02/06/memcache-internals/" target="_blank" rel="noopener">Memcached 架构</a></li></ul><h4 id="文档类型存储"><a href="#文档类型存储" class="headerlink" title="文档类型存储"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%96%87%E6%A1%A3%E7%B1%BB%E5%9E%8B%E5%AD%98%E5%82%A8" target="_blank" rel="noopener"></a>文档类型存储</h4><blockquote><p>抽象模型：将文档作为值的键-值存储</p></blockquote><p>文档类型存储以文档（XML、JSON、二进制文件等）为中心，文档存储了指定对象的全部信息。文档存储根据文档自身的内部结构提供 API 或查询语句来实现查询。请注意，许多键-值存储数据库有用值存储元数据的特性，这也模糊了这两种存储类型的界限。</p><p>基于底层实现，文档可以根据集合、标签、元数据或者文件夹组织。尽管不同文档可以被组织在一起或者分成一组，但相互之间可能具有完全不同的字段。</p><p>MongoDB 和 CouchDB 等一些文档类型存储还提供了类似 SQL 语言的查询语句来实现复杂查询。DynamoDB 同时支持键-值存储和文档类型存储。</p><p>文档类型存储具备高度的灵活性，常用于处理偶尔变化的数据。</p><h4 id="来源及延伸阅读：文档类型存储"><a href="#来源及延伸阅读：文档类型存储" class="headerlink" title="来源及延伸阅读：文档类型存储"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%E6%96%87%E6%A1%A3%E7%B1%BB%E5%9E%8B%E5%AD%98%E5%82%A8" target="_blank" rel="noopener"></a>来源及延伸阅读：文档类型存储</h4><ul><li><a href="https://en.wikipedia.org/wiki/Document-oriented_database" target="_blank" rel="noopener">面向文档的数据库</a></li><li><a href="https://www.mongodb.com/mongodb-architecture" target="_blank" rel="noopener">MongoDB 架构</a></li><li><a href="https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/" target="_blank" rel="noopener">CouchDB 架构</a></li><li><a href="https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up" target="_blank" rel="noopener">Elasticsearch 架构</a></li></ul><h4 id="列型存储"><a href="#列型存储" class="headerlink" title="列型存储"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%97%E5%9E%8B%E5%AD%98%E5%82%A8" target="_blank" rel="noopener"></a>列型存储</h4><p><a href="https://camo.githubusercontent.com/823668b07b4bff50574e934273c9244e4e5017d6/687474703a2f2f692e696d6775722e636f6d2f6e3136694f476b2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/823668b07b4bff50574e934273c9244e4e5017d6/687474703a2f2f692e696d6775722e636f6d2f6e3136694f476b2e706e67" alt=""></a><br><strong><a href="http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html" target="_blank" rel="noopener">资料来源: SQL 和 NoSQL，一个简短的历史</a></strong></p><blockquote><p>抽象模型：嵌套的  <code>ColumnFamily&lt;RowKey, Columns&lt;ColKey, Value, Timestamp&gt;&gt;</code>  映射</p></blockquote><p>类型存储的基本数据单元是列（名／值对）。列可以在列族（类似于 SQL 的数据表）中被分组。超级列族再分组普通列族。你可以使用行键独立访问每一列，具有相同行键值的列组成一行。每个值都包含版本的时间戳用于解决版本冲突。</p><p>Google 发布了第一个列型存储数据库  <a href="http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf" target="_blank" rel="noopener">Bigtable</a>，它影响了 Hadoop 生态系统中活跃的开源数据库  <a href="https://www.mapr.com/blog/in-depth-look-hbase-architecture" target="_blank" rel="noopener">HBase</a>  和 Facebook 的  <a href="http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureIntro_c.html" target="_blank" rel="noopener">Cassandra</a>。像 BigTable，HBase 和 Cassandra 这样的存储系统将键以字母顺序存储，可以高效地读取键列。</p><p>列型存储具备高可用性和高可扩展性。通常被用于大数据相关存储。</p><h5 id="来源及延伸阅读：列型存储"><a href="#来源及延伸阅读：列型存储" class="headerlink" title="来源及延伸阅读：列型存储"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%E5%88%97%E5%9E%8B%E5%AD%98%E5%82%A8" target="_blank" rel="noopener"></a>来源及延伸阅读：列型存储</h5><ul><li><a href="http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html" target="_blank" rel="noopener">SQL 与 NoSQL 简史</a></li><li><a href="http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf" target="_blank" rel="noopener">BigTable 架构</a></li><li><a href="https://www.mapr.com/blog/in-depth-look-hbase-architecture" target="_blank" rel="noopener">Hbase 架构</a></li><li><a href="http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureIntro_c.html" target="_blank" rel="noopener">Cassandra 架构</a></li></ul><h4 id="图数据库"><a href="#图数据库" class="headerlink" title="图数据库"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener"></a>图数据库</h4><p><a href="https://camo.githubusercontent.com/bf6508b65e98a7210d9861515833afa0d9434436/687474703a2f2f692e696d6775722e636f6d2f664e636c3635672e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/bf6508b65e98a7210d9861515833afa0d9434436/687474703a2f2f692e696d6775722e636f6d2f664e636c3635672e706e67" alt=""></a><br><strong><a href="https://en.wikipedia.org/wiki/File:GraphDatabase_PropertyGraph.png" target="_blank" rel="noopener">资料来源：图数据库</a></strong></p><blockquote><p>抽象模型： 图</p></blockquote><p>在图数据库中，一个节点对应一条记录，一个弧对应两个节点之间的关系。图数据库被优化用于表示外键繁多的复杂关系或多对多关系。</p><p>图数据库为存储复杂关系的数据模型，如社交网络，提供了很高的性能。它们相对较新，尚未广泛应用，查找开发工具或者资源相对较难。许多图只能通过  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%A1%A8%E8%BF%B0%E6%80%A7%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BBrest" target="_blank" rel="noopener">REST API</a>  访问。</p><h5 id="相关资源和延伸阅读：图"><a href="#相关资源和延伸阅读：图" class="headerlink" title="相关资源和延伸阅读：图"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%E5%9B%BE" target="_blank" rel="noopener"></a>相关资源和延伸阅读：图</h5><ul><li><a href="https://en.wikipedia.org/wiki/Graph_database" target="_blank" rel="noopener">图数据库</a></li><li><a href="https://neo4j.com/" target="_blank" rel="noopener">Neo4j</a></li><li><a href="https://blog.twitter.com/2010/introducing-flockdb" target="_blank" rel="noopener">FlockDB</a></li></ul><h4 id="来源及延伸阅读：NoSQL"><a href="#来源及延伸阅读：NoSQL" class="headerlink" title="来源及延伸阅读：NoSQL"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BBnosql" target="_blank" rel="noopener"></a>来源及延伸阅读：NoSQL</h4><ul><li><a href="http://stackoverflow.com/questions/3342497/explanation-of-base-terminology" target="_blank" rel="noopener">数据库术语解释</a></li><li><a href="https://medium.com/baqend-blog/nosql-databases-a-survey-and-decision-guidance-ea7823a822d#.wskogqenq" target="_blank" rel="noopener">NoSQL 数据库 - 调查及决策指南</a></li><li><a href="http://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database" target="_blank" rel="noopener">可扩展性</a></li><li><a href="https://www.youtube.com/watch?v=qI_g07C_Q5I" target="_blank" rel="noopener">NoSQL 介绍</a></li><li><a href="http://horicky.blogspot.com/2009/11/nosql-patterns.html" target="_blank" rel="noopener">NoSQL 模式</a></li></ul><h3 id="SQL-还是-NoSQL"><a href="#SQL-还是-NoSQL" class="headerlink" title="SQL 还是 NoSQL"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#sql-%E8%BF%98%E6%98%AF-nosql" target="_blank" rel="noopener"></a>SQL 还是 NoSQL</h3><p><a href="https://camo.githubusercontent.com/a6e2e844765c9d5382d9c9b64ef7693977981646/687474703a2f2f692e696d6775722e636f6d2f775847714735662e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/a6e2e844765c9d5382d9c9b64ef7693977981646/687474703a2f2f692e696d6775722e636f6d2f775847714735662e706e67" alt=""></a><br><strong><a href="https://www.infoq.com/articles/Transition-RDBMS-NoSQL/" target="_blank" rel="noopener">资料来源：从 RDBMS 转换到 NoSQL</a></strong></p><p>选取  <strong>SQL</strong>  的原因:</p><ul><li>结构化数据</li><li>严格的模式</li><li>关系型数据</li><li>需要复杂的联结操作</li><li>事务</li><li>清晰的扩展模式</li><li>既有资源更丰富：开发者、社区、代码库、工具等</li><li>通过索引进行查询非常快</li></ul><p>选取  <strong>NoSQL</strong>  的原因：</p><ul><li>半结构化数据</li><li>动态或灵活的模式</li><li>非关系型数据</li><li>不需要复杂的联结操作</li><li>存储 TB （甚至 PB）级别的数据</li><li>高数据密集的工作负载</li><li>IOPS 高吞吐量</li></ul><p>适合 NoSQL 的示例数据：</p><ul><li>埋点数据和日志数据</li><li>排行榜或者得分数据</li><li>临时数据，如购物车</li><li>频繁访问的（“热”）表</li><li>元数据／查找表</li></ul><h5 id="来源及延伸阅读：SQL-或-NoSQL"><a href="#来源及延伸阅读：SQL-或-NoSQL" class="headerlink" title="来源及延伸阅读：SQL 或 NoSQL"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BBsql-%E6%88%96-nosql" target="_blank" rel="noopener"></a>来源及延伸阅读：SQL 或 NoSQL</h5><ul><li><a href="https://www.youtube.com/watch?v=w95murBkYmU" target="_blank" rel="noopener">扩展你的用户数到第一个千万</a></li><li><a href="https://www.sitepoint.com/sql-vs-nosql-differences/" target="_blank" rel="noopener">SQL 和 NoSQL 的不同</a></li></ul><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>缓存</h2><p><a href="https://camo.githubusercontent.com/7acedde6aa7853baf2eb4a53f88e2595ebe43756/687474703a2f2f692e696d6775722e636f6d2f51367a32344c612e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/7acedde6aa7853baf2eb4a53f88e2595ebe43756/687474703a2f2f692e696d6775722e636f6d2f51367a32344c612e706e67" alt=""></a><br><strong><a href="http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html" target="_blank" rel="noopener">资料来源：可扩展的系统设计模式</a></strong></p><p>缓存可以提高页面加载速度，并可以减少服务器和数据库的负载。在这个模型中，分发器先查看请求之前是否被响应过，如果有则将之前的结果直接返回，来省掉真正的处理。</p><p>数据库分片均匀分布的读取是最好的。但是热门数据会让读取分布不均匀，这样就会造成瓶颈，如果在数据库前加个缓存，就会抹平不均匀的负载和突发流量对数据库的影响。</p><h3 id="客户端缓存"><a href="#客户端缓存" class="headerlink" title="客户端缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>客户端缓存</h3><p>缓存可以位于客户端（操作系统或者浏览器），<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86web-%E6%9C%8D%E5%8A%A1%E5%99%A8" target="_blank" rel="noopener">服务端</a>或者不同的缓存层。</p><h3 id="CDN-缓存"><a href="#CDN-缓存" class="headerlink" title="CDN 缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#cdn-%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>CDN 缓存</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%86%85%E5%AE%B9%E5%88%86%E5%8F%91%E7%BD%91%E7%BB%9Ccdn" target="_blank" rel="noopener">CDN</a>  也被视为一种缓存。</p><h3 id="Web-服务器缓存"><a href="#Web-服务器缓存" class="headerlink" title="Web 服务器缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#web-%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>Web 服务器缓存</h3><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86web-%E6%9C%8D%E5%8A%A1%E5%99%A8" target="_blank" rel="noopener">反向代理</a>和缓存（比如  <a href="https://www.varnish-cache.org/" target="_blank" rel="noopener">Varnish</a>）可以直接提供静态和动态内容。Web 服务器同样也可以缓存请求，返回相应结果而不必连接应用服务器。</p><h3 id="数据库缓存"><a href="#数据库缓存" class="headerlink" title="数据库缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>数据库缓存</h3><p>数据库的默认配置中通常包含缓存级别，针对一般用例进行了优化。调整配置，在不同情况下使用不同的模式可以进一步提高性能。</p><h3 id="应用缓存"><a href="#应用缓存" class="headerlink" title="应用缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BA%94%E7%94%A8%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>应用缓存</h3><p>基于内存的缓存比如 Memcached 和 Redis 是应用程序和数据存储之间的一种键值存储。由于数据保存在 RAM 中，它比存储在磁盘上的典型数据库要快多了。RAM 比磁盘限制更多，所以例如  <a href="https://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used" target="_blank" rel="noopener">least recently used (LRU)</a>  的<a href="https://en.wikipedia.org/wiki/Cache_algorithms" target="_blank" rel="noopener">缓存无效算法</a>可以将「热门数据」放在 RAM 中，而对一些比较「冷门」的数据不做处理。</p><p>Redis 有下列附加功能：</p><ul><li>持久性选项</li><li>内置数据结构比如有序集合和列表</li></ul><p>有多个缓存级别，分为两大类：<strong>数据库查询</strong>和<strong>对象</strong>：</p><ul><li>行级别</li><li>查询级别</li><li>完整的可序列化对象</li><li>完全渲染的 HTML</li></ul><p>一般来说，你应该尽量避免基于文件的缓存，因为这使得复制和自动缩放很困难。</p><h3 id="数据库查询级别的缓存"><a href="#数据库查询级别的缓存" class="headerlink" title="数据库查询级别的缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E7%BA%A7%E5%88%AB%E7%9A%84%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>数据库查询级别的缓存</h3><p>当你查询数据库的时候，将查询语句的哈希值与查询结果存储到缓存中。这种方法会遇到以下问题：</p><ul><li>很难用复杂的查询删除已缓存结果。</li><li>如果一条数据比如表中某条数据的一项被改变，则需要删除所有可能包含已更改项的缓存结果。</li></ul><h3 id="对象级别的缓存"><a href="#对象级别的缓存" class="headerlink" title="对象级别的缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AF%B9%E8%B1%A1%E7%BA%A7%E5%88%AB%E7%9A%84%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>对象级别的缓存</h3><p>将您的数据视为对象，就像对待你的应用代码一样。让应用程序将数据从数据库中组合到类实例或数据结构中：</p><ul><li>如果对象的基础数据已经更改了，那么从缓存中删掉这个对象。</li><li>允许异步处理：workers 通过使用最新的缓存对象来组装对象。</li></ul><p>建议缓存的内容：</p><ul><li>用户会话</li><li>完全渲染的 Web 页面</li><li>活动流</li><li>用户图数据</li></ul><h3 id="何时更新缓存"><a href="#何时更新缓存" class="headerlink" title="何时更新缓存"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BD%95%E6%97%B6%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98" target="_blank" rel="noopener"></a>何时更新缓存</h3><p>由于你只能在缓存中存储有限的数据，所以你需要选择一个适用于你用例的缓存更新策略。</p><h4 id="缓存模式"><a href="#缓存模式" class="headerlink" title="缓存模式"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener"></a>缓存模式</h4><p><a href="https://camo.githubusercontent.com/7f5934e49a678b67f65e5ed53134bc258b007ebb/687474703a2f2f692e696d6775722e636f6d2f4f4e6a4f52716b2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/7f5934e49a678b67f65e5ed53134bc258b007ebb/687474703a2f2f692e696d6775722e636f6d2f4f4e6a4f52716b2e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast" target="_blank" rel="noopener">资料来源：从缓存到内存数据网格</a></strong></p><p>应用从存储器读写。缓存不和存储器直接交互，应用执行以下操作：</p><ul><li>在缓存中查找记录，如果所需数据不在缓存中</li><li>从数据库中加载所需内容</li><li>将查找到的结果存储到缓存中</li><li>返回所需内容</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def get_user(self, user_id):</span><br><span class="line">    user = cache.get(&quot;user.&#123;0&#125;&quot;, user_id)</span><br><span class="line">    if user is None:</span><br><span class="line">        user = db.query(&quot;SELECT * FROM users WHERE user_id = &#123;0&#125;&quot;, user_id)</span><br><span class="line">        if user is not None:</span><br><span class="line">            key = &quot;user.&#123;0&#125;&quot;.format(user_id)</span><br><span class="line">            cache.set(key, json.dumps(user))</span><br><span class="line">    return user</span><br></pre></td></tr></table></figure><p><a href="https://memcached.org/" target="_blank" rel="noopener">Memcached</a>  通常用这种方式使用。</p><p>添加到缓存中的数据读取速度很快。缓存模式也称为延迟加载。只缓存所请求的数据，这避免了没有被请求的数据占满了缓存空间。</p><h5 id="缓存的缺点："><a href="#缓存的缺点：" class="headerlink" title="缓存的缺点："></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98%E7%9A%84%E7%BC%BA%E7%82%B9" target="_blank" rel="noopener"></a>缓存的缺点：</h5><ul><li>请求的数据如果不在缓存中就需要经过三个步骤来获取数据，这会导致明显的延迟。</li><li>如果数据库中的数据更新了会导致缓存中的数据过时。这个问题需要通过设置� TTL 强制更新缓存或者直写模式来缓解这种情况。</li><li>当一个节点出现故障的时候，它将会被一个新的节点替代，这增加了延迟的时间。</li></ul><h4 id="直写模式"><a href="#直写模式" class="headerlink" title="直写模式"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B4%E5%86%99%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener"></a>直写模式</h4><p><a href="https://camo.githubusercontent.com/56b870f4d199335ccdbc98b989ef6511ed14f0e2/687474703a2f2f692e696d6775722e636f6d2f3076426330684e2e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/56b870f4d199335ccdbc98b989ef6511ed14f0e2/687474703a2f2f692e696d6775722e636f6d2f3076426330684e2e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">资料来源：可扩展性、可用性、稳定性、模式</a></strong></p><p>应用使用缓存作为主要的数据存储，将数据读写到缓存中，而缓存负责从数据库中读写数据。</p><ul><li>应用向缓存中添加/更新数据</li><li>缓存同步地写入数据存储</li><li>返回所需内容</li></ul><p>应用代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set_user(12345, &#123;&quot;foo&quot;:&quot;bar&quot;&#125;)</span><br></pre></td></tr></table></figure><p>缓存代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def set_user(user_id, values):</span><br><span class="line">    user = db.query(&quot;UPDATE Users WHERE id = &#123;0&#125;&quot;, user_id, values)</span><br><span class="line">    cache.set(user_id, user)</span><br></pre></td></tr></table></figure><p>由于存写操作所以直写模式整体是一种很慢的操作，但是读取刚写入的数据很快。相比读取数据，用户通常比较能接受更新数据时速度较慢。缓存中的数据不会过时。</p><h5 id="直写模式的缺点："><a href="#直写模式的缺点：" class="headerlink" title="直写模式的缺点："></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B4%E5%86%99%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%BC%BA%E7%82%B9" target="_blank" rel="noopener"></a>直写模式的缺点：</h5><ul><li>由于故障或者缩放而创建的新的节点，新的节点不会缓存，直到数据库更新为止。缓存应用直写模式可以缓解这个问题。</li><li>写入的大多数数据可能永远都不会被读取，用 TTL 可以最小化这种情况的出现。</li></ul><h4 id="回写模式"><a href="#回写模式" class="headerlink" title="回写模式"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%9E%E5%86%99%E6%A8%A1%E5%BC%8F" target="_blank" rel="noopener"></a>回写模式</h4><p><a href="https://camo.githubusercontent.com/8aa9f1a2f050c1422898bb5e82f1f01773334e22/687474703a2f2f692e696d6775722e636f6d2f72675372766a472e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/8aa9f1a2f050c1422898bb5e82f1f01773334e22/687474703a2f2f692e696d6775722e636f6d2f72675372766a472e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">资料来源：可扩展性、可用性、稳定性、模式</a></strong></p><p>在回写模式中，应用执行以下操作：</p><ul><li>在缓存中增加或者更新条目</li><li>异步写入数据，提高写入性能。</li></ul><h5 id="回写模式的缺点："><a href="#回写模式的缺点：" class="headerlink" title="回写模式的缺点："></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%9B%9E%E5%86%99%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%BC%BA%E7%82%B9" target="_blank" rel="noopener"></a>回写模式的缺点：</h5><ul><li>缓存可能在其内容成功存储之前丢失数据。</li><li>执行直写模式比缓存或者回写模式更复杂。</li></ul><h4 id="刷新"><a href="#刷新" class="headerlink" title="刷新"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%B7%E6%96%B0" target="_blank" rel="noopener"></a>刷新</h4><p><a href="https://camo.githubusercontent.com/49dcb54307763b4f56d61a4a1369826e2e7d52e4/687474703a2f2f692e696d6775722e636f6d2f6b78746a7167452e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/49dcb54307763b4f56d61a4a1369826e2e7d52e4/687474703a2f2f692e696d6775722e636f6d2f6b78746a7167452e706e67" alt=""></a><br><strong><a href="http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast" target="_blank" rel="noopener">资料来源：从缓存到内存数据网格</a></strong></p><p>你可以将缓存配置成在到期之前自动刷新最近访问过的内容。</p><p>如果缓存可以准确预测将来可能请求哪些数据，那么刷新可能会导致延迟与读取时间的降低。</p><h5 id="刷新的缺点："><a href="#刷新的缺点：" class="headerlink" title="刷新的缺点："></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%88%B7%E6%96%B0%E7%9A%84%E7%BC%BA%E7%82%B9" target="_blank" rel="noopener"></a>刷新的缺点：</h5><ul><li>不能准确预测到未来需要用到的数据可能会导致性能不如不使用刷新。</li></ul><h3 id="缓存的缺点：-1"><a href="#缓存的缺点：-1" class="headerlink" title="缓存的缺点："></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%93%E5%AD%98%E7%9A%84%E7%BC%BA%E7%82%B9-1" target="_blank" rel="noopener"></a>缓存的缺点：</h3><ul><li>需要保持缓存和真实数据源之间的一致性，比如数据库根据<a href="https://en.wikipedia.org/wiki/Cache_algorithms" target="_blank" rel="noopener">缓存无效</a>。</li><li>需要改变应用程序比如增加 Redis 或者 memcached。</li><li>无效缓存是个难题，什么时候更新缓存是与之相关的复杂问题。</li></ul><h3 id="相关资源和延伸阅读-1"><a href="#相关资源和延伸阅读-1" class="headerlink" title="相关资源和延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-1" target="_blank" rel="noopener"></a>相关资源和延伸阅读</h3><ul><li><a href="http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast" target="_blank" rel="noopener">从缓存到内存数据</a></li><li><a href="http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html" target="_blank" rel="noopener">可扩展系统设计模式</a></li><li><a href="http://lethain.com/introduction-to-architecting-systems-for-scale/" target="_blank" rel="noopener">可缩放系统构架介绍</a></li><li><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns/" target="_blank" rel="noopener">可扩展性，可用性，稳定性和模式</a></li><li><a href="http://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache" target="_blank" rel="noopener">可扩展性</a></li><li><a href="http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Strategies.html" target="_blank" rel="noopener">AWS ElastiCache 策略</a></li><li><a href="https://en.wikipedia.org/wiki/Cache_(computing" target="_blank" rel="noopener">维基百科</a>)</li></ul><h2 id="异步"><a href="#异步" class="headerlink" title="异步"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%82%E6%AD%A5" target="_blank" rel="noopener"></a>异步</h2><p><a href="https://camo.githubusercontent.com/c01ec137453216bbc188e3a8f16da39ec9131234/687474703a2f2f692e696d6775722e636f6d2f353447597353782e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/c01ec137453216bbc188e3a8f16da39ec9131234/687474703a2f2f692e696d6775722e636f6d2f353447597353782e706e67" alt=""></a><br><strong><a href="http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer" target="_blank" rel="noopener">资料来源：可缩放系统构架介绍</a></strong></p><p>异步工作流有助于减少那些原本顺序执行的请求时间。它们可以通过提前进行一些耗时的工作来帮助减少请求时间，比如定期汇总数据。</p><h3 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97" target="_blank" rel="noopener"></a>消息队列</h3><p>消息队列接收，保留和传递消息。如果按顺序执行操作太慢的话，你可以使用有以下工作流的消息队列：</p><ul><li>应用程序将作业发布到队列，然后通知用户作业状态</li><li>一个 worker 从队列中取出该作业，对其进行处理，然后显示该作业完成</li></ul><p>不去阻塞用户操作，作业在后台处理。在此期间，客户端可能会进行一些处理使得看上去像是任务已经完成了。例如，如果要发送一条推文，推文可能会马上出现在你的时间线上，但是可能需要一些时间才能将你的推文推送到你的所有关注者那里去。</p><p><strong>Redis</strong>  是一个令人满意的简单的消息代理，但是消息有可能会丢失。</p><p><strong>RabbitMQ</strong>  很受欢迎但是要求你适应「AMQP」协议并且管理你自己的节点。</p><p><strong>Amazon SQS</strong>  是被托管的，但可能具有高延迟，并且消息可能会被传送两次。</p><h3 id="任务队列"><a href="#任务队列" class="headerlink" title="任务队列"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BB%BB%E5%8A%A1%E9%98%9F%E5%88%97" target="_blank" rel="noopener"></a>任务队列</h3><p>任务队列接收任务及其相关数据，运行它们，然后传递其结果。 它们可以支持调度，并可用于在后台运行计算密集型作业。</p><p><strong>Celery</strong>  支持调度，主要是用 Python 开发的。</p><h3 id="背压"><a href="#背压" class="headerlink" title="背压"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%83%8C%E5%8E%8B" target="_blank" rel="noopener"></a>背压</h3><p>如果队列开始明显增长，那么队列大小可能会超过内存大小，导致高速缓存未命中，磁盘读取，甚至性能更慢。<a href="http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html" target="_blank" rel="noopener">背压</a>可以通过限制队列大小来帮助我们，从而为队列中的作业保持高吞吐率和良好的响应时间。一旦队列填满，客户端将得到服务器忙或者 HTTP 503 状态码，以便稍后重试。客户端可以在稍后时间重试该请求，也许是<a href="https://en.wikipedia.org/wiki/Exponential_backoff" target="_blank" rel="noopener">指数退避</a>。</p><h3 id="异步的缺点："><a href="#异步的缺点：" class="headerlink" title="异步的缺点："></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BC%82%E6%AD%A5%E7%9A%84%E7%BC%BA%E7%82%B9" target="_blank" rel="noopener"></a>异步的缺点：</h3><ul><li>简单的计算和实时工作流等用例可能更适用于同步操作，因为引入队列可能会增加延迟和复杂性。</li></ul><h3 id="相关资源和延伸阅读-2"><a href="#相关资源和延伸阅读-2" class="headerlink" title="相关资源和延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E5%92%8C%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-2" target="_blank" rel="noopener"></a>相关资源和延伸阅读</h3><ul><li><a href="https://www.youtube.com/watch?v=1KRYH75wgy4" target="_blank" rel="noopener">这是一个数字游戏</a></li><li><a href="http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html" target="_blank" rel="noopener">超载时应用背压</a></li><li><a href="https://en.wikipedia.org/wiki/Little%27s_law" target="_blank" rel="noopener">利特尔法则</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-a-message-queue-and-a-task-queue-Why-would-a-task-queue-require-a-message-broker-like-RabbitMQ-Redis-Celery-or-IronMQ-to-function" target="_blank" rel="noopener">消息队列与任务队列有什么区别？</a></li></ul><h2 id="通讯"><a href="#通讯" class="headerlink" title="通讯"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%80%9A%E8%AE%AF" target="_blank" rel="noopener"></a>通讯</h2><p><a href="https://camo.githubusercontent.com/1d761d5688d28ce1fb12a0f1c8191bca96eece4c/687474703a2f2f692e696d6775722e636f6d2f354b656f6351732e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1d761d5688d28ce1fb12a0f1c8191bca96eece4c/687474703a2f2f692e696d6775722e636f6d2f354b656f6351732e6a7067" alt=""></a><br><strong><a href="http://www.escotal.com/osilayer.html" target="_blank" rel="noopener">资料来源：OSI 7层模型</a></strong></p><h3 id="超文本传输协议（HTTP）"><a href="#超文本传输协议（HTTP）" class="headerlink" title="超文本传输协议（HTTP）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AEhttp" target="_blank" rel="noopener"></a>超文本传输协议（HTTP）</h3><p>HTTP 是一种在客户端和服务器之间编码和传输数据的方法。它是一个请求/响应协议：客户端和服务端针对相关内容和完成状态信息的请求和响应。HTTP 是独立的，允许请求和响应流经许多执行负载均衡，缓存，加密和压缩的中间路由器和服务器。</p><p>一个基本的 HTTP 请求由一个动词（方法）和一个资源（端点）组成。 以下是常见的 HTTP 动词：</p><table><thead><tr><th style="text-align:left">动词</th><th style="text-align:left">描述</th><th style="text-align:left">*幂等</th><th style="text-align:left">安全性</th><th style="text-align:left">可缓存</th></tr></thead><tbody><tr><td style="text-align:left">GET</td><td style="text-align:left">读取资源</td><td style="text-align:left">Yes</td><td style="text-align:left">Yes</td><td style="text-align:left">Yes</td></tr><tr><td style="text-align:left">POST</td><td style="text-align:left">创建资源或触发处理数据的进程</td><td style="text-align:left">No</td><td style="text-align:left">No</td><td style="text-align:left">Yes，如果回应包含刷新信息</td></tr><tr><td style="text-align:left">PUT</td><td style="text-align:left">创建或替换资源</td><td style="text-align:left">Yes</td><td style="text-align:left">No</td><td style="text-align:left">No</td></tr><tr><td style="text-align:left">PATCH</td><td style="text-align:left">部分更新资源</td><td style="text-align:left">No</td><td style="text-align:left">No</td><td style="text-align:left">Yes，如果回应包含刷新信息</td></tr><tr><td style="text-align:left">DELETE</td><td style="text-align:left">删除资源</td><td style="text-align:left">Yes</td><td style="text-align:left">No</td><td style="text-align:left">No</td></tr></tbody></table><p><strong>多次执行不会产生不同的结果</strong>。</p><p>HTTP 是依赖于较低级协议（如  <strong>TCP</strong>  和  <strong>UDP</strong>）的应用层协议。</p><h4 id="来源及延伸阅读：HTTP"><a href="#来源及延伸阅读：HTTP" class="headerlink" title="来源及延伸阅读：HTTP"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BBhttp" target="_blank" rel="noopener"></a>来源及延伸阅读：HTTP</h4><ul><li><a href="https://www.quora.com/What-is-the-difference-between-HTTP-protocol-and-TCP-protocol" target="_blank" rel="noopener">README</a>  +</li><li><a href="https://www.nginx.com/resources/glossary/http/" target="_blank" rel="noopener">HTTP 是什么？</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-HTTP-protocol-and-TCP-protocol" target="_blank" rel="noopener">HTTP 和 TCP 的区别</a></li><li><a href="https://laracasts.com/discuss/channels/general-discussion/whats-the-differences-between-put-and-patch?page=1" target="_blank" rel="noopener">PUT 和 PATCH的区别</a></li></ul><h3 id="传输控制协议（TCP）"><a href="#传输控制协议（TCP）" class="headerlink" title="传输控制协议（TCP）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AEtcp" target="_blank" rel="noopener"></a>传输控制协议（TCP）</h3><p><a href="https://camo.githubusercontent.com/821620cf6aa83566f4def561e754e5991480ca8d/687474703a2f2f692e696d6775722e636f6d2f4a6441736476472e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/821620cf6aa83566f4def561e754e5991480ca8d/687474703a2f2f692e696d6775722e636f6d2f4a6441736476472e6a7067" alt=""></a><br><strong><a href="http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/" target="_blank" rel="noopener">资料来源：如何制作多人游戏</a></strong></p><p>TCP 是通过  <a href="https://en.wikipedia.org/wiki/Internet_Protocol" target="_blank" rel="noopener">IP 网络</a>的面向连接的协议。 使用<a href="https://en.wikipedia.org/wiki/Handshaking" target="_blank" rel="noopener">握手</a>建立和断开连接。 发送的所有数据包保证以原始顺序到达目的地，用以下措施保证数据包不被损坏：</p><ul><li>每个数据包的序列号和<a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Checksum_computation" target="_blank" rel="noopener">校验码</a>。</li><li><a href="https://en.wikipedia.org/wiki/Acknowledgement_(data_networks" target="_blank" rel="noopener">确认包</a>)和自动重传</li></ul><p>如果发送者没有收到正确的响应，它将重新发送数据包。如果多次超时，连接就会断开。TCP 实行<a href="https://en.wikipedia.org/wiki/Flow_control_(data" target="_blank" rel="noopener">流量控制</a>)和<a href="https://en.wikipedia.org/wiki/Network_congestion#Congestion_control" target="_blank" rel="noopener">拥塞控制</a>。这些确保措施会导致延迟，而且通常导致传输效率比 UDP 低。</p><p>为了确保高吞吐量，Web 服务器可以保持大量的 TCP 连接，从而导致高内存使用。在 Web 服务器线程间拥有大量开放连接可能开销巨大，消耗资源过多，也就是说，一个  <a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#memcached" target="_blank" rel="noopener">memcached</a>  服务器。<a href="https://en.wikipedia.org/wiki/Connection_pool" target="_blank" rel="noopener">连接池</a>  可以帮助除了在适用的情况下切换到 UDP。</p><p>TCP 对于需要高可靠性但时间紧迫的应用程序很有用。比如包括 Web 服务器，数据库信息，SMTP，FTP 和 SSH。</p><p>以下情况使用 TCP 代替 UDP：</p><ul><li>你需要数据完好无损。</li><li>你想对网络吞吐量自动进行最佳评估。</li></ul><h3 id="用户数据报协议（UDP）"><a href="#用户数据报协议（UDP）" class="headerlink" title="用户数据报协议（UDP）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%94%A8%E6%88%B7%E6%95%B0%E6%8D%AE%E6%8A%A5%E5%8D%8F%E8%AE%AEudp" target="_blank" rel="noopener"></a>用户数据报协议（UDP）</h3><p><a href="https://camo.githubusercontent.com/47eb14c0a2dff2166f8781a6ce8c7f33d4c33da8/687474703a2f2f692e696d6775722e636f6d2f797a44724a74412e6a7067" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/47eb14c0a2dff2166f8781a6ce8c7f33d4c33da8/687474703a2f2f692e696d6775722e636f6d2f797a44724a74412e6a7067" alt=""></a><br><strong><a href="http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1" target="_blank" rel="noopener">资料来源：如何制作多人游戏</a></strong></p><p>UDP 是无连接的。数据报（类似于数据包）只在数据报级别有保证。数据报可能会无序的到达目的地，也有可能会遗失。UDP 不支持拥塞控制。虽然不如 TCP 那样有保证，但 UDP 通常效率更高。</p><p>UDP 可以通过广播将数据报发送至子网内的所有设备。这对  <a href="https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol" target="_blank" rel="noopener">DHCP</a>  很有用，因为子网内的设备还没有分配 IP 地址，而 IP 对于 TCP 是必须的。</p><p>UDP 可靠性更低但适合用在网络电话、视频聊天，流媒体和实时多人游戏上。</p><p>以下情况使用 UDP 代替 TCP：</p><ul><li>你需要低延迟</li><li>相对于数据丢失更糟的是数据延迟</li><li>你想实现自己的错误校正方法</li></ul><h4 id="来源及延伸阅读：TCP-与-UDP"><a href="#来源及延伸阅读：TCP-与-UDP" class="headerlink" title="来源及延伸阅读：TCP 与 UDP"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BBtcp-%E4%B8%8E-udp" target="_blank" rel="noopener"></a>来源及延伸阅读：TCP 与 UDP</h4><ul><li><a href="http://gafferongames.com/networking-for-game-programmers/udp-vs-tcp/" target="_blank" rel="noopener">游戏编程的网络</a></li><li><a href="http://www.cyberciti.biz/faq/key-differences-between-tcp-and-udp-protocols/" target="_blank" rel="noopener">TCP 与 UDP 的关键区别</a></li><li><a href="http://stackoverflow.com/questions/5970383/difference-between-tcp-and-udp" target="_blank" rel="noopener">TCP 与 UDP 的不同</a></li><li><a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol" target="_blank" rel="noopener">传输控制协议</a></li><li><a href="https://en.wikipedia.org/wiki/User_Datagram_Protocol" target="_blank" rel="noopener">用户数据报协议</a></li><li><a href="http://www.cs.bu.edu/~jappavoo/jappavoo.github.com/451/papers/memcache-fb.pdf" target="_blank" rel="noopener">Memcache 在 Facebook 的扩展</a></li></ul><h3 id="远程过程调用协议（RPC）"><a href="#远程过程调用协议（RPC）" class="headerlink" title="远程过程调用协议（RPC）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%BF%9C%E7%A8%8B%E8%BF%87%E7%A8%8B%E8%B0%83%E7%94%A8%E5%8D%8F%E8%AE%AErpc" target="_blank" rel="noopener"></a>远程过程调用协议（RPC）</h3><p><a href="https://camo.githubusercontent.com/1a3d7771c0b0a7816d0533fffeb6eeeb442d9945/687474703a2f2f692e696d6775722e636f6d2f6946344d6b62352e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/1a3d7771c0b0a7816d0533fffeb6eeeb442d9945/687474703a2f2f692e696d6775722e636f6d2f6946344d6b62352e706e67" alt=""></a><br><strong><a href="http://www.puncsky.com/blog/2016/02/14/crack-the-system-design-interview" target="_blank" rel="noopener">Source: Crack the system design interview</a></strong></p><p>在 RPC 中，客户端会去调用另一个地址空间（通常是一个远程服务器）里的方法。调用代码看起来就像是调用的是一个本地方法，客户端和服务器交互的具体过程被抽象。远程调用相对于本地调用一般较慢而且可靠性更差，因此区分两者是有帮助的。热门的 RPC 框架包括  <a href="https://developers.google.com/protocol-buffers/" target="_blank" rel="noopener">Protobuf</a>、<a href="https://thrift.apache.org/" target="_blank" rel="noopener">Thrift</a>  和  <a href="https://avro.apache.org/docs/current/" target="_blank" rel="noopener">Avro</a>。</p><p>RPC 是一个“请求-响应”协议：</p><ul><li><strong>客户端程序</strong>  ── 调用客户端存根程序。就像调用本地方法一样，参数会被压入栈中。</li><li><strong>客户端 stub 程序</strong>  ── 将请求过程的 id 和参数打包进请求信息中。</li><li><strong>客户端通信模块</strong>  ── 将信息从客户端发送至服务端。</li><li><strong>服务端通信模块</strong>  ── 将接受的包传给服务端存根程序。</li><li><strong>服务端 stub 程序</strong>  ── 将结果解包，依据过程 id 调用服务端方法并将参数传递过去。</li></ul><p>RPC 调用示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GET /someoperation?data=anId</span><br><span class="line"></span><br><span class="line">POST /anotheroperation</span><br><span class="line">&#123;</span><br><span class="line">  &quot;data&quot;:&quot;anId&quot;;</span><br><span class="line">  &quot;anotherdata&quot;: &quot;another value&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>RPC 专注于暴露方法。RPC 通常用于处理内部通讯的性能问题，这样你可以手动处理本地调用以更好的适应你的情况。</p><p>当以下情况时选择本地库（也就是 SDK）：</p><ul><li>你知道你的目标平台。</li><li>你想控制如何访问你的“逻辑”。</li><li>你想对发生在你的库中的错误进行控制。</li><li>性能和终端用户体验是你最关心的事。</li></ul><p>遵循  <strong>REST</strong>  的 HTTP API 往往更适用于公共 API。</p><h4 id="缺点：RPC"><a href="#缺点：RPC" class="headerlink" title="缺点：RPC"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E7%82%B9rpc" target="_blank" rel="noopener"></a>缺点：RPC</h4><ul><li>RPC 客户端与服务实现捆绑地很紧密。</li><li>一个新的 API 必须在每一个操作或者用例中定义。</li><li>RPC 很难调试。</li><li>你可能没办法很方便的去修改现有的技术。举个例子，如果你希望在  <a href="http://www.squid-cache.org/" target="_blank" rel="noopener">Squid</a>  这样的缓存服务器上确保  <a href="http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/" target="_blank" rel="noopener">RPC 被正确缓存</a>的话可能需要一些额外的努力了。</li></ul><h3 id="表述性状态转移（REST）"><a href="#表述性状态转移（REST）" class="headerlink" title="表述性状态转移（REST）"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%A1%A8%E8%BF%B0%E6%80%A7%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BBrest" target="_blank" rel="noopener"></a>表述性状态转移（REST）</h3><p>REST 是一种强制的客户端/服务端架构设计模型，客户端基于服务端管理的一系列资源操作。服务端提供修改或获取资源的接口。所有的通信必须是无状态和可缓存的。</p><p>RESTful 接口有四条规则：</p><ul><li><strong>标志资源（HTTP 里的 URI）</strong>  ── 无论什么操作都使用同一个 URI。</li><li><strong>表示的改变（HTTP 的动作）</strong>  ── 使用动作, headers 和 body。</li><li><strong>可自我描述的错误信息（HTTP 中的 status code）</strong>  ── 使用状态码，不要重新造轮子。</li><li><strong><a href="http://restcookbook.com/Basics/hateoas/" target="_blank" rel="noopener">HATEOAS</a>（HTTP 中的HTML 接口）</strong>  ── 你的 web 服务器应该能够通过浏览器访问。</li></ul><p>REST 请求的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GET /someresources/anId</span><br><span class="line"></span><br><span class="line">PUT /someresources/anId</span><br><span class="line">&#123;&quot;anotherdata&quot;: &quot;another value&quot;&#125;</span><br></pre></td></tr></table></figure><p>REST 关注于暴露数据。它减少了客户端／服务端的耦合程度，经常用于公共 HTTP API 接口设计。REST 使用更通常与规范化的方法来通过 URI 暴露资源，<a href="https://github.com/for-GET/know-your-http-well/blob/master/headers.md" target="_blank" rel="noopener">通过 header 来表述</a>并通过 GET、POST、PUT、DELETE 和 PATCH 这些动作来进行操作。因为无状态的特性，REST 易于横向扩展和隔离。</p><h4 id="缺点：REST"><a href="#缺点：REST" class="headerlink" title="缺点：REST"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%BC%BA%E7%82%B9rest" target="_blank" rel="noopener"></a>缺点：REST</h4><ul><li>由于 REST 将重点放在暴露数据，所以当资源不是自然组织的或者结构复杂的时候它可能无法很好的适应。举个例子，返回过去一小时中与特定事件集匹配的更新记录这种操作就很难表示为路径。使用 REST，可能会使用 URI 路径，查询参数和可能的请求体来实现。</li><li>REST 一般依赖几个动作（GET、POST、PUT、DELETE 和 PATCH），但有时候仅仅这些没法满足你的需要。举个例子，将过期的文档移动到归档文件夹里去，这样的操作可能没法简单的用上面这几个 verbs 表达。</li><li>为了渲染单个页面，获取被嵌套在层级结构中的复杂资源需要客户端，服务器之间多次往返通信。例如，获取博客内容及其关联评论。对于使用不确定网络环境的移动应用来说，这些多次往返通信是非常麻烦的。</li><li>随着时间的推移，更多的字段可能会被添加到 API 响应中，较旧的客户端将会接收到所有新的数据字段，即使是那些它们不需要的字段，结果它会增加负载大小并引起更大的延迟。</li></ul><h3 id="RPC-与-REST-比较"><a href="#RPC-与-REST-比较" class="headerlink" title="RPC 与 REST 比较"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#rpc-%E4%B8%8E-rest-%E6%AF%94%E8%BE%83" target="_blank" rel="noopener"></a>RPC 与 REST 比较</h3><table><thead><tr><th style="text-align:left">操作</th><th style="text-align:left">RPC</th><th style="text-align:left">REST</th></tr></thead><tbody><tr><td style="text-align:left">注册</td><td style="text-align:left"><strong>POST</strong>  /signup</td><td style="text-align:left"><strong>POST</strong>  /persons</td></tr><tr><td style="text-align:left">注销</td><td style="text-align:left"><strong>POST</strong>  /resign  <br>{  <br>“personid”: “1234”  <br>}</td><td style="text-align:left"><strong>DELETE</strong>  /persons/1234</td></tr><tr><td style="text-align:left">读取用户信息</td><td style="text-align:left"><strong>GET</strong>  /readPerson?personid=1234</td><td style="text-align:left"><strong>GET</strong>  /persons/1234</td></tr><tr><td style="text-align:left">读取用户物品列表</td><td style="text-align:left"><strong>GET</strong>  /readUsersItemsList?personid=1234</td><td style="text-align:left"><strong>GET</strong>  /persons/1234/items</td></tr><tr><td style="text-align:left">向用户物品列表添加一项</td><td style="text-align:left"><strong>POST</strong>  /addItemToUsersItemsList <br>{ <br>“personid”: “1234”;  <br>“itemid”: “456” <br>}</td><td style="text-align:left"><strong>POST</strong>  /persons/1234/items <br>{<br>“itemid”: “456”  <br>}</td></tr><tr><td style="text-align:left">更新一个物品</td><td style="text-align:left"><strong>POST</strong>  /modifyItem  <br>{  <br>“itemid”: “456”;  <br>“key”: “value”  <br>}</td><td style="text-align:left"><strong>PUT</strong>  /items/456  <br>{  <br>“key”: “value”  <br>}</td></tr><tr><td style="text-align:left">删除一个物品</td><td style="text-align:left"><strong>POST</strong>  /removeItem  <br>{  <br>“itemid”: “456”  <br>}</td><td style="text-align:left"><strong>DELETE</strong>  /items/456</td></tr></tbody></table><p><strong><a href="https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc" target="_blank" rel="noopener">资料来源：你真的知道你为什么更喜欢 REST 而不是 RPC 吗</a></strong></p><h4 id="来源及延伸阅读：REST-与-RPC"><a href="#来源及延伸阅读：REST-与-RPC" class="headerlink" title="来源及延伸阅读：REST 与 RPC"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BBrest-%E4%B8%8E-rpc" target="_blank" rel="noopener"></a>来源及延伸阅读：REST 与 RPC</h4><ul><li><a href="https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/" target="_blank" rel="noopener">你真的知道你为什么更喜欢 REST 而不是 RPC 吗</a></li><li><a href="http://programmers.stackexchange.com/a/181186" target="_blank" rel="noopener">什么时候 RPC 比 REST 更合适？</a></li><li><a href="http://stackoverflow.com/questions/15056878/rest-vs-json-rpc" target="_blank" rel="noopener">REST vs JSON-RPC</a></li><li><a href="http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/" target="_blank" rel="noopener">揭开 RPC 和 REST 的神秘面纱</a></li><li><a href="https://www.quora.com/What-are-the-drawbacks-of-using-RESTful-APIs" target="_blank" rel="noopener">使用 REST 的缺点是什么</a></li><li><a href="http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview" target="_blank" rel="noopener">破解系统设计面试</a></li><li><a href="https://code.facebook.com/posts/1468950976659943/" target="_blank" rel="noopener">Thrift</a></li><li><a href="http://arstechnica.com/civis/viewtopic.php?t=1190508" target="_blank" rel="noopener">为什么在内部使用 REST 而不是 RPC</a></li></ul><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%AE%89%E5%85%A8" target="_blank" rel="noopener"></a>安全</h2><p>这一部分需要更多内容。<a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%A1%E7%8C%AE" target="_blank" rel="noopener">一起来吧</a>！</p><p>安全是一个宽泛的话题。除非你有相当的经验、安全方面背景或者正在申请的职位要求安全知识，你不需要了解安全基础知识以外的内容：</p><ul><li>在运输和等待过程中加密</li><li>对所有的用户输入和从用户那里发来的参数进行处理以防止  <a href="https://en.wikipedia.org/wiki/Cross-site_scripting" target="_blank" rel="noopener">XSS</a>  和  <a href="https://en.wikipedia.org/wiki/SQL_injection" target="_blank" rel="noopener">SQL 注入</a>。</li><li>使用参数化的查询来防止 SQL 注入。</li><li>使用<a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege" target="_blank" rel="noopener">最小权限原则</a>。</li></ul><h3 id="来源及延伸阅读-12"><a href="#来源及延伸阅读-12" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-12" target="_blank" rel="noopener"></a>来源及延伸阅读</h3><ul><li><a href="https://github.com/FallibleInc/security-guide-for-developers" target="_blank" rel="noopener">为开发者准备的安全引导</a></li><li><a href="https://www.owasp.org/index.php/OWASP_Top_Ten_Cheat_Sheet" target="_blank" rel="noopener">OWASP top ten</a></li></ul><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E9%99%84%E5%BD%95" target="_blank" rel="noopener"></a>附录</h2><p>一些时候你会被要求做出保守估计。比如，你可能需要估计从磁盘中生成 100 张图片的缩略图需要的时间或者一个数据结构需要多少的内存。<strong>2 的次方表</strong>和<strong>每个开发者都需要知道的一些时间数据</strong>（译注：OSChina 上有这篇文章的<a href="https://www.oschina.net/news/30009/every-programmer-should-know" target="_blank" rel="noopener">译文</a>）都是一些很方便的参考资料。</p><h3 id="2-的次方表"><a href="#2-的次方表" class="headerlink" title="2 的次方表"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#2-%E7%9A%84%E6%AC%A1%E6%96%B9%E8%A1%A8" target="_blank" rel="noopener"></a>2 的次方表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Power           Exact Value         Approx Value        Bytes</span><br><span class="line">---------------------------------------------------------------</span><br><span class="line">7                             128</span><br><span class="line">8                             256</span><br><span class="line">10                           1024   1 thousand           1 KB</span><br><span class="line">16                         65,536                       64 KB</span><br><span class="line">20                      1,048,576   1 million            1 MB</span><br><span class="line">30                  1,073,741,824   1 billion            1 GB</span><br><span class="line">32                  4,294,967,296                        4 GB</span><br><span class="line">40              1,099,511,627,776   1 trillion           1 TB</span><br></pre></td></tr></table></figure><h4 id="来源及延伸阅读-13"><a href="#来源及延伸阅读-13" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-13" target="_blank" rel="noopener"></a>来源及延伸阅读</h4><ul><li><a href="https://en.wikipedia.org/wiki/Power_of_two" target="_blank" rel="noopener">2 的次方</a></li></ul><h3 id="每个程序员都应该知道的延迟数"><a href="#每个程序员都应该知道的延迟数" class="headerlink" title="每个程序员都应该知道的延迟数"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%AF%8F%E4%B8%AA%E7%A8%8B%E5%BA%8F%E5%91%98%E9%83%BD%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84%E5%BB%B6%E8%BF%9F%E6%95%B0" target="_blank" rel="noopener"></a>每个程序员都应该知道的延迟数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Latency Comparison Numbers</span><br><span class="line">--------------------------</span><br><span class="line">L1 cache reference                           0.5 ns</span><br><span class="line">Branch mispredict                            5   ns</span><br><span class="line">L2 cache reference                           7   ns                      14x L1 cache</span><br><span class="line">Mutex lock/unlock                          100   ns</span><br><span class="line">Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache</span><br><span class="line">Compress 1K bytes with Zippy            10,000   ns       10 us</span><br><span class="line">Send 1 KB bytes over 1 Gbps network     10,000   ns       10 us</span><br><span class="line">Read 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD</span><br><span class="line">Read 1 MB sequentially from memory     250,000   ns      250 us</span><br><span class="line">Round trip within same datacenter      500,000   ns      500 us</span><br><span class="line">Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory</span><br><span class="line">Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip</span><br><span class="line">Read 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD</span><br><span class="line">Read 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD</span><br><span class="line">Send packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms</span><br><span class="line"></span><br><span class="line">Notes</span><br><span class="line">-----</span><br><span class="line">1 ns = 10^-9 seconds</span><br><span class="line">1 us = 10^-6 seconds = 1,000 ns</span><br><span class="line">1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns</span><br></pre></td></tr></table></figure><p>基于上述数字的指标：</p><ul><li>从磁盘以 30 MB/s 的速度顺序读取</li><li>以 100 MB/s 从 1 Gbps 的以太网顺序读取</li><li>从 SSD 以 1 GB/s 的速度读取</li><li>以 4 GB/s 的速度从主存读取</li><li>每秒能绕地球 6-7 圈</li><li>数据中心内每秒有 2,000 次往返</li></ul><h4 id="延迟数可视化"><a href="#延迟数可视化" class="headerlink" title="延迟数可视化"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%BB%B6%E8%BF%9F%E6%95%B0%E5%8F%AF%E8%A7%86%E5%8C%96" target="_blank" rel="noopener"></a>延迟数可视化</h4><p><a href="https://camo.githubusercontent.com/77f72259e1eb58596b564d1ad823af1853bc60a3/687474703a2f2f692e696d6775722e636f6d2f6b307431652e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/77f72259e1eb58596b564d1ad823af1853bc60a3/687474703a2f2f692e696d6775722e636f6d2f6b307431652e706e67" alt=""></a></p><h4 id="来源及延伸阅读-14"><a href="#来源及延伸阅读-14" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-14" target="_blank" rel="noopener"></a>来源及延伸阅读</h4><ul><li><a href="https://gist.github.com/jboner/2841832" target="_blank" rel="noopener">每个程序员都应该知道的延迟数 — 1</a></li><li><a href="https://gist.github.com/hellerbarde/2843375" target="_blank" rel="noopener">每个程序员都应该知道的延迟数 — 2</a></li><li><a href="http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf" target="_blank" rel="noopener">关于建设大型分布式系统的的设计方案、课程和建议</a></li><li><a href="https://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf" target="_blank" rel="noopener">关于建设大型可拓展分布式系统的软件工程咨询</a></li></ul><h3 id="其它的系统设计面试题"><a href="#其它的系统设计面试题" class="headerlink" title="其它的系统设计面试题"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%B6%E5%AE%83%E7%9A%84%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98" target="_blank" rel="noopener"></a>其它的系统设计面试题</h3><blockquote><p>常见的系统设计面试问题，给出了如何解决的方案链接</p></blockquote><p>问题</p><p>引用</p><p>设计类似于 Dropbox 的文件同步服务</p><p><a href="https://www.youtube.com/watch?v=PE4gwstWhmc" target="_blank" rel="noopener">youtube.com</a></p><p>设计类似于 Google 的搜索引擎</p><p><a href="http://queue.acm.org/detail.cfm?id=988407" target="_blank" rel="noopener">queue.acm.org</a><br><a href="http://programmers.stackexchange.com/questions/38324/interview-question-how-would-you-implement-google-search" target="_blank" rel="noopener">stackexchange.com</a><br><a href="http://www.ardendertat.com/2012/01/11/implementing-search-engines/" target="_blank" rel="noopener">ardendertat.com</a><br><a href="http://infolab.stanford.edu/~backrub/google.html" target="_blank" rel="noopener">stanford.edu</a></p><p>设计类似于 Google 的可扩展网络爬虫</p><p><a href="https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch" target="_blank" rel="noopener">quora.com</a></p><p>设计 Google 文档</p><p><a href="https://code.google.com/p/google-mobwrite/" target="_blank" rel="noopener">code.google.com</a><br><a href="https://neil.fraser.name/writing/sync/" target="_blank" rel="noopener">neil.fraser.name</a></p><p>设计类似 Redis 的建值存储</p><p><a href="http://www.slideshare.net/dvirsky/introduction-to-redis" target="_blank" rel="noopener">slideshare.net</a></p><p>设计类似 Memcached 的缓存系统</p><p><a href="http://www.slideshare.net/oemebamo/introduction-to-memcached" target="_blank" rel="noopener">slideshare.net</a></p><p>设计类似亚马逊的推荐系统</p><p><a href="http://tech.hulu.com/blog/2011/09/19/recommendation-system.html" target="_blank" rel="noopener">hulu.com</a><br><a href="http://ijcai13.org/files/tutorial_slides/td3.pdf" target="_blank" rel="noopener">ijcai13.org</a></p><p>设计类似 Bitly 的短链接系统</p><p><a href="http://n00tc0d3r.blogspot.com/" target="_blank" rel="noopener">n00tc0d3r.blogspot.com</a></p><p>设计类似 WhatsApp 的聊天应用</p><p><a href="http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html" target="_blank" rel="noopener">highscalability.com</a></p><p>设计类似 Instagram 的图片分享系统</p><p><a href="http://highscalability.com/flickr-architecture" target="_blank" rel="noopener">highscalability.com</a><br><a href="http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html" target="_blank" rel="noopener">highscalability.com</a></p><p>设计 Facebook 的新闻推荐方法</p><p><a href="http://www.quora.com/What-are-best-practices-for-building-something-like-a-News-Feed" target="_blank" rel="noopener">quora.com</a><br><a href="http://www.quora.com/Activity-Streams/What-are-the-scaling-issues-to-keep-in-mind-while-developing-a-social-network-feed" target="_blank" rel="noopener">quora.com</a><br><a href="http://www.slideshare.net/danmckinley/etsy-activity-feeds-architecture" target="_blank" rel="noopener">slideshare.net</a></p><p>设计 Facebook 的时间线系统</p><p><a href="https://www.facebook.com/note.php?note_id=10150468255628920" target="_blank" rel="noopener">facebook.com</a><br><a href="http://highscalability.com/blog/2012/1/23/facebook-timeline-brought-to-you-by-the-power-of-denormaliza.html" target="_blank" rel="noopener">highscalability.com</a></p><p>设计 Facebook 的聊天系统</p><p><a href="http://www.erlang-factory.com/upload/presentations/31/EugeneLetuchy-ErlangatFacebook.pdf" target="_blank" rel="noopener">erlang-factory.com</a><br><a href="https://www.facebook.com/note.php?note_id=14218138919&amp;id=9445547199&amp;index=0" target="_blank" rel="noopener">facebook.com</a></p><p>设计类似 Facebook 的图表搜索系统</p><p><a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-out-the-infrastructure-for-graph-search/10151347573598920" target="_blank" rel="noopener">facebook.com</a><br><a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-indexing-and-ranking-in-graph-search/10151361720763920" target="_blank" rel="noopener">facebook.com</a><br><a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-the-natural-language-interface-of-graph-search/10151432733048920" target="_blank" rel="noopener">facebook.com</a></p><p>设计类似 CloudFlare 的内容传递网络</p><p><a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2112&amp;context=compsci" target="_blank" rel="noopener">cmu.edu</a></p><p>设计类似 Twitter 的热门话题系统</p><p><a href="http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/" target="_blank" rel="noopener">michael-noll.com</a><br><a href="http://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/" target="_blank" rel="noopener">snikolov .wordpress.com</a></p><p>设计一个随机 ID 生成系统</p><p><a href="https://blog.twitter.com/2010/announcing-snowflake" target="_blank" rel="noopener">blog.twitter.com</a><br><a href="https://github.com/twitter/snowflake/" target="_blank" rel="noopener">github.com</a></p><p>返回一定时间段内次数前 k 高的请求</p><p><a href="https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf" target="_blank" rel="noopener">ucsb.edu</a><br><a href="http://davis.wpi.edu/xmdv/docs/EDBT11-diyang.pdf" target="_blank" rel="noopener">wpi.edu</a></p><p>设计一个数据源于多个数据中心的服务系统</p><p><a href="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html" target="_blank" rel="noopener">highscalability.com</a></p><p>设计一个多人网络卡牌游戏</p><p><a href="http://www.indieflashblog.com/how-to-create-an-asynchronous-multiplayer-game.html" target="_blank" rel="noopener">indieflashblog.com</a><br><a href="http://buildnewgames.com/real-time-multiplayer/" target="_blank" rel="noopener">buildnewgames.com</a></p><p>设计一个垃圾回收系统</p><p><a href="http://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/" target="_blank" rel="noopener">stuffwithstuff.com</a><br><a href="http://courses.cs.washington.edu/courses/csep521/07wi/prj/rick.pdf" target="_blank" rel="noopener">washington.edu</a></p><p>添加更多的系统设计问题</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%A1%E7%8C%AE" target="_blank" rel="noopener">贡献</a></p><h3 id="真实架构"><a href="#真实架构" class="headerlink" title="真实架构"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%9C%9F%E5%AE%9E%E6%9E%B6%E6%9E%84" target="_blank" rel="noopener"></a>真实架构</h3><blockquote><p>关于现实中真实的系统是怎么设计的文章。</p></blockquote><p><a href="https://camo.githubusercontent.com/b7c71be73fb466344c2d773178ae74e3fbb1dcc6/687474703a2f2f692e696d6775722e636f6d2f5463556f3266772e706e67" target="_blank" rel="noopener"><img src="https://camo.githubusercontent.com/b7c71be73fb466344c2d773178ae74e3fbb1dcc6/687474703a2f2f692e696d6775722e636f6d2f5463556f3266772e706e67" alt=""></a><br><strong><a href="https://www.infoq.com/presentations/Twitter-Timeline-Scalability" target="_blank" rel="noopener">Source: Twitter timelines at scale</a></strong></p><p><strong>不要专注于以下文章的细节，专注于以下方面：</strong></p><ul><li>发现这些文章中的共同的原则、技术和模式。</li><li>学习每个组件解决哪些问题，什么情况下使用，什么情况下不适用</li><li>复习学过的文章</li></ul><p>类型</p><p>系统</p><p>引用</p><p>Data processing</p><p><strong>MapReduce</strong>  - Google的分布式数据处理</p><p><a href="http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf" target="_blank" rel="noopener">research.google.com</a></p><p>Data processing</p><p><strong>Spark</strong>  - Databricks 的分布式数据处理</p><p><a href="http://www.slideshare.net/AGrishchenko/apache-spark-architecture" target="_blank" rel="noopener">slideshare.net</a></p><p>Data processing</p><p><strong>Storm</strong>  - Twitter 的分布式数据处理</p><p><a href="http://www.slideshare.net/previa/storm-16094009" target="_blank" rel="noopener">slideshare.net</a></p><p>Data store</p><p><strong>Bigtable</strong>  - Google 的列式数据库</p><p><a href="http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf" target="_blank" rel="noopener">harvard.edu</a></p><p>Data store</p><p><strong>HBase</strong>  - Bigtable 的开源实现</p><p><a href="http://www.slideshare.net/alexbaranau/intro-to-hbase" target="_blank" rel="noopener">slideshare.net</a></p><p>Data store</p><p><strong>Cassandra</strong>  - Facebook 的列式数据库</p><p><a href="http://www.slideshare.net/planetcassandra/cassandra-introduction-features-30103666" target="_blank" rel="noopener">slideshare.net</a></p><p>Data store</p><p><strong>DynamoDB</strong>  - Amazon 的文档数据库</p><p><a href="http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf" target="_blank" rel="noopener">harvard.edu</a></p><p>Data store</p><p><strong>MongoDB</strong>  - 文档数据库</p><p><a href="http://www.slideshare.net/mdirolf/introduction-to-mongodb" target="_blank" rel="noopener">slideshare.net</a></p><p>Data store</p><p><strong>Spanner</strong>  - Google 的全球分布数据库</p><p><a href="http://research.google.com/archive/spanner-osdi2012.pdf" target="_blank" rel="noopener">research.google.com</a></p><p>Data store</p><p><strong>Memcached</strong>  - 分布式内存缓存系统</p><p><a href="http://www.slideshare.net/oemebamo/introduction-to-memcached" target="_blank" rel="noopener">slideshare.net</a></p><p>Data store</p><p><strong>Redis</strong>  - 能够持久化及具有值类型的分布式内存缓存系统</p><p><a href="http://www.slideshare.net/dvirsky/introduction-to-redis" target="_blank" rel="noopener">slideshare.net</a></p><p>File system</p><p><strong>Google File System (GFS)</strong>  - 分布式文件系统</p><p><a href="http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf" target="_blank" rel="noopener">research.google.com</a></p><p>File system</p><p><strong>Hadoop File System (HDFS)</strong>  - GFS 的开源实现</p><p><a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" target="_blank" rel="noopener">apache.org</a></p><p>Misc</p><p><strong>Chubby</strong>  - Google 的分布式系统的低耦合锁服务</p><p><a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/chubby-osdi06.pdf" target="_blank" rel="noopener">research.google.com</a></p><p>Misc</p><p><strong>Dapper</strong>  - 分布式系统跟踪基础设施</p><p><a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf" target="_blank" rel="noopener">research.google.com</a></p><p>Misc</p><p><strong>Kafka</strong>  - LinkedIn 的发布订阅消息系统</p><p><a href="http://www.slideshare.net/mumrah/kafka-talk-tri-hug" target="_blank" rel="noopener">slideshare.net</a></p><p>Misc</p><p><strong>Zookeeper</strong>  - 集中的基础架构和协调服务</p><p><a href="http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper" target="_blank" rel="noopener">slideshare.net</a></p><p>添加更多</p><p><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E8%B4%A1%E7%8C%AE" target="_blank" rel="noopener">贡献</a></p><h3 id="公司的系统架构"><a href="#公司的系统架构" class="headerlink" title="公司的系统架构"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%AC%E5%8F%B8%E7%9A%84%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84" target="_blank" rel="noopener"></a>公司的系统架构</h3><p>Company</p><p>Reference(s)</p><p>Amazon</p><p><a href="http://highscalability.com/amazon-architecture" target="_blank" rel="noopener">Amazon 的架构</a></p><p>Cinchcast</p><p><a href="http://highscalability.com/blog/2012/7/16/cinchcast-architecture-producing-1500-hours-of-audio-every-d.html" target="_blank" rel="noopener">每天产生 1500 小时的音频</a></p><p>DataSift</p><p><a href="http://highscalability.com/blog/2011/11/29/datasift-architecture-realtime-datamining-at-120000-tweets-p.html" target="_blank" rel="noopener">每秒实时挖掘 120000 条 tweet</a></p><p>DropBox</p><p><a href="https://www.youtube.com/watch?v=PE4gwstWhmc" target="_blank" rel="noopener">我们如何缩放 Dropbox</a></p><p>ESPN</p><p><a href="http://highscalability.com/blog/2013/11/4/espns-architecture-at-scale-operating-at-100000-duh-nuh-nuhs.html" target="_blank" rel="noopener">每秒操作 100000 次</a></p><p>Google</p><p><a href="http://highscalability.com/google-architecture" target="_blank" rel="noopener">Google 的架构</a></p><p>Instagram</p><p><a href="http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html" target="_blank" rel="noopener">1400 万用户，达到兆级别的照片存储</a><br><a href="http://instagram-engineering.tumblr.com/post/13649370142/what-powers-instagram-hundreds-of-instances" target="_blank" rel="noopener">是什么在驱动 Instagram</a></p><p>Justin.tv</p><p><a href="http://highscalability.com/blog/2010/3/16/justintvs-live-video-broadcasting-architecture.html" target="_blank" rel="noopener">Justin.Tv 的直播广播架构</a></p><p>Facebook</p><p><a href="https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/key-value/fb-memcached-nsdi-2013.pdf" target="_blank" rel="noopener">Facebook 的可扩展 memcached</a><br><a href="https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/data-store/tao-facebook-distributed-datastore-atc-2013.pdf" target="_blank" rel="noopener">TAO: Facebook 社交图的分布式数据存储</a><br><a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf" target="_blank" rel="noopener">Facebook 的图片存储</a></p><p>Flickr</p><p><a href="http://highscalability.com/flickr-architecture" target="_blank" rel="noopener">Flickr 的架构</a></p><p>Mailbox</p><p><a href="http://highscalability.com/blog/2013/6/18/scaling-mailbox-from-0-to-one-million-users-in-6-weeks-and-1.html" target="_blank" rel="noopener">在 6 周内从 0 到 100 万用户</a></p><p>Pinterest</p><p><a href="http://highscalability.com/blog/2013/4/15/scaling-pinterest-from-0-to-10s-of-billions-of-page-views-a.html" target="_blank" rel="noopener">从零到每月数十亿的浏览量</a><br><a href="http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html" target="_blank" rel="noopener">1800 万访问用户，10 倍增长，12 名员工</a></p><p>Playfish</p><p><a href="http://highscalability.com/blog/2010/9/21/playfishs-social-gaming-architecture-50-million-monthly-user.html" target="_blank" rel="noopener">月用户量 5000 万并在不断增长</a></p><p>PlentyOfFish</p><p><a href="http://highscalability.com/plentyoffish-architecture" target="_blank" rel="noopener">PlentyOfFish 的架构</a></p><p>Salesforce</p><p><a href="http://highscalability.com/blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html" target="_blank" rel="noopener">他们每天如何处理 13 亿笔交易</a></p><p>Stack Overflow</p><p><a href="http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html" target="_blank" rel="noopener">Stack Overflow 的架构</a></p><p>TripAdvisor</p><p><a href="http://highscalability.com/blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html" target="_blank" rel="noopener">40M 访问者，200M 页面浏览量，30TB 数据</a></p><p>Tumblr</p><p><a href="http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html" target="_blank" rel="noopener">每月 150 亿的浏览量</a></p><p>Twitter</p><p><a href="http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster" target="_blank" rel="noopener">Making Twitter 10000 percent faster</a><br><a href="http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html" target="_blank" rel="noopener">每天使用 MySQL 存储2.5亿条 tweet</a><br><a href="http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html" target="_blank" rel="noopener">150M 活跃用户，300K QPS，22 MB/S 的防火墙</a><br><a href="https://www.infoq.com/presentations/Twitter-Timeline-Scalability" target="_blank" rel="noopener">可扩展时间表</a><br><a href="https://www.youtube.com/watch?v=5cKTP36HVgI" target="_blank" rel="noopener">Twitter 的大小数据</a><br><a href="https://www.youtube.com/watch?v=z8LU0Cj6BOU" target="_blank" rel="noopener">Twitter 的行为：规模超过 1 亿用户</a></p><p>Uber</p><p><a href="http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html" target="_blank" rel="noopener">Uber 如何扩展自己的实时化市场</a></p><p>WhatsApp</p><p><a href="http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html" target="_blank" rel="noopener">Facebook 用 190 亿美元购买 WhatsApp 的架构</a></p><p>YouTube</p><p><a href="https://www.youtube.com/watch?v=w5WVu624fY8" target="_blank" rel="noopener">YouTube 的可扩展性</a><br><a href="http://highscalability.com/youtube-architecture" target="_blank" rel="noopener">YouTube 的架构</a></p><h3 id="公司工程博客"><a href="#公司工程博客" class="headerlink" title="公司工程博客"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E5%85%AC%E5%8F%B8%E5%B7%A5%E7%A8%8B%E5%8D%9A%E5%AE%A2" target="_blank" rel="noopener"></a>公司工程博客</h3><blockquote><p>你即将面试的公司的架构</p><p>你面对的问题可能就来自于同样领域</p></blockquote><ul><li><a href="http://nerds.airbnb.com/" target="_blank" rel="noopener">Airbnb Engineering</a></li><li><a href="https://developer.atlassian.com/blog/" target="_blank" rel="noopener">Atlassian Developers</a></li><li><a href="http://cloudengineering.autodesk.com/blog/" target="_blank" rel="noopener">Autodesk Engineering</a></li><li><a href="https://aws.amazon.com/blogs/aws/" target="_blank" rel="noopener">AWS Blog</a></li><li><a href="http://word.bitly.com/" target="_blank" rel="noopener">Bitly Engineering Blog</a></li><li><a href="https://www.box.com/blog/engineering/" target="_blank" rel="noopener">Box Blogs</a></li><li><a href="http://blog.cloudera.com/blog/" target="_blank" rel="noopener">Cloudera Developer Blog</a></li><li><a href="https://tech.dropbox.com/" target="_blank" rel="noopener">Dropbox Tech Blog</a></li><li><a href="http://engineering.quora.com/" target="_blank" rel="noopener">Engineering at Quora</a></li><li><a href="http://www.ebaytechblog.com/" target="_blank" rel="noopener">Ebay Tech Blog</a></li><li><a href="https://blog.evernote.com/tech/" target="_blank" rel="noopener">Evernote Tech Blog</a></li><li><a href="http://codeascraft.com/" target="_blank" rel="noopener">Etsy Code as Craft</a></li><li><a href="https://www.facebook.com/Engineering" target="_blank" rel="noopener">Facebook Engineering</a></li><li><a href="http://code.flickr.net/" target="_blank" rel="noopener">Flickr Code</a></li><li><a href="http://engineering.foursquare.com/" target="_blank" rel="noopener">Foursquare Engineering Blog</a></li><li><a href="http://githubengineering.com/" target="_blank" rel="noopener">GitHub Engineering Blog</a></li><li><a href="http://googleresearch.blogspot.com/" target="_blank" rel="noopener">Google Research Blog</a></li><li><a href="https://engineering.groupon.com/" target="_blank" rel="noopener">Groupon Engineering Blog</a></li><li><a href="https://engineering.heroku.com/" target="_blank" rel="noopener">Heroku Engineering Blog</a></li><li><a href="http://product.hubspot.com/blog/topic/engineering" target="_blank" rel="noopener">Hubspot Engineering Blog</a></li><li><a href="http://highscalability.com/" target="_blank" rel="noopener">High Scalability</a></li><li><a href="http://instagram-engineering.tumblr.com/" target="_blank" rel="noopener">Instagram Engineering</a></li><li><a href="https://software.intel.com/en-us/blogs/" target="_blank" rel="noopener">Intel Software Blog</a></li><li><a href="https://blogs.janestreet.com/category/ocaml/" target="_blank" rel="noopener">Jane Street Tech Blog</a></li><li><a href="http://engineering.linkedin.com/blog" target="_blank" rel="noopener">LinkedIn Engineering</a></li><li><a href="https://engineering.microsoft.com/" target="_blank" rel="noopener">Microsoft Engineering</a></li><li><a href="https://blogs.msdn.microsoft.com/pythonengineering/" target="_blank" rel="noopener">Microsoft Python Engineering</a></li><li><a href="http://techblog.netflix.com/" target="_blank" rel="noopener">Netflix Tech Blog</a></li><li><a href="https://devblog.paypal.com/category/engineering/" target="_blank" rel="noopener">Paypal Developer Blog</a></li><li><a href="http://engineering.pinterest.com/" target="_blank" rel="noopener">Pinterest Engineering Blog</a></li><li><a href="https://engineering.quora.com/" target="_blank" rel="noopener">Quora Engineering</a></li><li><a href="http://www.redditblog.com/" target="_blank" rel="noopener">Reddit Blog</a></li><li><a href="https://developer.salesforce.com/blogs/engineering/" target="_blank" rel="noopener">Salesforce Engineering Blog</a></li><li><a href="https://slack.engineering/" target="_blank" rel="noopener">Slack Engineering Blog</a></li><li><a href="https://labs.spotify.com/" target="_blank" rel="noopener">Spotify Labs</a></li><li><a href="http://www.twilio.com/engineering" target="_blank" rel="noopener">Twilio Engineering Blog</a></li><li><a href="https://engineering.twitter.com/" target="_blank" rel="noopener">Twitter Engineering</a></li><li><a href="http://eng.uber.com/" target="_blank" rel="noopener">Uber Engineering Blog</a></li><li><a href="http://yahooeng.tumblr.com/" target="_blank" rel="noopener">Yahoo Engineering Blog</a></li><li><a href="http://engineeringblog.yelp.com/" target="_blank" rel="noopener">Yelp Engineering Blog</a></li><li><a href="https://www.zynga.com/blogs/engineering" target="_blank" rel="noopener">Zynga Engineering Blog</a></li></ul><h4 id="来源及延伸阅读-15"><a href="#来源及延伸阅读-15" class="headerlink" title="来源及延伸阅读"></a><a href="https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E6%9D%A5%E6%BA%90%E5%8F%8A%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB-15" target="_blank" rel="noopener"></a>来源及延伸阅读</h4><ul><li><a href="https://github.com/kilimchoi/engineering-blogs" target="_blank" rel="noopener">kilimchoi/engineering-blogs</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 系统架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flutter的原理及美团的实践</title>
      <link href="/2018/08/13/Flutter%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E7%BE%8E%E5%9B%A2%E7%9A%84%E5%AE%9E%E8%B7%B5/"/>
      <url>/2018/08/13/Flutter%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E7%BE%8E%E5%9B%A2%E7%9A%84%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<p><strong>导读</strong></p><p>Flutter是Google开发的一套全新的跨平台、开源UI框架，支持iOS、Android系统开发，并且是未来新操作系统Fuchsia的默认开发套件。自从2017年5月发布<a href="https://github.com/flutter/flutter/releases/tag/v0.0.6" target="_blank" rel="noopener">第一个版本</a>以来，目前Flutter已经发布了近60个版本，并且在2018年5月发布了第一个<a href="https://developers.googleblog.com/2018/05/ready-for-production-apps-flutter-beta-3.html" target="_blank" rel="noopener">“Ready for Production Apps”</a>的Beta 3版本，6月20日发布了第一个<a href="https://medium.com/flutter-io/flutter-release-preview-1-943a9b6ee65a" target="_blank" rel="noopener">“Release Preview”</a>版本。</p><p><strong>初识Flutter</strong></p><p>Flutter的目标是使同一套代码同时运行在Android和iOS系统上，并且拥有媲美原生应用的性能，Flutter甚至提供了两套控件来适配Android和iOS（滚动效果、字体和控件图标等等），为了让App在细节处看起来更像原生应用。</p><p>在Flutter诞生之前，已经有许多跨平台UI框架的方案，比如基于WebView的Cordova、AppCan等，还有使用HTML+JavaScript渲染成原生控件的React Native、Weex等。<br><a id="more"></a><br>基于WebView的框架优点很明显，它们几乎可以完全继承现代Web开发的所有成果（丰富得多的控件库、满足各种需求的页面框架、完全的动态化、自动化测试工具等等），当然也包括Web开发人员，不需要太多的学习和迁移成本就可以开发一个App。同时WebView框架也有一个致命（在对体验&amp;性能有较高要求的情况下）的缺点，那就是WebView的渲染效率和JavaScript执行性能太差。再加上Android各个系统版本和设备厂商的定制，很难保证所在所有设备上都能提供一致的体验。</p><p>为了解决WebView性能差的问题，以React Native为代表的一类框架将最终渲染工作交还给了系统，虽然同样使用类HTML+JS的UI构建逻辑，但是最终会生成对应的自定义原生控件，以充分利用原生控件相对于WebView的较高的绘制效率。与此同时这种策略也将框架本身和App开发者绑在了系统的控件系统上，不仅框架本身需要处理大量平台相关的逻辑，随着系统版本变化和API的变化，开发者可能也需要处理不同平台的差异，甚至有些特性只能在部分平台上实现，这样框架的跨平台特性就会大打折扣。</p><p>Flutter则开辟了一种全新的思路，从头到尾重写一套跨平台的UI框架，包括UI控件、渲染逻辑甚至开发语言。渲染引擎依靠跨平台的Skia图形库来实现，依赖系统的只有图形绘制相关的接口，可以在最大程度上保证不同平台、不同设备的体验一致性，逻辑处理使用支持AOT的Dart语言，执行效率也比JavaScript高得多。</p><p>Flutter同时支持Windows、Linux和macOS操作系统作为开发环境，并且在Android Studio和VS Code两个IDE上都提供了全功能的支持。Flutter所使用的Dart语言同时支持AOT和JIT运行方式，JIT模式下还有一个备受欢迎的开发利器“热刷新”（Hot Reload），即在Android Studio中编辑Dart代码后，只需要点击保存或者“Hot Reload”按钮，就可以立即更新到正在运行的设备上，不需要重新编译App，甚至不需要重启App，立即就可以看到更新后的样式。</p><p>在Flutter中，所有功能都可以通过组合多个Widget来实现，包括对齐方式、按行排列、按列排列、网格排列甚至事件处理等等。Flutter控件主要分为两大类，StatelessWidget和StatefulWidget，StatelessWidget用来展示静态的文本或者图片，如果控件需要根据外部数据或者用户操作来改变的话，就需要使用StatefulWidget。State的概念也是来源于Facebook的流行Web框架<a href="http://facebook.github.io/react/" target="_blank" rel="noopener">React</a>，React风格的框架中使用控件树和各自的状态来构建界面，当某个控件的状态发生变化时由框架负责对比前后状态差异并且采取最小代价来更新渲染结果。</p><p><strong>Hot Reload</strong></p><p>在Dart代码文件中修改字符串“Hello, World”，添加一个惊叹号，点击保存或者热刷新按钮就可以立即更新到界面上，仅需几百毫秒：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSdX3Ctl85AibB7g9ibK5WBKIFyYKXHbTMWUduub5aeRoTm62P0CZiaZUJg/640?wx_fmt=gif" alt=""></p><p>Flutter通过将新的代码注入到正在运行的DartVM中，来实现Hot Reload这种神奇的效果，在DartVM将程序中的类结构更新完成后，Flutter会立即重建整个控件树，从而更新界面。但是热刷新也有一些限制，并不是所有的代码改动都可以通过热刷新来更新：  </p><ol><li><p>编译错误，如果修改后的Dart代码无法通过编译，Flutter会在控制台报错，这时需要修改对应的代码。</p></li><li><p>控件类型从<code>StatelessWidget</code>到<code>StatefulWidget</code>的转换，因为Flutter在执行热刷新时会保留程序原来的state，而某个控件从stageless→stateful后会导致Flutter重新创建控件时报错“myWidget is not a subtype of StatelessWidget”，而从stateful→stateless会报错“type ‘myWidget’ is not a subtype of type ‘StatefulWidget’ of ‘newWidget’”。</p></li><li><p>全局变量和静态成员变量，这些变量不会在热刷新时更新。</p></li><li><p>修改了main函数中创建的根控件节点，Flutter在热刷新后只会根据原来的根节点重新创建控件树，不会修改根节点。</p></li><li><p>某个类从普通类型转换成枚举类型，或者类型的泛型参数列表变化，都会使热刷新失败。</p></li></ol><p>热刷新无法实现更新时，执行一次热重启（Hot Restart）就可以全量更新所有代码，同样不需要重启App，区别是restart会将所有Dart代码打包同步到设备上，并且所有状态都会重置。</p><p><strong>Flutter插件</strong></p><p>Flutter使用的Dart语言无法直接调用Android系统提供的Java接口，这时就需要使用插件来实现中转。Flutter官方提供了丰富的原生接口封装：</p><ul><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/android_alarm_manager" target="_blank" rel="noopener">android_alarm_manager</a>，访问Android系统的<code>AlertManager</code>。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/android_intent" target="_blank" rel="noopener">android_intent</a>，构造Android的Intent对象。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/battery" target="_blank" rel="noopener">battery</a>，获取和监听系统电量变化。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/connectivity" target="_blank" rel="noopener">connectivity</a>，获取和监听系统网络连接状态。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/device_info" target="_blank" rel="noopener">device info</a>，获取设备型号等信息。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/image_picker" target="_blank" rel="noopener">image_picker</a>，从设备中选取或者拍摄照片。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/package_info" target="_blank" rel="noopener">package_info</a>，获取App安装包的版本等信息。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/path_provider" target="_blank" rel="noopener">path_provider</a>，获取常用文件路径。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/quick_actions" target="_blank" rel="noopener">quick_actions</a>，App图标添加快捷方式，iOS的<a href="https://developer.apple.com/ios/human-interface-guidelines/extensions/home-screen-actions" target="_blank" rel="noopener">eponymous concept</a>和Android的<a href="https://developer.android.com/guide/topics/ui/shortcuts.html" target="_blank" rel="noopener">App Shortcuts</a>。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/sensors" target="_blank" rel="noopener">sensors</a>，访问设备的加速度和陀螺仪传感器。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/shared_preferences" target="_blank" rel="noopener">shared_preferences</a>，App KV存储功能。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/url_launcher" target="_blank" rel="noopener">url_launcher</a>，启动URL，包括打电话、发短信和浏览网页等功能。</p></li><li><p><a href="https://github.com/flutter/plugins/blob/master/packages/video_player" target="_blank" rel="noopener">video_player</a>，播放视频文件或者网络流的控件。</p></li></ul><p>在Flutter中，依赖包由<a href="https://pub.dartlang.org/" target="_blank" rel="noopener">Pub</a>仓库管理，项目依赖配置在pubspec.yaml文件中声明即可（类似于NPM的版本声明<a href="https://www.dartlang.org/tools/pub/versioning" target="_blank" rel="noopener">Pub Versioning Philosophy</a>），对于未发布在Pub仓库的插件可以使用git仓库地址或文件路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dependencies:   </span><br><span class="line">  url_launcher: &quot;&gt;=0.1.2 &lt;0.2.0&quot;  </span><br><span class="line">  collection: &quot;^0.1.2&quot;  </span><br><span class="line">  plugin1:     </span><br><span class="line">    git:       </span><br><span class="line">      url: &quot;git://github.com/flutter/plugin1.git&quot;  </span><br><span class="line">  plugin2:     </span><br><span class="line">    path: ../plugin2/</span><br></pre></td></tr></table></figure><p>以shared_preferences为例，在pubspec中添加代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dependencies:  </span><br><span class="line">  flutter:    </span><br><span class="line">    sdk: flutter  </span><br><span class="line">  shared_preferences: &quot;^0.4.1&quot;</span><br></pre></td></tr></table></figure><p>脱字号“^”开头的版本表示<a href="https://www.dartlang.org/tools/pub/dependencies#caret-syntax" target="_blank" rel="noopener">和当前版本接口保持兼容</a>的最新版，<code>^1.2.3</code> 等效于 <code>&gt;=1.2.3 &lt;2.0.0</code> 而<code>^0.1.2</code> 等效于 <code>&gt;=0.1.2 &lt;0.2.0</code>，添加依赖后点击“Packages get”按钮即可下载插件到本地，在代码中添加import语句就可以使用插件提供的接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import &apos;package:shared_preferences/shared_preferences.Dart&apos;;</span><br><span class="line"></span><br><span class="line">class _MyAppState extends State&lt;MyAppCounter&gt; &#123;  </span><br><span class="line">  int _count = 0;  </span><br><span class="line">  static const String COUNTER_KEY = &apos;counter&apos;;  </span><br><span class="line">  </span><br><span class="line">  _MyAppState() &#123;    </span><br><span class="line">    init();  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  init() async &#123;    </span><br><span class="line">    var pref = await SharedPreferences.getInstance();    </span><br><span class="line">    _count = pref.getInt(COUNTER_KEY) ?? 0;    </span><br><span class="line">setState(() &#123;&#125;);  </span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">  increaseCounter() async &#123;    </span><br><span class="line">    SharedPreferences pref = await SharedPreferences.getInstance();    </span><br><span class="line">pref.setInt(COUNTER_KEY, ++_count);    </span><br><span class="line">setState(() &#123;&#125;);  </span><br><span class="line">  &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>Dart</strong></p><p><a href="https://www.dartlang.org/" target="_blank" rel="noopener">Dart</a>是一种强类型、跨平台的客户端开发语言。具有专门为客户端优化、高生产力、快速高效、可移植（兼容ARM/x86）、易学的OO编程风格和原生支持响应式编程（Stream &amp; Future）等优秀特性。Dart主要由Google负责开发和维护，在<a href="http://gotocon.com/aarhus-2011/presentation/Opening%20Keynote:%20Dart,%20a%20new%20programming%20language%20for%20structured%20web%20programming" target="_blank" rel="noopener">2011年10启动项目</a>，2017年9月发布第一个2.0-dev版本。</p><p><strong>Dart本身提供了三种运行方式：</strong></p><ol><li><p>使用Dart2js编译成JavaScript代码，运行在常规浏览器中（<a href="https://webdev.dartlang.org/" target="_blank" rel="noopener">Dart Web</a>）。</p></li><li><p>使用DartVM直接在命令行中运行Dart代码（<a href="https://www.dartlang.org/Dart-vm" target="_blank" rel="noopener">DartVM</a>）。</p></li><li><p>AOT方式编译成机器码，例如Flutter App框架（<a href="https://flutter.io/" target="_blank" rel="noopener">Flutter</a>）。</p></li></ol><p><strong>Flutter在筛选了20多种语言后，最终选择Dart作为开发语言主要有几个原因：</strong></p><ol><li><p>健全的类型系统，同时支持静态类型检查和运行时类型检查。</p></li><li><p>代码体积优化（Tree Shaking），编译时只保留运行时需要调用的代码（不允许反射这样的隐式引用），所以庞大的Widgets库不会造成发布体积过大。</p></li><li><p>丰富的底层库，Dart自身提供了非常多的库。</p></li><li><p>多生代无锁垃圾回收器，专门为UI框架中常见的大量Widgets对象创建和销毁优化。</p></li><li><p>跨平台，iOS和Android共用一套代码。</p></li><li><p>JIT &amp; AOT运行模式，支持开发时的快速迭代和正式发布后最大程度发挥硬件性能。</p></li></ol><p><strong>在Dart中，有一些重要的基本概念需要了解：</strong></p><ul><li><p>所有变量的值都是对象，也就是类的实例。甚至数字、函数和<code>null</code>也都是对象，都继承自<a href="https://api.dartlang.org/dev/Dart-core/Object-class.html" target="_blank" rel="noopener">Object</a>类。</p></li><li><p>虽然Dart是强类型语言，但是显式变量类型声明是可选的，Dart支持类型推断。如果不想使用类型推断，可以用<a href="https://www.dartlang.org/guides/language/effective-Dart/design#do-annotate-with-object-instead-of-dynamic-to-indicate-any-object-is-allowed" target="_blank" rel="noopener">dynamic</a>类型。</p></li><li><p>Dart支持泛型，<code>List</code>表示包含int类型的列表，<code>List</code>则表示包含任意类型的列表。</p></li><li><p>Dart支持顶层（top-level）函数和类成员函数，也支持嵌套函数和本地函数。</p></li><li><p>Dart支持顶层变量和类成员变量。</p></li><li><p>Dart没有public、protected和private这些关键字，使用下划线“_”开头的变量或者函数，表示只在库内可见。参考<a href="https://www.dartlang.org/guides/language/language-tour#libraries-and-visibility" target="_blank" rel="noopener">库和可见性</a>。</p></li></ul><p>DartVM的内存分配策略非常简单，创建对象时只需要在现有堆上移动指针，内存增长始终是线形的，省去了查找可用内存段的过程：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSIQN4hGnESAyQfYcdWMo4icWPibxnbBwgNGbVgsqWKiaYOWPiaPvPicuBJDA/640?wx_fmt=png" alt=""></p><p>Dart中类似线程的概念叫做Isolate，每个Isolate之间是无法共享内存的，所以这种分配策略可以让Dart实现无锁的快速分配。  </p><p>Dart的垃圾回收也采用了多生代算法，新生代在回收内存时采用了“半空间”算法，触发垃圾回收时Dart会将当前半空间中的“活跃”对象拷贝到备用空间，然后整体释放当前空间的所有内存：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSjcnoNuHzs65tRsaKibJTniciaZom4d0MArBynfKZ9WeuvlRNub9epueug/640?wx_fmt=png" alt=""></p><p>整个过程中Dart只需要操作少量的“活跃”对象，大量的没有引用的“死亡”对象则被忽略，这种算法也非常适合Flutter框架中大量Widget重建的场景。  </p><p><strong>Flutter Framework</strong></p><p>Flutter的框架部分完全使用Dart语言实现，并且有着清晰的分层架构。分层架构使得我们可以在调用Flutter提供的便捷开发功能（预定义的一套高质量Material控件）之外，还可以直接调用甚至修改每一层实现（因为整个框架都属于“用户空间”的代码），这给我们提供了最大程度的自定义能力。Framework底层是Flutter引擎，引擎主要负责图形绘制（Skia）、文字排版（libtxt）和提供Dart运行时，引擎全部使用C++实现，Framework层使我们可以用Dart语言调用引擎的强大能力。</p><p><strong>分层架构</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhS6zQPvgJFwr81D4RhZoKBFvicDg7gdWeEBOhnia00LKFicGUTlO8aCOTzw/640?wx_fmt=png" alt=""></p><p>Framework的最底层叫做Foundation，其中定义的大都是非常基础的、提供给其他所有层使用的工具类和方法。绘制库（Painting）封装了Flutter Engine提供的绘制接口，主要是为了在绘制控件等固定样式的图形时提供更直观、更方便的接口，比如绘制缩放后的位图、绘制文本、插值生成阴影以及在盒子周围绘制边框等等。</p><p>Animation是动画相关的类，提供了类似Android系统的ValueAnimator的功能，并且提供了丰富的内置插值器。Gesture提供了手势识别相关的功能，包括触摸事件类定义和多种内置的手势识别器。GestureBinding类是Flutter中处理手势的抽象服务类，继承自BindingBase类。</p><p>Binding系列的类在Flutter中充当着类似于Android中的SystemService系列（ActivityManager、PackageManager）功能，每个Binding类都提供一个服务的单例对象，App最顶层的Binding会包含所有相关的Bingding抽象类。如果使用Flutter提供的控件进行开发，则需要使用WidgetsFlutterBinding，如果不使用Flutter提供的任何控件，而直接调用Render层，则需要使用RenderingFlutterBinding。</p><p>Flutter本身支持Android和iOS两个平台，除了性能和开发语言上的“native”化之外，它还提供了两套设计语言的控件实现Material &amp; Cupertino，可以帮助App更好地在不同平台上提供原生的用户体验。</p><p><strong>渲染库（Rendering）</strong></p><p>Flutter的控件树在实际显示时会转换成对应的渲染对象（<code>RenderObject</code>）树来实现布局和绘制操作。一般情况下，我们只会在调试布局，或者需要使用自定义控件来实现某些特殊效果的时候，才需要考虑渲染对象树的细节。渲染库主要提供的功能类有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">abstract class RendererBinding extends BindingBase with ServicesBinding, SchedulerBinding, HitTestable &#123; ... &#125;</span><br><span class="line">abstract class RenderObject extends AbstractNode with DiagnosticableTreeMixin implements HitTestTarget &#123;</span><br><span class="line">abstract class RenderBox extends RenderObject &#123; ... &#125;</span><br><span class="line">class RenderParagraph extends RenderBox &#123; ... &#125;</span><br><span class="line">class RenderImage extends RenderBox &#123; ... &#125;</span><br><span class="line">class RenderFlex extends RenderBox with ContainerRenderObjectMixin&lt;RenderBox, FlexParentData&gt;,                                        </span><br><span class="line">        RenderBoxContainerDefaultsMixin&lt;RenderBox, FlexParentData&gt;,                                        </span><br><span class="line">        DebugOverflowIndicatorMixin &#123; ... &#125;</span><br></pre></td></tr></table></figure><p><code>RendererBinding</code>是渲染树和Flutter引擎的胶水层，负责管理帧重绘、窗口尺寸和渲染相关参数变化的监听。<code>RenderObject</code>渲染树中所有节点的基类，定义了布局、绘制和合成相关的接口。<code>RenderBox</code>和其三个常用的子类<code>RenderParagraph</code>、<code>RenderImage</code>、<code>RenderFlex</code>则是具体布局和绘制逻辑的实现类。</p><p>在Flutter界面渲染过程分为三个阶段：布局、绘制、合成，布局和绘制在Flutter框架中完成，合成则交由引擎负责：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSrEdmrPUIZ7VibOxRj7b6JiazHXC1pdjzYAibvFgt8qrlEKMeC3BFOXS0A/640?wx_fmt=png" alt=""></p><p>控件树中的每个控件通过实现<code>RenderObjectWidget#createRenderObject(BuildContext context) → RenderObject</code>方法来创建对应的不同类型的<code>RenderObject</code>对象，组成渲染对象树。因为Flutter极大地简化了布局的逻辑，所以整个布局过程中只需要深度遍历一次：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSABgdHx52vsa1ChuP0WclRnOPO6FOPQq0lmn43JPlDwo9N1kN7PT9rw/640?wx_fmt=png" alt=""></p><p>渲染对象树中的每个对象都会在布局过程中接受父对象的<code>Constraints</code>参数，决定自己的大小，然后父对象就可以按照自己的逻辑决定各个子对象的位置，完成布局过程。</p><p>子对象不存储自己在容器中的位置，所以在它的位置发生改变时并不需要重新布局或者绘制。子对象的位置信息存储在它自己的<code>parentData</code>字段中，但是该字段由它的父对象负责维护，自身并不关心该字段的内容。同时也因为这种简单的布局逻辑，Flutter可以在某些节点设置布局边界（Relayout boundary），即当边界内的任何对象发生重新布局时，不会影响边界外的对象，反之亦然：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSCViclibZZfeKBYVttE4aqcf1Ovmj75jCgYAF7ia1A84ZpEraoGTsx1Ugg/640?wx_fmt=png" alt=""></p><p>布局完成后，渲染对象树中的每个节点都有了明确的尺寸和位置，Flutter会把所有对象绘制到不同的图层上：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSEIGK7637y6zXXicL4jx0I3Rv8zSbCqtLSweAUViav331S2eZJOxJVxFA/640?wx_fmt=png" alt=""></p><p>因为绘制节点时也是深度遍历，可以看到第二个节点在绘制它的背景和前景不得不绘制在不同的图层上，因为第四个节点切换了图层（因为“4”节点是一个需要独占一个图层的内容，比如视频），而第六个节点也一起绘制到了红色图层。这样会导致第二个节点的前景（也就是“5”）部分需要重绘时，和它在逻辑上毫不相干但是处于同一图层的第六个节点也必须重绘。为了避免这种情况，Flutter提供了另外一个“重绘边界”的概念：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSLPdWTNKMeKwUiaIZExZiauJGKEhHD5PvgTnNCARJCypbOygEYLgsxE3g/640?wx_fmt=png" alt=""></p><p>在进入和走出重绘边界时，Flutter会强制切换新的图层，这样就可以避免边界内外的互相影响。典型的应用场景就是ScrollView，当滚动内容重绘时，一般情况下其他内容是不需要重绘的。虽然重绘边界可以在任何节点手动设置，但是一般不需要我们来实现，Flutter提供的控件默认会在需要设置的地方自动设置。  </p><p><strong>控件库（Widgets）</strong></p><p>Flutter的控件库提供了非常丰富的控件，包括最基本的文本、图片、容器、输入框和动画等等。在Flutter中“一切皆是控件”，通过组合、嵌套不同类型的控件，就可以构建出任意功能、任意复杂度的界面。它包含的最主要的几个类有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class WidgetsFlutterBinding extends BindingBase with GestureBinding, ServicesBinding, SchedulerBinding,            </span><br><span class="line">        PaintingBinding, RendererBinding, WidgetsBinding &#123; ... &#125;</span><br><span class="line">abstract class Widget extends DiagnosticableTree &#123; ... &#125;</span><br><span class="line">abstract class StatelessWidget extends Widget &#123; ... &#125;</span><br><span class="line">abstract class StatefulWidget extends Widget &#123; ... &#125;</span><br><span class="line">abstract class RenderObjectWidget extends Widget &#123; ... &#125;</span><br><span class="line">abstract class Element extends DiagnosticableTree implements BuildContext &#123; ... &#125;</span><br><span class="line">class StatelessElement extends ComponentElement &#123; ... &#125;</span><br><span class="line">class StatefulElement extends ComponentElement &#123; ... &#125;</span><br><span class="line">abstract class RenderObjectElement extends Element &#123; ... &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>基于Flutter控件系统开发的程序都需要使用<code>WidgetsFlutterBinding</code>，它是Flutter的控件框架和Flutter引擎的胶水层。<code>Widget</code>就是所有控件的基类，它本身所有的属性都是只读的。<code>RenderObjectWidget</code>所有的实现类则负责提供配置信息并创建具体的<code>RenderObjectElement</code>。<code>Element</code>是Flutter用来分离控件树和真正的渲染对象的中间层，控件用来描述对应的element属性，控件重建后可能会复用同一个element。<code>RenderObjectElement</code>持有真正负责布局、绘制和碰撞测试（hit test）的<code>RenderObject</code>对象。</p><p><code>StatelessWidget</code>和<code>StatefulWidget</code>并不会直接影响<code>RenderObject</code>创建，只负责创建对应的<code>RenderObjectWidget`</code>StatelessElement<code>和</code>StatefulElement`也是类似的功能。</p><p>它们之间的关系如下图：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhStLPRYI74DhPibRYjJviaialNxqJasvDDt0I7ibjZJQ4xibHDUZFr36y9AHQ/640?wx_fmt=png" alt=""></p><p>如果控件的属性发生了变化（因为控件的属性是只读的，所以变化也就意味着重新创建了新的控件树），但是其树上每个节点的类型没有变化时，element树和render树可以完全重用原来的对象（因为element和render object的属性都是可变的）：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSRia0Y5IrNZcWClOra0u3ib0wSH94lbqUWCLQZs0p3YzekpHRWTOaehJQ/640?wx_fmt=png" alt=""></p><p>但是，如果控件树种某个节点的类型发生了变化，则element树和render树中的对应节点也需要重新创建：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSfia5T7N1ZrsWJFB0TQqXnErBhFoUDTGuAfbRu7yLibTg10RnUBicUsSpA/640?wx_fmt=png" alt=""></p><p><strong>外卖全品类页面实践</strong>  </p><p>在调研了Flutter的各项特性和实现原理之后，外卖计划灰度上线Flutter版的全品类页面。对于将Flutter页面作为App的一部分这种集成模式，官方并没有提供<a href="https://github.com/flutter/flutter/wiki/Add-Flutter-to-existing-apps" target="_blank" rel="noopener">完善的支持</a>，所以我们首先需要了解Flutter是如何编译、打包并且运行起来的。</p><h4 id="Flutter-App构建过程"><a href="#Flutter-App构建过程" class="headerlink" title="Flutter App构建过程"></a><strong>Flutter App构建过程</strong></h4><p>最简单的Flutter工程至少包含两个文件：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSHBtc8B9B6icfoQsGiaTOP4nFBEuG5Dmcqy3ibGCYXjJ6kibGic66Xqvwklg/640?wx_fmt=png" alt=""></p><p>运行Flutter程序时需要对应平台的宿主工程，在Android上Flutter通过自动创建一个Gradle项目来生成宿主，在项目目录下执行<code>flutter create .</code>，Flutter会创建ios和android两个目录，分别构建对应平台的宿主项目，Android目录内容如下：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhS0eMw3WIECLnT9hhcO2lpLveeuQI4EFZPTrZeqkEFM0aL6IRbzacZYQ/640?wx_fmt=png" alt=""></p><p>此Gradle项目中只有一个app module，构建产物即是宿主APK。Flutter在本地运行时默认采用Debug模式，在项目目录执行<code>flutter run</code>即可安装到设备中并自动运行，Debug模式下Flutter使用JIT方式来执行Dart代码，所有的Dart代码都会打包到APK文件中assets目录下，由libflutter.so中提供的DartVM读取并执行：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSI6vtZAurX0G77pMEEl5rWC4RgiauCupGsZ4CoYBdUYuXia7WicpyaKCHQ/640?wx_fmt=png" alt=""></p><p>kernel_blob.bin是Flutter引擎的底层接口和Dart语言基本功能部分代码：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">third_party/dart/runtime/bin/*.dart</span><br><span class="line">third_party/dart/runtime/lib/*.dart</span><br><span class="line">third_party/dart/sdk/lib/_http/*.dart</span><br><span class="line">third_party/dart/sdk/lib/async/*.dart</span><br><span class="line">third_party/dart/sdk/lib/collection/*.dart</span><br><span class="line">third_party/dart/sdk/lib/convert/*.dart</span><br><span class="line">third_party/dart/sdk/lib/core/*.dart</span><br><span class="line">third_party/dart/sdk/lib/developer/*.dart</span><br><span class="line">third_party/dart/sdk/lib/html/*.dart</span><br><span class="line">third_party/dart/sdk/lib/internal/*.dart</span><br><span class="line">third_party/dart/sdk/lib/io/*.dart</span><br><span class="line">third_party/dart/sdk/lib/isolate/*.dart</span><br><span class="line">third_party/dart/sdk/lib/math/*.dart</span><br><span class="line">third_party/dart/sdk/lib/mirrors/*.dart</span><br><span class="line">third_party/dart/sdk/lib/profiler/*.dart</span><br><span class="line">third_party/dart/sdk/lib/typed_data/*.dart</span><br><span class="line">third_party/dart/sdk/lib/vmservice/*.dart</span><br><span class="line">flutter/lib/ui/*.dart</span><br></pre></td></tr></table></figure><p>platform.dill则是实现了页面逻辑的代码，也包括Flutter Framework和其他由pub依赖的库代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">flutter_tutorial_2/lib/main.dart</span><br><span class="line">flutter/packages/flutter/lib/src/widgets/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/services/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/semantics/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/scheduler/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/rendering/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/physics/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/painting/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/gestures/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/foundation/*.dart</span><br><span class="line">flutter/packages/flutter/lib/src/animation/*.dart</span><br><span class="line">.pub-cache/hosted/pub.flutter-io.cn/collection-1.14.6/lib/*.dart</span><br><span class="line">.pub-cache/hosted/pub.flutter-io.cn/meta-1.1.5/lib/*.dart</span><br><span class="line">.pub-cache/hosted/pub.flutter-io.cn/shared_preferences-0.4.2/*.dart</span><br></pre></td></tr></table></figure><p>kernel_blob.bin和platform.dill都是由flutter_tools中的<a href="https://github.com/flutter/flutter/blob/master/packages/flutter_tools/lib/src/bundle.dart" target="_blank" rel="noopener">bundle.dart</a>中调用<a href="https://github.com/flutter/flutter/blob/master/packages/flutter_tools/lib/src/compile.dart" target="_blank" rel="noopener">KernelCompiler</a>生成。</p><p>在Release模式（<code>flutter run --release</code>）下，Flutter会使用Dart的AOT运行模式，编译时将Dart代码转换成ARM指令：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSXknDOSoLvYHf72keXRXlI2JggX5LibIl9XcJaWHEeE2Fl822C234F0g/640?wx_fmt=png" alt=""></p><p>kernel_blob.bin和platform.dill都不在打包后的APK中，取代其功能的是(isolate/vm)<em>snapshot</em>(data/instr)四个文件。snapshot文件由Flutter SDK中的<code>flutter/bin/cache/artifacts/engine/android-arm-release/darwin-x64/gen_snapshot</code>命令生成，vm_snapshot_<em>是Dart虚拟机运行所需要的数据和代码指令，isolate_snapshot_</em>则是每个isolate运行所需要的数据和代码指令。  </p><p><strong>Flutter App运行机制</strong></p><p>Flutter构建出的APK在运行时会将所有assets目录下的资源文件解压到App私有文件目录中的flutter目录下，主要包括处理字符编码的icudtl.dat，还有Debug模式的kernel_blob.bin、platform.dill和Release模式下的4个snapshot文件。默认情况下Flutter在<code>Application#onCreate</code>时调用<code>FlutterMain#startInitialization</code>来启动解压任务，然后在<code>FlutterActivityDelegate#onCreate</code>中调用<code>FlutterMain#ensureInitializationComplete</code>来等待解压任务结束。</p><p>Flutter在Debug模式下使用JIT执行方式，主要是为了支持广受欢迎的热刷新功能：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhS72bvib5nKX8hIkSPhT2xVtw7yviaEGl118qdhbQaE1lSgqxygxNaWeVA/640?wx_fmt=png" alt=""></p><p>触发热刷新时Flutter会检测发生改变的Dart文件，将其同步到App私有缓存目录下，DartVM加载并且修改对应的类或者方法，重建控件树后立即可以在设备上看到效果。  </p><p>在Release模式下Flutter会直接将snapshot文件映射到内存中执行其中的指令：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSJ1zsGznGecBBianxBxHbGcJTSN31dOoFICZFklnJ12JjA17rzibB449Q/640?wx_fmt=png" alt=""></p><p>在Release模式下，<code>FlutterActivityDelegate#onCreate</code>中调用<code>FlutterMain#ensureInitializationComplete</code>方法中会将AndroidManifest中设置的snapshot（没有设置则使用上面提到的默认值）文件名等运行参数设置到对应的C++同名类对象中，构造<code>FlutterNativeView</code>实例时调用<code>nativeAttach</code>来初始化DartVM，运行编译好的Dart代码。</p><p><strong>打包Android Library</strong></p><p>了解Flutter项目的构建和运行机制后，我们就可以按照其需求打包成AAR然后集成到现有原生App中了。首先在andorid/app/build.gradle中修改：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhS0UPrHdbOdHEzH4dhECdeZibpNgKFXIYI2QqYTqFfzSYxibxgcC7bXvmA/640?wx_fmt=png" alt=""></p><p>简单修改后我们就可以使用Android Studio或者Gradle命令行工具将Flutter代码打包到aar中了。Flutter运行时所需要的资源都会包含在aar中，将其发布到maven服务器或者本地maven仓库后，就可以在原生App项目中引用。  </p><p>但这只是集成的第一步，为了让Flutter页面无缝衔接到外卖App中，我们需要做的还有很多。</p><h4 id="图片资源复用"><a href="#图片资源复用" class="headerlink" title="图片资源复用"></a><strong>图片资源复用</strong></h4><p>Flutter默认将所有的图片资源文件打包到assets目录下，但是我们并不是用Flutter开发全新的页面，图片资源原来都会按照Android的规范放在各个drawable目录，即使是全新的页面也会有很多图片资源复用的场景，所以在assets目录下新增图片资源并不合适。</p><p>Flutter官方并没有提供直接调用drawable目录下的图片资源的途径，毕竟drawable这类文件的处理会涉及大量的Android平台相关的逻辑（屏幕密度、系统版本、语言等等），assets目录文件的读取操作也在引擎内部使用C++实现，在Dart层面实现读取drawable文件的功能比较困难。Flutter在处理assets目录中的文件时也支持添加<a href="https://flutter.io/assets-and-images/#resolution-aware" target="_blank" rel="noopener">多倍率的图片资源</a>，并能够在使用时<a href="https://docs.flutter.io/flutter/widgets/Image/Image.asset.html" target="_blank" rel="noopener">自动选择</a>，但是Flutter要求每个图片必须提供1x图，然后才会识别到对应的其他倍率目录下的图片：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flutter:  </span><br><span class="line">  assets:    </span><br><span class="line">    - images/cat.png    </span><br><span class="line">- images/2x/cat.png    </span><br><span class="line">- images/3.5x/cat.png</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new Image.asset(&apos;images/cat.png&apos;);</span><br></pre></td></tr></table></figure><p>这样配置后，才能正确地在不同分辨率的设备上使用对应密度的图片。但是为了减小APK包体积我们的位图资源一般只提供常用的2x分辨率，其他分辨率的设备会在运行时自动缩放到对应大小。针对这种特殊的情况，我们在不增加包体积的前提下，同样提供了和原生App一样的能力：</p><ol><li><p>在调用Flutter页面之前将指定的图片资源按照设备屏幕密度缩放，并存储在App私有目录下。</p></li><li><p>Flutter中使用时通过自定义的<code>WMImage</code>控件来加载，实际是通过转换成FileImage并自动设置scale为devicePixelRatio来加载。</p></li></ol><p>这样就可以同时解决APK包大小和图片资源缺失1x图的问题。</p><p><strong>Flutter和原生代码的通信</strong></p><p>我们只用Flutter实现了一个页面，现有的大量逻辑都是用Java实现，在运行时会有许多场景必须使用原生应用中的逻辑和功能，例如网络请求，我们统一的网络库会在每个网络请求中添加许多通用参数，也会负责成功率等指标的监控，还有异常上报，我们需要在捕获到关键异常时将其堆栈和环境信息上报到服务器。这些功能不太可能立即使用Dart实现一套出来，所以我们需要使用Dart提供的Platform Channel功能来实现Dart→Java之间的互相调用。</p><p>以网络请求为例，我们在Dart中定义一个<code>MethodChannel</code>对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import &apos;dart:async&apos;;</span><br><span class="line">import &apos;package:flutter/services.dart&apos;;</span><br><span class="line">const MethodChannel _channel = const MethodChannel(&apos;com.sankuai.waimai/network&apos;);</span><br><span class="line">Future&lt;Map&lt;String, dynamic&gt;&gt; post(String path, [Map&lt;String, dynamic&gt; form]) async &#123;  </span><br><span class="line">  return _channel.invokeMethod(&quot;post&quot;, &#123;&apos;path&apos;: path, &apos;body&apos;: form&#125;).then((result) &#123;    </span><br><span class="line">    return new Map&lt;String, dynamic&gt;.from(result);  </span><br><span class="line">  &#125;).catchError((_) =&gt; null);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后在Java端实现相同名称的MethodChannel：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public class FlutterNetworkPlugin implements MethodChannel.MethodCallHandler &#123;    </span><br><span class="line">  private static final String CHANNEL_NAME = &quot;com.sankuai.waimai/network&quot;;    </span><br><span class="line">  </span><br><span class="line">  @Override    </span><br><span class="line">  public void onMethodCall(MethodCall methodCall, final MethodChannel.Result result) &#123;        </span><br><span class="line">    switch (methodCall.method) &#123;            </span><br><span class="line">  case &quot;post&quot;:                </span><br><span class="line">    RetrofitManager.performRequest(post((String) methodCall.argument(&quot;path&quot;), (Map) methodCall.argument(&quot;body&quot;)),                        </span><br><span class="line">  new DefaultSubscriber&lt;Map&gt;() &#123;                            </span><br><span class="line">    @Override                            </span><br><span class="line">public void onError(Throwable e) &#123;                                </span><br><span class="line">  result.error(e.getClass().getCanonicalName(), e.getMessage(), null);                            </span><br><span class="line">    &#125;                            </span><br><span class="line">    @Override                            </span><br><span class="line">    public void onNext(Map stringBaseResponse) &#123;                                </span><br><span class="line">      result.success(stringBaseResponse);                            </span><br><span class="line">    &#125;                        </span><br><span class="line">      &#125;, tag);                </span><br><span class="line">    break;            </span><br><span class="line">  default:                </span><br><span class="line">    result.notImplemented();                </span><br><span class="line">break;        </span><br><span class="line">&#125;    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Flutter页面中注册后，调用post方法就可以调用对应的Java实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">loadData: (callback) async &#123;    </span><br><span class="line">  Map&lt;String, dynamic&gt; data = await post(&quot;home/groups&quot;);    </span><br><span class="line">  if (data == null) &#123;      </span><br><span class="line">    callback(false);      </span><br><span class="line">return;    </span><br><span class="line">  &#125;    </span><br><span class="line">  _data = AllCategoryResponse.fromJson(data);    </span><br><span class="line">  if (_data == null || _data.code != 0) &#123;      </span><br><span class="line">    callback(false);      </span><br><span class="line">return;    </span><br><span class="line">  &#125;    </span><br><span class="line">  callback(true);  </span><br><span class="line">&#125;),</span><br></pre></td></tr></table></figure><p><strong>SO库兼容性</strong></p><p>Flutter官方只提供了四种CPU架构的SO库：armeabi-v7a、arm64-v8a、x86和x86-64，其中x86系列只支持Debug模式，但是外卖使用的大量SDK都只提供了armeabi架构的库。</p><p>虽然我们可以通过修改引擎<code>src</code>根目录和<code>third_party/dart</code>目录下<code>build/config/arm.gni</code>，<code>third_party/skia</code>目录下的<code>BUILD.gn</code>等配置文件来编译出armeabi版本的Flutter引擎，但是实际上市面上绝大部分设备都已经支持armeabi-v7a，其提供的硬件加速浮点运算指令可以大大提高Flutter的运行速度，在灰度阶段我们可以主动屏蔽掉不支持armeabi-v7a的设备，直接使用armeabi-v7a版本的引擎。</p><p>做到这点我们首先需要修改Flutter提供的引擎，在Flutter安装目录下的<code>bin/cache/artifacts/engine</code>下有Flutter下载的所有平台的引擎：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSFClL9s3EGtXPTNb7g28r3lzlccUFPspA5Nk5iau8icypSrialFYSmmOKA/640?wx_fmt=png" alt=""></p><p>我们只需要修改android-arm、android-arm-profile和android-arm-release下的flutter.jar，将其中的lib/armeabi-v7a/libflutter.so移动到lib/armeabi/libflutter.so即可：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd $FLUTTER_ROOT/bin/cache/artifacts/engine</span><br><span class="line">for arch in android-arm android-arm-profile android-arm-release; do  </span><br><span class="line">  pushd $arch  </span><br><span class="line">  cp flutter.jar flutter-armeabi-v7a.jar # 备份  </span><br><span class="line">  unzip flutter.jar lib/armeabi-v7a/libflutter.so  </span><br><span class="line">  mv lib/armeabi-v7a lib/armeabi  </span><br><span class="line">  zip -d flutter.jar lib/armeabi-v7a/libflutter.so  </span><br><span class="line">  zip flutter.jar lib/armeabi/libflutter.so  </span><br><span class="line">  popd</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>这样在打包后Flutter的SO库就会打到APK的lib/armeabi目录中。在运行时如果设备不支持armeabi-v7a可能会崩溃，所以我们需要主动识别并屏蔽掉这类设备，在Android上判断设备是否支持armeabi-v7a也很简单：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public static boolean isARMv7Compatible() &#123;    </span><br><span class="line">  try &#123;        </span><br><span class="line">    if (SDK_INT &gt;= LOLLIPOP) &#123;            </span><br><span class="line">  for (String abi : Build.SUPPORTED_32_BIT_ABIS) &#123;                </span><br><span class="line">    if (abi.equals(&quot;armeabi-v7a&quot;)) &#123;                    </span><br><span class="line">  return true;                </span><br><span class="line">&#125;            </span><br><span class="line">  &#125;        </span><br><span class="line">&#125; else &#123;            </span><br><span class="line">  if (CPU_ABI.equals(&quot;armeabi-v7a&quot;) || CPU_ABI.equals(&quot;arm64-v8a&quot;)) &#123;                </span><br><span class="line">    return true;            </span><br><span class="line">  &#125;        </span><br><span class="line">&#125;    </span><br><span class="line">  &#125; catch (Throwable e) &#123;        </span><br><span class="line">    L.wtf(e);    </span><br><span class="line">  &#125;    </span><br><span class="line">  return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>灰度和自动降级策略</strong></p><p>Horn是一个美团内部的跨平台配置下发SDK，使用Horn可以很方便地指定灰度开关：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSBMWVcjRaHSnicIfaSK0lBxmIhticsHibn5yrEpsxwf7Cz9o3UrPHHibbGA/640?wx_fmt=png" alt=""></p><p>在条件配置页面定义一系列条件，然后在参数配置页面添加新的字段flutter即可：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSlIh0npDcOaeTGEalVSwxwAxuge0fhyT8Lr1CRxjVXBUHF60WmP1Rhw/640?wx_fmt=png" alt=""></p><p>因为在客户端做了ABI兜底策略，所以这里定义的ABI规则并没有启用。  </p><p>Flutter目前仍然处于Beta阶段，灰度过程中难免发生崩溃现象，观察到崩溃后再针对机型或者设备ID来做降级虽然可以尽量降低影响，但是我们可以做到更迅速。外卖的Crash采集SDK同时也支持JNI Crash的收集，我们专门为Flutter注册了崩溃监听器，一旦采集到Flutter相关的JNI Crash就立即停止该设备的Flutter功能，启动Flutter之前会先判断<code>FLUTTER_NATIVE_CRASH_FLAG</code>文件是否存在，如果存在则表示该设备发生过Flutter相关的崩溃，很有可能是不兼容导致的问题，当前版本周期内在该设备上就不再使用Flutter功能。</p><p>除了崩溃以外，Flutter页面中的Dart代码也可能发生异常，例如服务器下发数据格式错误导致解析失败等等，Dart也提供了全局的异常捕获功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import &apos;package:wm_app/plugins/wm_metrics.dart&apos;;</span><br><span class="line"></span><br><span class="line">void main() &#123;  </span><br><span class="line">  runZoned(() =&gt; runApp(WaimaiApp()), onError: (Object obj, StackTrace stack) &#123;    </span><br><span class="line">    uploadException(&quot;$obj\n$stack&quot;);  </span><br><span class="line">  &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样我们就可以实现全方位的异常监控和完善的降级策略，最大程度减少灰度时可能对用户带来的影响。</p><p><strong>分析崩溃堆栈和异常数据</strong></p><p>Flutter的引擎部分全部使用C/C++实现，为了减少包大小，所有的SO库在发布时都会去除符号表信息。和其他的JNI崩溃堆栈一样，我们上报的堆栈信息中只能看到内存地址偏移量等信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***</span><br><span class="line">Build fingerprint: &apos;Rock/odin/odin:7.1.1/NMF26F/1527007828:user/dev-keys&apos;</span><br><span class="line">Revision: &apos;0&apos;</span><br><span class="line">Author: collect by &apos;libunwind&apos;</span><br><span class="line">ABI: &apos;arm64-v8a&apos;</span><br><span class="line">pid: 28937, tid: 29314, name: 1.ui  &gt;&gt;&gt; com.sankuai.meituan.takeoutnew &lt;&lt;&lt;</span><br><span class="line">ignal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0</span><br><span class="line"></span><br><span class="line">backtrace:    </span><br><span class="line">  r0 00000000  r1 ffffffff  r2 c0e7cb2c  r3 c15affcc    </span><br><span class="line">  r4 c15aff88  r5 c0e7cb2c  r6 c15aff90  r7 bf567800    </span><br><span class="line">  r8 c0e7cc58  r9 00000000  sl c15aff0c  fp 00000001    </span><br><span class="line">  ip 80000000  sp c0e7cb28  lr c11a03f9  pc c1254088  cpsr 200c0030    </span><br><span class="line">  #00 pc 002d7088  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #01 pc 002d5a23  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #02 pc 002d95b5  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #03 pc 002d9f33  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #04 pc 00068e6d  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #05 pc 00067da5  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #06 pc 00067d5f  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #07 pc 003b1877  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #08 pc 003b1db5  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so    </span><br><span class="line">  #09 pc 0000241c  /data/data/com.sankuai.meituan.takeoutnew/app_flutter/vm_snapshot_instr</span><br></pre></td></tr></table></figure><p>单纯这些信息很难定位问题，所以我们需要使用NDK提供的ndk-stack来解析出具体的代码位置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ndk-stack -sym PATH [-dump PATH]</span><br><span class="line">Symbolizes the stack trace from an Android native crash.  </span><br><span class="line">  -sym PATH   sets the root directory for symbols  </span><br><span class="line">  -dump PATH  sets the file containing the crash dump (default stdin)</span><br></pre></td></tr></table></figure><p>如果使用了定制过的引擎，必须使用<code>engine/src/out/android-release</code>下编译出的libflutter.so文件。一般情况下我们使用的是官方版本的引擎，可以在<a href="https://console.cloud.google.com/storage/browser/flutter_infra/flutter/" target="_blank" rel="noopener">flutter_infra</a>页面直接下载带有符号表的SO文件，根据打包时使用的Flutter工具版本下载对应的文件即可。比如0.4.4 beta版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ flutter --version # version命令可以看到Engine对应的版本 06afdfe54e</span><br><span class="line">Flutter 0.4.4 • channel beta • https://github.com/flutter/flutter.git</span><br><span class="line">Framework • revision f9bb4289e9 (5 weeks ago) • 2018-05-11 21:44:54 -0700</span><br><span class="line">Engine • revision 06afdfe54e</span><br><span class="line">Tools • Dart 2.0.0-dev.54.0.flutter-46ab040e58</span><br><span class="line">$ cat flutter/bin/internal/engine.version # flutter安装目录下的engine.version文件也可以看到完整的版本信息 06afdfe54ebef9168a90ca00a6721c2d36e6aafa</span><br><span class="line">06afdfe54ebef9168a90ca00a6721c2d36e6aafa</span><br></pre></td></tr></table></figure><p>拿到引擎版本号后在<a href="https://console.cloud.google.com/storage/browser/flutter_infra/flutter/06afdfe54ebef9168a90ca00a6721c2d36e6aafa/" target="_blank" rel="noopener">https://console.cloud.google.com/storage/browser/flutter_infra/flutter/06afdfe54ebef9168a90ca00a6721c2d36e6aafa/</a> 看到该版本对应的所有构建产物，下载android-arm-release、android-arm64-release和android-x86目录下的symbols.zip，并存放到对应目录：</p><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt=""></p><p>执行ndk-stack即可看到实际发生崩溃的代码和具体行数信息：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ndk-stack -sym flutter-production-syms/06afdfe54ebef9168a90ca00a6721c2d36e6aafa/armeabi-v7a -dump flutter_jni_crash.txt </span><br><span class="line">********** Crash dump: **********</span><br><span class="line">Build fingerprint: &apos;Rock/odin/odin:7.1.1/NMF26F/1527007828:user/dev-keys&apos;</span><br><span class="line">pid: 28937, tid: 29314, name: 1.ui  &gt;&gt;&gt; com.sankuai.meituan.takeoutnew &lt;&lt;&lt;</span><br><span class="line">signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0</span><br><span class="line">Stack frame #00 pc 002d7088  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine minikin::WordBreaker::setText(unsigned short const*, unsigned int) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../flutter/third_party/txt/src/minikin/WordBreaker.cpp:55</span><br><span class="line">Stack frame #01 pc 002d5a23  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine minikin::LineBreaker::setText() at /b/build/slave/Linux_Engine/build/src/out/android_release/../../flutter/third_party/txt/src/minikin/LineBreaker.cpp:74</span><br><span class="line">Stack frame #02 pc 002d95b5  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine txt::Paragraph::ComputeLineBreaks() at /b/build/slave/Linux_Engine/build/src/out/android_release/../../flutter/third_party/txt/src/txt/paragraph.cc:273</span><br><span class="line">Stack frame #03 pc 002d9f33  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine txt::Paragraph::Layout(double, bool) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../flutter/third_party/txt/src/txt/paragraph.cc:428</span><br><span class="line">Stack frame #04 pc 00068e6d  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine blink::ParagraphImplTxt::layout(double) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../flutter/lib/ui/text/paragraph_impl_txt.cc:54</span><br><span class="line">Stack frame #05 pc 00067da5  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine tonic::DartDispatcher&lt;tonic::IndicesHolder&lt;0u&gt;, void (blink::Paragraph::*)(double)&gt;::Dispatch(void (blink::Paragraph::*)(double)) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../topaz/lib/tonic/dart_args.h:150</span><br><span class="line">Stack frame #06 pc 00067d5f  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine void tonic::DartCall&lt;void (blink::Paragraph::*)(double)&gt;(void (blink::Paragraph::*)(double), _Dart_NativeArguments*) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../topaz/lib/tonic/dart_args.h:198</span><br><span class="line">Stack frame #07 pc 003b1877  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine dart::NativeEntry::AutoScopeNativeCallWrapperNoStackCheck(_Dart_NativeArguments*, void (*)(_Dart_NativeArguments*)) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../third_party/dart/runtime/vm/native_entry.cc:198</span><br><span class="line">Stack frame #08 pc 003b1db5  /data/app/com.sankuai.meituan.takeoutnew-1/lib/arm/libflutter.so: Routine dart::NativeEntry::LinkNativeCall(_Dart_NativeArguments*) at /b/build/slave/Linux_Engine/build/src/out/android_release/../../third_party/dart/runtime/vm/native_entry.cc:348</span><br><span class="line">Stack frame #09 pc 0000241c  /data/data/com.sankuai.meituan.takeoutnew/app_flutter/vm_snapshot_instr</span><br></pre></td></tr></table></figure><p>Dart异常则比较简单，默认情况下Dart代码在编译成机器码时并没有去除符号表信息，所以Dart的异常堆栈本身就可以标识真实发生异常的代码文件和行数信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">FlutterException: type &apos;_InternalLinkedHashMap&lt;dynamic, dynamic&gt;&apos; is not a subtype of type &apos;num&apos; in type cast</span><br><span class="line">#0      _$CategoryGroupFromJson (package:wm_app/lib/all_category/model/category_model.g.dart:29)</span><br><span class="line">#1      new CategoryGroup.fromJson (package:wm_app/all_category/model/category_model.dart:51)</span><br><span class="line">#2      _$CategoryListDataFromJson.&lt;anonymous closure&gt; (package:wm_app/lib/all_category/model/category_model.g.dart:5)</span><br><span class="line">#3      MappedListIterable.elementAt (dart:_internal/iterable.dart:414)</span><br><span class="line">#4      ListIterable.toList (dart:_internal/iterable.dart:219)</span><br><span class="line">#5      _$CategoryListDataFromJson (package:wm_app/lib/all_category/model/category_model.g.dart:6)</span><br><span class="line">#6      new CategoryListData.fromJson (package:wm_app/all_category/model/category_model.dart:19)</span><br><span class="line">#7      _$AllCategoryResponseFromJson (package:wm_app/lib/all_category/model/category_model.g.dart:19)</span><br><span class="line">#8      new AllCategoryResponse.fromJson (package:wm_app/all_category/model/category_model.dart:29)</span><br><span class="line">#9      AllCategoryPage.build.&lt;anonymous closure&gt; (package:wm_app/all_category/category_page.dart:46)&lt;asynchronous suspension&gt;</span><br><span class="line">#10     _WaimaiLoadingState.build (package:wm_app/all_category/widgets/progressive_loading_page.dart:51)</span><br><span class="line">#11     StatefulElement.build (package:flutter/src/widgets/framework.dart:3730)</span><br><span class="line">#12     ComponentElement.performRebuild (package:flutter/src/widgets/framework.dart:3642)</span><br><span class="line">#13     Element.rebuild (package:flutter/src/widgets/framework.dart:3495)</span><br><span class="line">#14     BuildOwner.buildScope (package:flutter/src/widgets/framework.dart:2242)</span><br><span class="line">#15     _WidgetsFlutterBinding&amp;BindingBase&amp;GestureBinding&amp;ServicesBinding&amp;SchedulerBinding&amp;PaintingBinding&amp;RendererBinding&amp;WidgetsBinding.drawFrame (package:flutter/src/widgets/binding.dart:626)</span><br><span class="line">#16     _WidgetsFlutterBinding&amp;BindingBase&amp;GestureBinding&amp;ServicesBinding&amp;SchedulerBinding&amp;PaintingBinding&amp;RendererBinding._handlePersistentFrameCallback (package:flutter/src/rendering/binding.dart:208)</span><br><span class="line">#17     _WidgetsFlutterBinding&amp;BindingBase&amp;GestureBinding&amp;ServicesBinding&amp;SchedulerBinding._invokeFrameCallback (package:flutter/src/scheduler/binding.dart:990)</span><br><span class="line">#18     _WidgetsFlutterBinding&amp;BindingBase&amp;GestureBinding&amp;ServicesBinding&amp;SchedulerBinding.handleDrawFrame (package:flutter/src/scheduler/binding.dart:930)</span><br><span class="line">#19     _WidgetsFlutterBinding&amp;BindingBase&amp;GestureBinding&amp;ServicesBinding&amp;SchedulerBinding._handleDrawFrame (package:flutter/src/scheduler/binding.dart:842)</span><br><span class="line">#20     _rootRun (dart:async/zone.dart:1126)</span><br><span class="line">#21     _CustomZone.run (dart:async/zone.dart:1023)</span><br><span class="line">#22     _CustomZone.runGuarded (dart:async/zone.dart:925)</span><br><span class="line">#23     _invoke (dart:ui/hooks.dart:122)</span><br><span class="line">#24     _drawFrame (dart:ui/hooks.dart:109)</span><br></pre></td></tr></table></figure><p><strong>Flutter和原生性能对比</strong></p><p>虽然使用原生实现（左）和Flutter实现（右）的全品类页面在实际使用过程中几乎分辨不出来：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSPiclFbPB7VrPaadiaiag5OTBYoZ0D57MJZpVPb5THhB9r4meereBz14rw/640?wx_fmt=jpeg" alt=""></p><p>但是我们还需要在性能方面有一个比较明确的数据对比。  </p><p>我们最关心的两个页面性能指标就是页面加载时间和页面渲染速度。测试页面加载速度可以直接使用美团内部的Metrics性能测试工具，我们将页面Activity对象创建作为页面加载的开始时间，页面API数据返回作为页面加载结束时间。</p><p>从两个实现的页面分别启动400多次的数据中可以看到，原生实现（AllCategoryActivity）的加载时间中位数为210ms，Flutter实现（FlutterCategoryActivity）的加载时间中位数为231ms。考虑到目前我们还没有针对FlutterView做缓存和重用，FlutterView每次创建都需要初始化整个Flutter环境并加载相关代码，多出的20ms还在预期范围内：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSibylRmhTT8PxpTeKnDKWYL2x6RGdnrs5oD5r1RsmNeFjib944E97dc0g/640?wx_fmt=png" alt=""></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSNcfLnbZ89p3u8BfQLIP9sGcgkOzlTxD3SaU617SrVQmXehWd3Q0Lvg/640?wx_fmt=png" alt=""></p><p>因为Flutter的UI逻辑和绘制代码都不在主线程执行，Metrics原有的FPS功能无法统计到Flutter页面的真实情况，我们需要用特殊方法来对比两种实现的渲染效率。Android原生实现的界面渲染耗时使用系统提供的<code>FrameMetrics</code>接口进行监控：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public class AllCategoryActivity extends WmBaseActivity &#123;    </span><br><span class="line"></span><br><span class="line">  @Override    </span><br><span class="line">  protected void onCreate(Bundle savedInstanceState) &#123;        </span><br><span class="line">    if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) &#123;            </span><br><span class="line">  getWindow().addOnFrameMetricsAvailableListener(new Window.OnFrameMetricsAvailableListener() &#123;                </span><br><span class="line">    List&lt;Integer&gt; frameDurations = new ArrayList&lt;&gt;(100);                </span><br><span class="line">        @Override                </span><br><span class="line">public void onFrameMetricsAvailable(Window window, FrameMetrics frameMetrics, int dropCountSinceLastInvocation) &#123;                    </span><br><span class="line">  frameDurations.add((int) (frameMetrics.getMetric(TOTAL_DURATION) / 1000000));                    </span><br><span class="line">  if (frameDurations.size() == 100) &#123;                        </span><br><span class="line">    getWindow().removeOnFrameMetricsAvailableListener(this);                        </span><br><span class="line">L.w(&quot;AllCategory&quot;, Arrays.toString(frameDurations.toArray()));                    </span><br><span class="line">  &#125;                </span><br><span class="line">&#125;            </span><br><span class="line">  &#125;, new Handler(Looper.getMainLooper()));        </span><br><span class="line">&#125;        </span><br><span class="line">super.onCreate(savedInstanceState);        </span><br><span class="line">// ...    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Flutter在Framework层只能取到每帧中UI操作的CPU耗时，GPU操作在Flutter引擎内部实现，所以要修改引擎来监控完整的渲染耗时，在Flutter引擎目录下<code>src/flutter/shell/common/rasterizer.cc</code>文件中添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">void Rasterizer::DoDraw(std::unique_ptr&lt;flow::LayerTree&gt; layer_tree) &#123;  </span><br><span class="line">  if (!layer_tree || !surface_) &#123;    </span><br><span class="line">    return;  </span><br><span class="line">  &#125;  </span><br><span class="line">  if (DrawToSurface(*layer_tree)) &#123;    </span><br><span class="line">    last_layer_tree_ = std::move(layer_tree);</span><br><span class="line">#if defined(OS_ANDROID)    </span><br><span class="line">    if (compositor_context_-&gt;frame_count().count() == 101) &#123;      </span><br><span class="line">  std::ostringstream os;      </span><br><span class="line">  os &lt;&lt; &quot;[&quot;;      </span><br><span class="line">  const std::vector&lt;TimeDelta&gt; &amp;engine_laps = compositor_context_-&gt;engine_time().Laps();      </span><br><span class="line">  const std::vector&lt;TimeDelta&gt; &amp;frame_laps = compositor_context_-&gt;frame_time().Laps();      </span><br><span class="line">  size_t i = 1;      </span><br><span class="line">  for (auto engine_iter = engine_laps.begin() + 1, frame_iter = frame_laps.begin() + 1;           </span><br><span class="line">      i &lt; 101 &amp;&amp; engine_iter != engine_laps.end(); i++, engine_iter++, frame_iter++) &#123;        </span><br><span class="line">os &lt;&lt; (*engine_iter + *frame_iter).ToMilliseconds() &lt;&lt; &quot;,&quot;;      </span><br><span class="line">  &#125;      </span><br><span class="line">  os &lt;&lt; &quot;]&quot;;      </span><br><span class="line">  __android_log_write(ANDROID_LOG_WARN, &quot;AllCategory&quot;, os.str().c_str());    </span><br><span class="line">&#125;</span><br><span class="line">#endif  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>即可得到每帧绘制时真正消耗的时间。测试时我们将两种实现的页面分别打开100次，每次打开后执行两次滚动操作，使其绘制100帧，将这100帧的每帧耗时记录下来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for (( i = 0; i &lt; 100; i++ )); do    </span><br><span class="line">  openWMPage allcategory    </span><br><span class="line">  sleep 1    </span><br><span class="line">  adb shell input swipe 500 1000 500 300 900    </span><br><span class="line">  adb shell input swipe 500 1000 500 300 900    </span><br><span class="line">  adb shell input keyevent 4</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>将测试结果的100次启动中每帧耗时取平均値，得到每帧平均耗时情况（横坐标轴为帧序列，纵坐标轴为每帧耗时，单位为毫秒）：</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsU2kaPC5bZszVXv4KsP3uhSWjI6Vt7wAK1EZgjaSGicUlVTHfK4jN2m4QwvVDc3jXibdGm5NSlYUujA/640?wx_fmt=png" alt=""></p><p>Android原生实现和Flutter版本都会在页面打开的前5帧超过16ms，刚打开页面时原生实现需要创建大量View，Flutter也需要创建大量Widget，后续帧中可以重用大部分控件和渲染节点（原生的RenderNode和Flutter的RenderObject），所以启动时的布局和渲染操作都是最耗时的。  </p><p>10000帧（100次×100帧每次）中Android原生总平均値为10.21ms，Flutter总平均値为12.28ms，Android原生实现总丢帧数851帧8.51%，Flutter总丢帧987帧9.87%。在原生实现的触摸事件处理和过度绘制充分优化的前提下，Flutter完全可以媲美原生的性能。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>Flutter目前仍处于早期阶段，也还没有发布正式的Release版本，不过我们看到Flutter团队一直在为这一目标而努力。虽然Flutter的开发生态不如Android和iOS原生应用那么成熟，许多常用的复杂控件还需要自己实现，有的甚至会比较困难（比如官方尚未提供的<a href="https://github.com/flutter/flutter/issues/12319" target="_blank" rel="noopener">ListView.scrollTo(index)</a>功能），但是在高性能和跨平台方面Flutter在众多UI框架中还是有很大优势的。</p><p>开发Flutter应用只能使用Dart语言，Dart本身既有静态语言的特性，也支持动态语言的部分特性，对于Java和JavaScript开发者来说门槛都不高，3-5天可以快速上手，大约1-2周可以熟练掌握。</p><p>在开发全品类页面的Flutter版本时我们也深刻体会到了Dart语言的魅力，Dart的语言特性使得Flutter的界面构建过程也比Android原生的XML+JAVA更直观，代码量也从原来的900多行减少到500多行（排除掉引用的公共组件）。Flutter页面集成到App后APK体积至少会增加5.5MB，其中包括3.3MB的SO库文件和2.2MB的ICU数据文件，此外业务代码1300行编译产物的大小有2MB左右。</p><p>Flutter本身的特性适合追求iOS和Android跨平台的一致体验，追求高性能的UI交互效果的场景，不适合追求动态化部署的场景。Flutter在Android上已经可以实现动态化部署，但是由于Apple的限制，在iOS上实现动态化部署非常困难，Flutter团队也正在和Apple积极沟通。</p><p>美团外卖大前端团队将来也会继续在更多场景下使用Flutter实现，并且将实践过程中发现和修复的问题积极反馈到开源社区，帮助Flutter更好地发展。如果你也对Flutter感兴趣，欢迎加入我们。</p><p>原帖地址：<a href="https://mp.weixin.qq.com/s/cJjKZCqc8UuzvEtxK1BJCw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cJjKZCqc8UuzvEtxK1BJCw</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 移动开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>以YouTube产品为例：为你解读交互7大定律</title>
      <link href="/2018/07/23/%E4%BB%A5YouTube%E4%BA%A7%E5%93%81%E4%B8%BA%E4%BE%8B%EF%BC%9A%E4%B8%BA%E4%BD%A0%E8%A7%A3%E8%AF%BB%E4%BA%A4%E4%BA%927%E5%A4%A7%E5%AE%9A%E5%BE%8B/"/>
      <url>/2018/07/23/%E4%BB%A5YouTube%E4%BA%A7%E5%93%81%E4%B8%BA%E4%BE%8B%EF%BC%9A%E4%B8%BA%E4%BD%A0%E8%A7%A3%E8%AF%BB%E4%BA%A4%E4%BA%927%E5%A4%A7%E5%AE%9A%E5%BE%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="一、费茨定律（Fitts’Law）"><a href="#一、费茨定律（Fitts’Law）" class="headerlink" title="一、费茨定律（Fitts’Law）"></a>一、费茨定律（Fitts’Law）</h2><blockquote><p>Fitts定律提供了一种人体运动模型，由Paul Fitts于1954年建立，可以准确预测移动和选择目标所需的时间。</p><p>简而言之，Fitts定律指出：<strong>获取目标的时间是目标距离和大小的函数</strong>。</p><p>光标到达一个目标的时间，与当前光标所在的位置和目标位置的距离（D）和目标大小（S）有关。它的数学公式是：时间 T = a + b log2(D/S+1)。 随着距离的增加，运动需要更长时间，随着尺寸减小，选择再次需要更长时间。</p><p>——Interactoin Design Foundation<br><a id="more"></a><br><img src="http://image.woshipm.com/wp-files/2018/07/KOdDtGaIxPwdCQEEykRx.png" alt=""></p></blockquote><p>Fitts定律广泛应用于UX和UI设计。例如：该定律影响了交互式按钮变大的惯例（特别是在手指操作的移动设备上）， 因为较小的按钮不容易被点击。同样，<strong>用户的任务关注区域与任务相关按钮之间的距离应尽可能短。</strong></p><p>在YouTube的首页和视频播放页，频道的【订阅】按钮以非常明显的红色展示给用户，同时按钮大小在页面中的权重还是比较大的。这种表现方式，能够在最短时间内，吸引用户的注意力，激起用户产生订阅的欲望，使用户的鼠标更容易达到目标位置。</p><p>此外，像搜索框、菜单栏、个人中心、设置等一些高频功能的入口，它们的位置出现在浏览器窗口的边缘。因为，从费茨定律的角度来看，窗口或屏幕边缘区域理论上无限高或无线宽，是一个巨大的目标，用户无法用鼠标超出它们，而且容易达到目标。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/uBwZT9UpHW87uAyceYFt.png" alt=""></p><p>为提高用户点击准确率，一些按钮的实际点击区域比你看到的要大，比如：视频播放窗口下的拇指、分享、添加、更多按钮组合。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/fcBfyKowbzmfUyLWJER9.png" alt=""></p><h2 id="二、希克定律（Hick’s-Law）"><a href="#二、希克定律（Hick’s-Law）" class="headerlink" title="二、希克定律（Hick’s Law）"></a>二、希克定律（Hick’s Law）</h2><p>以英国和美国心理学家威廉·埃德蒙·希克和雷·海曼命名的希克定律或者希克-海曼定律。</p><blockquote><p>指一个人面临的选择（n）越多，所需要作出决定的时间（T）就越长，它的数学公式是：反应时间 T=a+b log2（n）。在我们的设计中如果给用户的选择更多，那么用户所需要做出决定的时间就越长。</p><p>——《维基百科》</p></blockquote><p>对于一个以视频内容为核心的产品来说，视频质量会直接影响用户兴趣、频道质量、算法推荐结果。因此，视频评价体系在设计时，YouTube以最少的选项完成了视频评价体系的基础数据收集工作，既向上的拇指和向下的拇指，两个按钮。</p><p>仔细再看，在两个按钮的下方，有一个类似进度的状态条。笔者认为：这是一种精简&amp;弱化版的评星，而且这种按钮和评级条的组合，既为用户提供了非常快捷的评价交互，也实时直观地显示出了视频的整体评价结果：蓝色占比越大，说明视频越被喜欢。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/HrjoVDQMi5d9Muw0wrow.png" alt=""></p><p>笔者下面要分享的一件事情，虽然与YouTube无关，但与视频评价和希克定律关系很大。</p><p>OK，我来为你分享一下全球流媒体巨头Netflix的一次改版经历。在去年，Netflix用“拇指向上和向下”的形式代替了“5星打分制”，据说今年夏天Netflix还将关闭用户评论区。</p><p>Netflix的产品经理认为，这种改变有两个好处：</p><ul><li>第一个好处是去掉了用户评价的模糊区，让算法学习更高效。比如：如果有个用户喜欢一个电影，但又不想给太高分，就打了“三颗星”，这种喜欢程度不仅让人很难理解，机器学习起来效率也很低。所以，点赞这种“是否”的评判系统，可以消灭模糊的灰色地带。</li><li>第二个好处是，和5颗星的选择相比，二选一更容易选，降低了用户的反馈门槛，这样一来，不仅用户更喜欢反馈，反馈的人也增加了，机器学习结果更准确。也就是说，点赞能给出明确的信号，让Netflix知道用户对这个内容感兴趣，从而给用户推荐更多类似的内容，而拇指向下则会让类似的内容不再出现在用户的主页上。</li></ul><p><img src="http://image.woshipm.com/wp-files/2018/07/7NaffXiKQ2s0RvAxs7hR.png" alt=""></p><h2 id="三、特斯勒定律（Tesler’s-Law）"><a href="#三、特斯勒定律（Tesler’s-Law）" class="headerlink" title="三、特斯勒定律（Tesler’s Law）"></a>三、特斯勒定律（Tesler’s Law）</h2><blockquote><p>特斯勒定律，又称复杂性守恒定律，指出每一个应用程序具有无法被转移或者隐匿的复杂固有量。相反，它必须在产品开发或用户交互中处理。这就提出了谁应该解决复杂性的问题。</p><p>例如：软件开发人员是否应该增加软件代码的复杂性，以使用户更简单，或者用户应该处理复杂的界面，以便软件代码可以简单。</p><p>——《维基百科》</p></blockquote><p>这个定律是说<strong>产品/系统固有的复杂性存在一个临界点，超过了这个点过程就不能再简化了，</strong>我们只能将这种复杂性转移。比如：我们如果发现页面的功能是必须的，但当前的页面信息过载，那么就需要将次要的功能收起或者转移。</p><p>YouTube右上角的更多按钮，里面包含YouTube品牌下的其他产品，视频下方的按钮也有一个更多按钮，同样在视频介绍和评论区，设计了展开和更多按钮。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/qAH9mi47ANGIJehbM9yD.png" alt=""></p><h2 id="四、亲密法则（Law-of-Proximity）"><a href="#四、亲密法则（Law-of-Proximity）" class="headerlink" title="四、亲密法则（Law of Proximity）"></a>四、亲密法则（Law of Proximity）</h2><p><strong>亲密法则是感知组织格式塔法的一部分，它指出彼此接近或接近的物体往往被组合在一起。</strong>换句话说，我们的大脑可以很容易地将物体彼此靠近地关联起来，比分开很远的物体更好，这种聚类的发生是因为人类具有组织和组合事物的自然倾向。</p><p>从另一个角度说，此相关的物体应当靠近，归组到一起。这种同类相吸的亲密法则在Robin P Williams的《写给大家看的设计书》中也被提及。</p><p>YouTube将功能入口和按钮按照一定的规则进行了区域、位置、视觉上的划分。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/1YA2UrWQ1G5Zyp8wExYb.png" alt=""></p><h2 id="五、奥卡姆的剃刀定律（Occam’s-Razor）"><a href="#五、奥卡姆的剃刀定律（Occam’s-Razor）" class="headerlink" title="五、奥卡姆的剃刀定律（Occam’s Razor）"></a>五、奥卡姆的剃刀定律（Occam’s Razor）</h2><blockquote><p>Occam的剃刀（ 拉丁文：lex parsimoniae “ 简约法则 ”）是一种解决问题的原则，最简单的解决方案往往是正确的解决方案。当提出相互竞争的假设来解决问题时，应该选择具有最少假设的解决方案。这个想法归功于奥克汉姆的威廉（约1287-1347），他是英国方济各会修士，学者哲学家和神学家。</p><p>————《维基百科》</p></blockquote><p>回忆自己的绘画、摄影还有设计经历，笔者认为：绘画是做加法，而摄影和设计是做减法。</p><p>奥卡姆的剃须刀法则主要就是说我们做产品/系统时功能上不可以太繁琐，应该保证简洁和工具化。</p><p>为什么要将复杂变简单呢？</p><p>因为复杂容易使人迷失，只有简单化后才利于人们理解和操作。随着社会、经济的发展，时间和精力成为人们的稀缺资源。从这个意义上讲，<strong>简化才意味着对事物真正的掌控，正所谓“大道至简”。</strong>比如：iPhone和iPad只有一个圆形的物理按钮，简单到连三岁的小孩都会使用。</p><p>YouTube移动端的载入页仅一个Logo，在收件箱页，当用户没有好友时，系统通过一个明显的按钮引导用户操作。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/OxRt6TIpCBPD9Svo2DDy.png" alt=""></p><h2 id="六、米勒定律（又称7±2定律）"><a href="#六、米勒定律（又称7±2定律）" class="headerlink" title="六、米勒定律（又称7±2定律）"></a>六、米勒定律（又称7±2定律）</h2><p>1956年美国科学家米勒对人类短时记忆能力进行了研究，他注意到年轻人的记忆广度大约为5到9个单位之间，就是7±2法则。这个法则对我们做界面设计的启迪就是，如果希望用户记住导航区域的内容或者一个路径的顺序，那么数量应该控制在七个左右，移动端底部标签通常情况下最多也是5个。</p><p><img src="http://image.woshipm.com/wp-files/2018/07/lb9ZoOnP0y2lFwg9ghoR.png" alt=""></p><h2 id="七、防错法则"><a href="#七、防错法则" class="headerlink" title="七、防错法则"></a>七、防错法则</h2><p>防错法则认为大部分的意外都是由设计的疏忽，而不是人为操作疏忽。通过改变设计可以把过失降到最低。该原则最初是用于工业管理的，但在交互设计也十分适用。</p><p>一个走心、友好的放错设计通常表现在：</p><ul><li>及时地告诉用户哪里操作错了。比如：在填写表单时，系统给出及时地报错提示；</li><li>重要、谨慎的操作，系统通常会有二次确认；</li><li>为用户提供撤销的机会；</li><li>为用户提供纠错的机会。</li></ul><p><img src="http://image.woshipm.com/wp-files/2018/07/QAqBAvuSo4Un1He5dvPC.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><blockquote><p>Unless you have a better choice, you will follow the standard.<br>除非有更好的选择，否则就遵从标准。<br>——Alan Cooper</p></blockquote><p><img src="http://image.woshipm.com/wp-files/2018/07/TkyEjVonBuG0ffX2tpyn.png" alt=""></p><p>本文遵循7±2定律，就为你分享7大定律在YouTube的应用。其实还有很多经典的设计法则，比如：格式塔等，笔者以后还会找一个款产品，为你详细分享。</p><p>感谢你的阅读，我们下期再见~</p><p>原文地址：<a href="http://www.woshipm.com/ucd/1143120.html" target="_blank" rel="noopener">http://www.woshipm.com/ucd/1143120.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 产品经理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Serverless 的适用场景</title>
      <link href="/2018/07/12/Serverless-%E7%9A%84%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
      <url>/2018/07/12/Serverless-%E7%9A%84%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF/</url>
      
        <content type="html"><![CDATA[<p>尽管 Serverless 在编写传统的 Web 应用上，有一定的缺点。然而，它的事件驱动及运行时计算，使得它在某些场景上相当的合适。</p><h3 id="发送通知"><a href="#发送通知" class="headerlink" title="发送通知"></a>发送通知</h3><p>由我们在上一节中提到的，对于诸如 PUSH Notification、<a href="https://www.phodal.com/blog/serverless-development-guide-aws-simple-email-service/" target="_blank" rel="noopener">邮件通知接口</a>、短信，这一类服务来说，他们都需要基础设施来搭建。并且，他们对实时性的要求相对没有那么高。</p><p>即使在时间上晚来几秒钟，用户还是能接受的。在我们所见到的短信发送的例子里，一般都会假设用户能在 60 秒内收到短信。因此，在这种时间 1s 的误差，用户也不会恼火的。而对于 APP 的消息推送而言，这种要求就更低了，用户反而不太希望能收到这样的推送<br><a id="more"></a></p><h3 id="WebHook"><a href="#WebHook" class="headerlink" title="WebHook"></a>WebHook</h3><p>当我们没有服务器，又想要一个 Webhook 来触发我们一系列的操作的时候。我们就可以考虑使用 Serverless，我们不需要一直就这么支付一个服务器的费用。通过 Serverless，我们就可以轻松完成这样的工作，并且节省大量的费用。</p><p>一个比较明显的例子，就如  <a href="https://www.phodal.com/blog/serverless-development-guide-create-github-hooks/" target="_blank" rel="noopener">GitHub Hooks</a></p><blockquote><p>GitHub 上的 Webhook 允许我们构建或设置在 GitHub.com 上订阅某些事件的 GitHub 应用程序。当触发这些事件之一时，我们将向 webhook 配置的 URL 发送 HTTP POST 有效内容。</p></blockquote><p>比如说，当我们 PUSH 了代码，我们想触发我们的持续集成。这个时候，就可以通过一个 Webhook 来做这样的事情。</p><h3 id="轻量级-API"><a href="#轻量级-API" class="headerlink" title="轻量级 API"></a>轻量级 API</h3><p>Serverless 特别适合于，轻量级快速变化地 API。</p><p>其实，我一直没有想到一个合适的例子。在我的假想里，一个 AutoSuggest 的 API 可能就是这样的 API，但是这种 API 在有些时候，往往会伴随着相当复杂的业务。</p><p>于是，便想举一个 Featrue Toggle 的例子，尽管有一些不合适。但是，可能是最有价值的部分。</p><h3 id="物联网"><a href="#物联网" class="headerlink" title="物联网"></a>物联网</h3><p>当我们谈及物联网的时候，我们会讨论事件触发、传输协议、海量数据（数据存储、数据分析）。而有了 Serverless，那么再多的数据，处理起来也是相当容易的一件事。</p><p>对于一个物联网应用的服务端来说，系统需要收集来自各个地方的数据，并创建一个个 pipeline 来处理、过滤、转换这些数据，并将数据存储到数据库中。</p><p>对于硬件开发人员来说，对接不同的硬件，本身就是一种挑战。而直接使用诸如 AWS IoT 这样国，可以在某种程度上，帮助我们更好地开发出写服务端连接的应用。</p><p>同时，对于物联网应用的客户端来说，则需要从数据库抽取数据进行展示。这部分，可能算不上是一个挑战点。</p><h3 id="数据统计分析"><a href="#数据统计分析" class="headerlink" title="数据统计分析"></a>数据统计分析</h3><p>数据统计本身只需要很少的计算量，但是生成图表，则可以定期生成。</p><p>在接收数据的时候，我们不需要考虑任何延时带来的问题。50~200 ms 的延时，并不会对我们的系统造成什么影响。</p><h3 id="Trigger-及定时任务"><a href="#Trigger-及定时任务" class="headerlink" title="Trigger 及定时任务"></a>Trigger 及定时任务</h3><p>对于哪些需要爬虫来抓取和生成的程序来说，Serverless  <strong>可能</strong>是一个不错的舞台。</p><p>尽管，这样的工作也可以由云服务器来做，我们只需要定时的启动一下服务器。通过服务器中的自启动脚本来做相应的事，但是当我们完成了一系列的工作之后。我们需要将数据存储在一个远程的服务器上。而为了让系统中的其它应用，也能直接访问这些数据。那么，我们可能会考虑使用一个云数据库。这个时候，Serverless 应用看上去更具有吸引力。</p><p>在那篇《CRON 定时执行 Lambda 任务》中，我们也可以看到 AWS Lambda 可以支持 Lambda 计算，定时启动服务，并计算。</p><h3 id="精益创业"><a href="#精益创业" class="headerlink" title="精益创业"></a>精益创业</h3><p><img src="http://serverless.ink/images/launch-page.jpg" alt="Landing Page"></p><p>Landing Page</p><p>Serverless 的快速上线、开发，意味着它可以快速验证一个想法 MVP。如 Dropbox 在开始的时候，只创造了一个 Landing Page。作为一个想使用这个服务的用户，我们会在其中填上我们的邮箱。</p><p>而如果是使用 Serverless 来构建这样的应用，那么我们只需要创建一个静态页面，然后用一个 Serverless 服务来保存用户的邮箱到数据库中，如我在 GitHub 上的  <a href="https://github.com/phodal/serverless-landingpage" target="_blank" rel="noopener">serverless-landingpage</a>  所做的那样。</p><h3 id="Chat-机器人"><a href="#Chat-机器人" class="headerlink" title="Chat 机器人"></a>Chat 机器人</h3><p>聊天机器人，也是一个相当好的应用场景。</p><p>But，由于国内的条件限制（信息监管），这并不是一件容易的事。因此，从渠道（如微信、blabla）上，都在尽可能地降低这方面的可能性。</p><p>但是，我们还可以做一个微信公众号的服务。当用户输入一个关键词时，做出相应的回复，这实质上和聊天机器人是差不多的。只需要结合《<a href="https://www.phodal.com/blog/serverless-development-guide-serverless-lambda-wechat-public-platform/" target="_blank" rel="noopener">基于 Serverless 与 Lambda 的微信公共平台</a>》 就可以轻松实现，并实现快速上线。</p><p>节选自《<a href="https://github.com/phodal/serverless" target="_blank" rel="noopener">Serverless 架构应用开发指南</a> 》</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Serverless 架构的优缺点</title>
      <link href="/2018/07/12/Serverless-%E6%9E%B6%E6%9E%84%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9/"/>
      <url>/2018/07/12/Serverless-%E6%9E%B6%E6%9E%84%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<h2 id="Serverless-的优势"><a href="#Serverless-的优势" class="headerlink" title="Serverless 的优势"></a>Serverless 的优势</h2><p>在我使用 Serverless Framework 开发 AWS Serverless 应用的过程中，最方便的莫过于，第一次部署和第二次、第三次部署没有什么区别。只需要执行  <code>serverless deploy</code>，几分钟后，我们代码就运行在线上。如果是一个传统的 AWS 应用，我需要 SSH 到我的服务器上部署，这样才能写好我的自动部署脚本。除此，我还需要担忧这个过程中，有哪些用户有使用。</p><p>除了，我觉得的部署方便，还有就是价格合理。我的 AWS EC2 实例上运行着我的博客、以及其他的一些网络。然而，我那 PV 只有 500 左右的博客，大部分时间都是在空转。便觉得有些浪费，可是运行才收费的 Serverless 就不会有这样的问题。可以让我大胆地去使用这些服务。当然了，还有其它一些显著的优势。<br><a id="more"></a></p><h3 id="1-降低启动成本"><a href="#1-降低启动成本" class="headerlink" title="1. 降低启动成本"></a>1. 降低启动成本</h3><p>当我们作为一家公司开发一个 Web 应用时，在开发的时候，我们需要版本管理服务器、持续集成服务器、测试服务器、应用版本管理仓库等作为基础的服务。线上运行的时候，为了应对大量的请求，我们需要一个好的数据库服务器。当我们的应用面向了普通的用户时，我们需要：</p><ul><li>邮件服务，用于发送提醒、注册等服务</li><li>短信服务（依国家实名规定），用于注册、登录等用户授权操作</li></ul><p>对于大公司而言，这些都是现成的基础设施。可对于新创企业来说，这都是一些启动成本。</p><h4 id="减少运营成本"><a href="#减少运营成本" class="headerlink" title="减少运营成本"></a>减少运营成本</h4><p>对于初创公司来说，他们没有基础设施，也没有财力，也可能没有能力去建设基础设施。采用云服务往往是最好的选择，可以节省大量的资金。他们可以将注意力放在：创造对用户有价值的产品上。如果一家创业公司采用云服务，而不是自己搭建服务器。那么，他就会拥有<strong>更多的时间</strong>开发业务功能，而不是关注在这些。只需要为运行时的软件付钱。</p><p>而采用<strong>函数计算</strong>的 Serverless 与云服务器最大的不同之处在于：<strong>云服务器需要一直运行，而函数计算是按需计算</strong>。按需计算就意味着，在请求到来的时候，才运行函数。没有请求的时候，是不算钱的。</p><p>项目初期，其用户数往往是缓慢增长的，而我们在选择服务器的时候，往往会依可能出现的用户来估算。在这个时候，往往会浪费一些不必要的成本。不过，就算用户突然间爆发，Serverless 应用也可以轻松处理。只需要修改一下数据库配置，再重新部署一份。</p><h4 id="降低开发成本"><a href="#降低开发成本" class="headerlink" title="降低开发成本"></a>降低开发成本</h4><p>一个成功的 Serverless 服务供应商，应该能提供一系列的<strong>配套服务</strong>。这意味着，你只需要在配置文件上写下，这个数据库的表名，那么我们的数据就会存储到对应的数据库里。甚至于，**如果一个当服务提供者提供一系列的函数计算模板，那么我们只需要写好我们的配置即可。这一系列的东西都可以自动、高效的完成。</p><p>在这种情况下，<strong>使用某一个云服务，就会调用某一个系统自带的 API 一样简单</strong>。</p><p>当然，将应用设计成无状态应用，对于早期的系统，可能是一种挑战。除此，诸如 AWS 这样庞大的系统，对于新手程序员来说，也不能容易消化掉的一个系统。</p><h3 id="2-实现快速上线"><a href="#2-实现快速上线" class="headerlink" title="2. 实现快速上线"></a>2. 实现快速上线</h3><p>对于一个 Web 项目来说，启动一个项目需要一系列的 hello, world。当我们在本地搭建环境的时候，是一个 hello, world，当我们将程序部署到开发环境时，也是一个部署相关的 hello, world。虽然看上去有些不同，但是总的来说，都是 it works!。</p><p>Serverless 在部署上的优势，使得你可以轻松地实现上线。</p><h4 id="更快的部署流水线"><a href="#更快的部署流水线" class="headerlink" title="更快的部署流水线"></a>更快的部署流水线</h4><p>实际上，Serverless 应用之所以在部署上有优势，是因为其相当于<strong>内建自动化部署</strong>——我们在开发应用的时候，已经在不断地增强部署功能。</p><p>在我们日常的开发中，为了实现自动化部署，我们需要先手动部署，以设计出一个相关无错的部署配置，如 Docker 的 Dockerfile，又或者是 Ansible 的 playbook。除此，我们还需要设计好蓝绿发布等等的功能。</p><p>而在函数计算、Serverless 应用里，这些都是由供应商提供的功能。每次我们写完代码，只需要运行一下：<code>sls deploy</code>  就足够了。在诸如 AWS Lambda 的函数计算里，函数一般在上传后几秒钟内，就能做好调用准备。</p><p>这就意味着，当我们和日常一样，使用一个模板来开发我们的应用。我们就可以在 Clone 完代码后的几分钟内，完成第一次部署。</p><p>唯一的难点，可能是要选用什么配置类型的服务，如选用哪个级别吞吐量的 DynamoDB、哪个内存大小的 Lambda 计算。</p><h4 id="更快的开发速度"><a href="#更快的开发速度" class="headerlink" title="更快的开发速度"></a>更快的开发速度</h4><p>由于 Serverless 服务提供者，已经准备好了一系列的基础服务。作为开发人员的我们，只需要关注于如何更好去实现业务，而非技术上的一些限制。</p><p>服务提供者已经向我们准备，并测试好了这一系列的服务。它们基本上是稳定、可靠的，不会遇上特别大的问题。事实上，当我们拥有足够强大的代码，如使用测试来保证健壮性，那么结合持续集成，我们就可以在 PUSH 代码的时候，直接部署到生产环境。当然，可能不需要这么麻烦，我们只需要添加一个 predeploy 的 hook，在这个 hook 里做一些自动测试的工作，就可以在本地直接发布新的版本。</p><p>这个过程里，我们并不需要考虑太多的发布事宜。</p><h3 id="3-系统安全性更高"><a href="#3-系统安全性更高" class="headerlink" title="3. 系统安全性更高"></a>3. 系统安全性更高</h3><p>依我维护我博客的经验来看，要保持服务器一直运行不是一件容易的事。在不经意的时候，总会发现有 Cracker 在攻击你网站。我们需要防范不同类型的攻击，如在我的服务器里一直有黑客在尝试密码登录，可是我的博客的服务器是要密钥才能登录的。在一次神奇的尝试登录攻击后，我的 SSH 守护进程崩溃了。这意味着，我只能从 EC2 后台重启服务器。</p><p>有了 Serverless，我不再需要担心有人尝试登录系统，因为我都不知道怎么登录服务器。</p><p>我不再需要考虑系统底层安全问题，每次登录 AWS EC2，我总需要更新一遍软件；每当我看到某个软件有漏洞时，如之前的 OpenSSH，我就登录上去看一下版本，更新一下软件。真 TM 费时又费力，还没有一点好处。</p><p>唯一需要担心的，可能是有人发起 DDOS 攻击。而根据<a href="https://thenewstack.io/zombie-toasters-eat-startup/" target="_blank" rel="noopener">Could Zombie Toasters DDoS My Serverless Deployment?</a>的计算，每百万的请求，大概是 0.2 刀，每小时 360000000 个请求，也就 72 刀。</p><h3 id="4-适应微服务架构"><a href="#4-适应微服务架构" class="headerlink" title="4. 适应微服务架构"></a>4. 适应微服务架构</h3><p>如我们所见在最近几年里看到的那样，微服务并不没有大量地替换掉单体应用——毕竟使用新的架构来替换旧的系统，在业务上的价值并不大。因此，对于很多企业来说，并没有这样的强烈需求及紧迫性。活着，才是一件更紧迫的事。</p><p>而 Serverless 天生就与微服务架构是<strong>相辅相成</strong>的。一个 Serverless 应用拥有自己的网关、数据库、接口，你可还以使用自己喜欢的语言（受限于服务提供者）来开发服务。换句话来说，在这种情形下，一个 Serverless 可能是一个完美的微服务实例。</p><p>在可见的一二年里，Serverless 将替换到某些系统中的一些组件、服务。</p><h3 id="5-自动扩展能力"><a href="#5-自动扩展能力" class="headerlink" title="5. 自动扩展能力"></a>5. 自动扩展能力</h3><p>Serverless 的背后是 诸如 AWS Lambda 这样的 FaaS（Function as a Services）。</p><p>对于传统应用来说，要应对更多的请求的方式，就是部署更多的实例。然而，这个时候往往已经来不及了。而对于 FaaS 来说，我们并不需要这么做，FaaS 会自动的扩展。它可以在需要时尽可能多地启动实例副本，而不会发生冗长的部署和配置延迟。</p><p>这依赖于我们的服务是无状态的，我们才能次无忌惮地不断运行起新的实例。</p><h2 id="Serverless-的问题"><a href="#Serverless-的问题" class="headerlink" title="Serverless 的问题"></a>Serverless 的问题</h2><p>作为一个运行时，才启动的应用来说，Serverless 也存在着一个个我们所需要的问题。</p><h3 id="1-不适合长时间运行应用"><a href="#1-不适合长时间运行应用" class="headerlink" title="1. 不适合长时间运行应用"></a>1. 不适合长时间运行应用</h3><p>Serverless 在请求到来时才运行。这意味着，当应用不运行的时候就会进入 “休眠状态”，下次当请求来临时，应用将会需要一个启动时间，即<strong>冷启动</strong>。这个时候，可以结合 CRON 的方式或者 CloudWatch 来定期唤醒应用。</p><p>如果你的应用需要一直长期不间断的运行、处理大量的请求，那么你可能就不适合采用 Serverless 架构。在这种情况下，采用 EC2 这样的云服务器往往是一种更好的选择。因为 EC2 从价格上来说，更加便宜。</p><p>引用  <a href="https://www.zhihu.com/people/lu-zou-36" target="_blank" rel="noopener">Lu Zou</a>  在 《<a href="https://zhuanlan.zhihu.com/p/31122433" target="_blank" rel="noopener">花了 1000G，我终于弄清楚了 Serverless 是什么（上）：什么是 Serverless 架构？</a>》上的评论：</p><blockquote><p>EC2 相当于你买了一辆车，而 Lambda 相当于你租了你一辆车。</p></blockquote><p>长期租车的成本肯定比买车贵，但是你就少掉了一部分的维护成本。因此，这个问题实际上是一个值得深入计算的问题。</p><h3 id="2-完全依赖于第三方服务"><a href="#2-完全依赖于第三方服务" class="headerlink" title="2. 完全依赖于第三方服务"></a>2. 完全依赖于第三方服务</h3><p>是的，当你决定使用某个云服务的时候，也就意味着你可能走了一条不归路。在这种情况下，只能将不重要的 API 放在 Serverless 上。</p><p>当你已经有大量的基础设施的时候，Serverless 对于你来说，并不是一个好东西。当我们采用 Serverless 架构的时候，我们就和特别的服务供应商绑定了。我们使用了 AWS 家的服务，那么我们再将服务迁到 Google Cloud 上就没有那么容易了。</p><p>我们需要修改一下系列的底层代码，能采取的应对方案，便是建立隔离层。这意味着，在设计应用的时候，就需要：</p><ul><li>隔离 API 网关</li><li>隔离数据库层，考虑到市面上还没有成熟的 ORM 工具，让你即支持 Firebase，又支持 DynamoDB</li><li>等等</li></ul><p>这些也将带给我们一些额外的成本，可能<strong>带来的问题会比解决的问题多</strong>。</p><h3 id="3-冷启动时间"><a href="#3-冷启动时间" class="headerlink" title="3. 冷启动时间"></a>3. 冷启动时间</h3><p>如上所说，Serverless 应用存在一个冷启动时间的问题。</p><p>据 New Relic 官方博客《<a href="https://blog.newrelic.com/2017/01/11/aws-lambda-cold-start-optimization/" target="_blank" rel="noopener">Understanding AWS Lambda Performance—How Much Do Cold Starts Really Matter?</a>》称，AWS Lambda 的冷启动时间。</p><p><img src="http://serverless.ink/images/aws-lambda-monitoring-functions-chart.png" alt="AWS 启动时间"></p><p>AWS 启动时间</p><p>又或者是我之前统计的请求响应时间：</p><p><img src="http://serverless.ink/images/times.png" alt="Serverless 请求时间"></p><p>Serverless 请求时间</p><p>尽管这个冷启动时间大部分情况下，可以在 50ms 以内。而这是对于 Node.js 应用来说，对于拥有虚拟机的 Java 和 C# 可能就没有那么幸运了。</p><h3 id="4-缺乏调试和开发工具"><a href="#4-缺乏调试和开发工具" class="headerlink" title="4. 缺乏调试和开发工具"></a>4. 缺乏调试和开发工具</h3><p>当我使用 Serverless Framework 的时候，遇到了这样的问题：缺乏调试和开发工具。后来，我发现了 serverless-offline、dynamodb-local 等一系列插件之后，问题有一些改善。</p><p>然而，对于日志系统来说，这仍然是一个艰巨的挑战。</p><p>每次你调试的时候，你需要一遍又一遍地上传代码。而每次上传的时候，你就好像是在部署服务器。然后 Fuck 了，我并不能总是快速地定位出问题在哪。于是，我修改了一下代码，添加了一行  <code>console.log</code>，然后又一次地部署了下代码。问题解决了，挺好的，我删了一下  <code>console.log</code>，然后又一次地部署了下代码。</p><p>后来，我学乖了，找了一个类似于 log4j 这样的可以分级别纪录日志的 Node.js 库  <code>winston</code>。它可以支持 error、warn、info、verbose、debug、silly 六个不同级别的日志。</p><h3 id="5-构建复杂"><a href="#5-构建复杂" class="headerlink" title="5. 构建复杂"></a>5. 构建复杂</h3><p>Serverless 很便宜，但是这并不意味着它很简单。</p><p>早先，在知道 AWS Lambda 之后，我本来想进行一些尝试。但是 CloudForamtion 让我觉得太难了，它的配置是如此的复杂，并且难以阅读及编写（JSON 格式）。</p><p>考虑到 CloudForamtion 的复杂度，我是在接触了 Serverless Framework 之后，才重新燃起了一些信心。</p><p>Serverless Framework 的配置更加简单，采用的是 YAML 格式。在部署的时候，Serverless Framework 会根据我们的配置生成 CloudForamtion 配置。</p><p>在那篇《<a href="https://www.phodal.com/blog/serverless-development-guide-use-kinesis-firehose-stream-data-s3/" target="_blank" rel="noopener">Kinesis Firehose 持久化数据到 S3</a>》想着的数据统计文章里，我们介绍了 Serverless 框架的配置。与一般的 Lambda 配置来说，这里的配置就稍微复杂一些。然而，这也并非是一个真正用于生产的配置。我的意思是，真实的应用场景远远比这复杂。</p><h3 id="6-语言版本落后"><a href="#6-语言版本落后" class="headerlink" title="6. 语言版本落后"></a>6. 语言版本落后</h3><p>在 Node.js 6 出来的时候，AWS Lambda 只支持 Node.js 4.3.2；在 Node.js 9.0 出来的时候，AWS Lambda 支持到 6.10.3。</p><p>如下是 AWS Lambda 支持以下运行时版本：</p><ul><li>Node.js – v4.3.2 和 6.10.3</li><li>Java - Java 8</li><li>Python – Python 3.6 和 2.7</li><li>.NET 内核 – .NET 内核 1.0.1 (C#)</li></ul><p>对于 Java 和 Python 来说，他们的版本上可能基本都是够用的，我不知道 C# 怎么样。但是 Node.js 的版本显然是有点老旧的，但是都 Node.js 9.2.0 了。不过，话说来说，这可能与版本帝 Chrome 带来的前端版本潮有一点关系。</p><p>节选自《<a href="https://github.com/phodal/serverless" target="_blank" rel="noopener">Serverless 架构应用开发指南</a> 》</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>什么是 Serverless 架构？</title>
      <link href="/2018/07/12/%E4%BB%80%E4%B9%88%E6%98%AF-Serverless-%E6%9E%B6%E6%9E%84%EF%BC%9F/"/>
      <url>/2018/07/12/%E4%BB%80%E4%B9%88%E6%98%AF-Serverless-%E6%9E%B6%E6%9E%84%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>花了 1000G，我终于弄清楚了 Serverless 是什么？</p></blockquote><p>在过去的 24 小时，我通过微信公众号的『电子书』一事，大概处理了 8000 个请求：</p><p><img src="http://serverless.ink/images/counts.png" alt="Serverless 请求统计"><br><a id="more"></a><br>Serverless 请求统计</p><p>大部分的请求都是在 200ms 内完成的，而在最开始的请求潮里（刚发推送的时候，十分钟里近 1500 个请求），平均的响应时间都在 50ms 内。</p><p><img src="http://serverless.ink/images/times.png" alt="Serverless 请求时间"></p><p>Serverless 请求时间</p><p>这也表明了，Serverless 相当的可靠。显然，当请求越多的时候，响应时间越快，这简直有违常理——一般来说，随着请求的增加，响应时间会越来越慢。</p><p>毫无疑问，在最近的几年里，微服务渐渐成为了一个相当流行的架构风格。微服务大致从 2014 年起，开始流行开来，如下图所示：</p><p><img src="http://serverless.ink/images/microservice-compare-serverless.png" alt="microservices vs serverless"></p><p>microservices vs serverless</p><p>而 Serverless 是从 2016 年起，开始受到开发者的关注。并且从其发展趋势来看，它大有可能在两年后，拥有今天微服务一样的地位。可见，它是一个相当具有潜力的架构。</p><h2 id="什么是-Serverless-架构？？"><a href="#什么是-Serverless-架构？？" class="headerlink" title="什么是 Serverless 架构？？"></a>什么是 Serverless 架构？？</h2><p>为了弄清 Serverless 究竟是什么东西，Serverless 到底是个什么，我使用 Serverless 尝试了一个又一个示例，我自己也做了四五个应用，总算是对 Serverelss 有了一个大致上的认识。</p><h3 id="虚拟化与隔离"><a href="#虚拟化与隔离" class="headerlink" title="虚拟化与隔离"></a>虚拟化与隔离</h3><blockquote><p>开发人员为了保证开发环境的正确（即，这个 Bug 不是环境因素造成的），想出了一系列的隔离方式：虚拟机、容器虚拟化、语言虚拟机、应用容器（如 Java 的 Tomcat）、虚拟环境（如 Python 中的 virtualenv），甚至是独立于语言的 DSL。<a href="http://serverless.ink/#fn1" target="_blank" rel="noopener">1</a></p></blockquote><p>从最早的物理服务器开始，我们都在不断地抽象或者虚拟化服务器。</p><p><img src="http://serverless.ink/images/server-growth.jpg" alt="服务器发展"></p><p>服务器发展</p><ul><li>我们使用 XEN、KVM等虚拟化技术，隔离了硬件以及运行在这之上的操作系统。</li><li>我们使用云计算进一步地自动管理这些虚拟化的资源。</li><li>我们使用 Docker 等容器技术，隔离了应用的操作系统与服务器的操作。</li></ul><p>现在，我们有了 Serverless，我们可以隔离操作系统，乃至更底层的技术细节。</p><h3 id="为什么是花了-1000G-？"><a href="#为什么是花了-1000G-？" class="headerlink" title="为什么是花了 1000G ？"></a>为什么是花了 1000G ？</h3><p>现在，让我简单地解释『花了 1000G，我终于弄清楚了 Serverless 是什么？』这句话，来说说 Serverless 到底是什么鬼？</p><p>在实践的过程中，我采用的是 AWS Lambda 作为 Serverless 服务背后的计算引擎。AWS Lambda 是一种函数即服务（Function-as-a-Servcie，FaaS）的计算服务，简单的来说就是：开发人员直接编写运行在云上的函数、功能、服务。由云服务产商提供操作系统、运行环境、网关等一系列的基础环境，我们只需要关注于编写我们的业务代码即可。</p><p>是的，你没听错，我们只需要<strong>考虑怎么用代码提供价值即可</strong>。我们甚至连可扩展、蓝绿部署等一系列的问题都不用考虑，Amazon 优秀的运营工程师已经帮助我们打造了这一系列的基础设施。并且与传统的 AWS 服务一样，如 Elastic Compute Cloud（EC2），它们都是按流量算钱的。</p><p>那么问题又来了，它到底是怎么对一个函数收钱的。我在 Lambda 函数上运行一个 Hello, world 它会怎么收我的钱呢？</p><p>如果要对一个运行的函数收费，那么想必只有运行时间、CPU、内存占用、硬盘这几个条件。可针对于不同的需求，提供不同的 CPU 是一件很麻烦的事。对于代码来说，一个应用占用的硬盘空间几乎可以忽略不计。当然，这些应用会在你的 S3 上有一个备份。于是，诸如 AWS 采用的是运行时间 + 内存的计算方式。</p><table><thead><tr><th>内存 (MB)</th><th>每个月的免费套餐秒数</th><th>每 100ms 的价格 (USD)</th></tr></thead><tbody><tr><td>128</td><td>3,200,000</td><td>0.000000208</td></tr><tr><td>192</td><td>2,133,333</td><td>0.000000313</td></tr><tr><td>256</td><td>1,600,000</td><td>0.000000417</td></tr><tr><td>…</td><td>…</td><td>…</td></tr><tr><td>1024</td><td>400,000</td><td>0.000001667</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><p>在运行程序的时候，AWS 会统计出一个时间和内存，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">REPORT RequestId: 041138f9-bc81-11e7-aa63-0dbab83f773d  Duration: 2.49 ms   Billed Duration: 100 ms     Memory Size: 1024 MB    Max Memory Used: 20 MB</span><br></pre></td></tr></table></figure><p>其中的  <code>Memory Size</code>  即是我们选用的套餐类型，Duration 即是运行的时间，Max Memory Used 是我们应用运行时占用的内存。根据我们的 Max Memory Used 数值及应用的计算量，我们可以很轻松地计算出我们所需要的套餐。</p><p>当然，选择不同大小的内存，也意味着选择不同功率的 CPU。</p><blockquote><p>在 AWS Lambda 资源模型中，您可以选择您想为函数分配的内存量，并按 CPU 功率和其他资源的比例进行分配。例如，选择 256MB 的内存分配至您的 Lambda 函数的 CPU 功率约是请求 128MB 内存的两倍，若选择 512MB 的内存，其分配的 CPU 功率约是一半。您可以在 128MB 到 1.5GB 的范围间以 64MB 的增量设置您的内存。</p></blockquote><p>因此，如果我们选用 1024M 的套餐，然后运行了 320 次，一共算是使用了 320G 的计算量。而其运行时间会被舍入到最近的 100ms，就算我们运行了 2.49ms，那么也是按 100ms 算的。那么假设，我们的 320 次计算都花了 1s，也就是 10×100ms，那么我们要支付的费用是：10×320×0.000001667=0.0053344刀，即使转成人民币也就是不到 4 毛钱的 0.03627392。</p><p>如果我们先用的是 128M 的套餐，那么运行了 2000 次，就是 200G 的计算量了。</p><p>如果我们先用的是 128M 的套餐，那么运行了 8000 次，就是 1000G 的计算量了。</p><p>不过如上表所示，AWS 为 Lambda 提供了一个免费套餐（无期限地提供给新老用户）包含每月 1M 免费请求以及每月 400 000 GB 秒的计算时间。这就意味着，在很长的时间里，我们一分钱都不用花。</p><h3 id="Serverless-是什么？"><a href="#Serverless-是什么？" class="headerlink" title="Serverless 是什么？"></a>Serverless 是什么？</h3><p>而从上节的内容中，我们可以知道这么几点：</p><ul><li>在 Serverless 应用中，开发者只需要专注于业务，剩下的运维等工作都不需要操心</li><li>Serverless 是<strong>真正的按需使用</strong>，请求到来时才开始运行</li><li>Serverless 是按运行时间和内存来算钱的</li><li>Serverless 应用严重依赖于特定的云平台、第三方服务</li></ul><p>当然这些都是一些虚无缥缈地东西。</p><p>按 AWS 官方对于 Serverless 的介绍是这样的：</p><blockquote><p>服务器架构是基于互联网的系统，其中应用开发不使用常规的服务进程。相反，它们仅依赖于第三方服务（例如AWS Lambda服务），客户端逻辑和服务托管远程过程调用的组合。”<a href="http://serverless.ink/#fn2" target="_blank" rel="noopener">2</a></p></blockquote><p>在一个基于 AWS 的 Serverless 应用里，应用的组成是：</p><ul><li>网关 API Gateway 来接受和处理成千上万个并发 API 调用，包括流量管理、授权和访问控制、监控等</li><li>计算服务 Lambda 来进行代码相关的一切计算工作，诸如授权验证、请求、输出等等</li><li>基础设施管理 CloudFormation 来创建和配置 AWS 基础设施部署，诸如所使用的 S3 存储桶的名称等</li><li>静态存储 S3 作为前端代码和静态资源存放的地方</li><li>数据库 DynamoDB 来存储应用的数据</li><li>等等</li></ul><p>以博客系统为例，当我们访问一篇博客的时候，只是一个 GET 请求，可以由 S3 为我们提供前端的静态资源和响应的 HTML。</p><p><img src="http://serverless.ink/images/serverless-spa-architecture.png" alt="Serverless SPA 架构"></p><p>Serverless SPA 架构</p><p>而当我们创建一个博客的时候：</p><ul><li>我们的请求先来到了 API Gateway，API Gateway 计费器 + 1</li><li>接着请求来到了 Lambda，进行数据处理，如生成 ID、创建时间等等，Lambda 计费器 + 1</li><li>Lambda 在计算完后，将数据存储到 DynamoDB 上，DynamoDB 计费器 + 1</li><li>最后，我们会生成静态的博客到 S3 上，而 S3 只在使用的时候按存储收费。</li></ul><p>在这个过程中，我们使用了一系列稳定存在的云服务，并且只在使用时才计费。由于这些服务可以自然、方便地进行调用，我们实际上只需要关注在我们的 Lambda 函数上，以及如何使用这些服务完成整个开发流程。</p><p>因此，Serverless 并不意味着没有服务器，只是服务器以特定功能的第三方服务的形式存在。</p><p>当然并不一定使用这些云服务（如 AWS），才能称为 Serverless。诸如我的同事在 《<a href="https://blog.jimmylv.info/2017-06-30-serverless-in-action-build-personal-reading-statistics-system/" target="_blank" rel="noopener">Serverless 实战：打造个人阅读追踪系统</a>》，采用的是：IFTTT + WebTask + GitHub Webhook 的技术栈。它只是意味着，你所有的应用中的一部分服务直接使用的是第三方服务。</p><p>在这种情况下，系统间的分层可能会变成一个又一个的服务。原本，在今天主流的微服务设计里，每一个领域或者子域都是一个服务。而在 Serverless 应用中，这些领域及子域因为他们的功能，又可能会进一步切分成一个又一个 Serverless 函数。</p><p><img src="http://serverless.ink/images/mono-ms-sls.jpg" alt="更小的函数"></p><p>更小的函数</p><p>只是这些服务、函数比以往的粒度更加细致。</p><p>节选自《<a href="https://github.com/phodal/serverless" target="_blank" rel="noopener">Serverless 架构应用开发指南</a> 》</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱在人工智能中的应用</title>
      <link href="/2018/07/11/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%9C%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
      <url>/2018/07/11/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%9C%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>随着互联网的发展，网络数据内容呈现爆炸式增长的态势。由于互联网内容的大规模、异质多元、组织结构松散的特点，给人们有效获取信息和知识提出了挑战。</p><p>知识图谱（Knowledge Graph）以其强大的语义处理能力和开放组织能力，为互联网时代的知识化组织和智能应用奠定了基础。</p><p>知识图谱是知识工程的一个分支，以知识工程中语义网络作为理论基础，并且结合了机器学习，自然语言处理和知识表示和推理的最新成果，在大数据的推动下受到了业界和学术界的广泛关注。</p><p>本文从知识图谱出发，分别浅述了知识图谱的基本概念、知识图谱与人工智能的关系、知识图谱构建技术、知识图谱的在行业中的典型应用，最后对目前的知识图谱技术做出总结并展望。<br><a id="more"></a></p><h3 id="1-知识图谱（Knowledge-Graph）的基本概念"><a href="#1-知识图谱（Knowledge-Graph）的基本概念" class="headerlink" title="1. 知识图谱（Knowledge Graph）的基本概念"></a>1. 知识图谱（Knowledge Graph）的基本概念</h3><font color="#dd0000">知识图谱（Knowledge Graph），是结构化的语义知识库，用于以符号形式描述物理世界中的概念及其相互关系，其基本组成单位是『实体-关系-实体』三元组，以及实体及其相关属性-值对，实体之间通过关系相互联结，构成网状的知识结构。</font><p>其中：</p><ul><li><p>实体：对应现实世界的语义本体</p></li><li><p>关系：对应本体间的关系，连接了不同类型的实体</p></li><li><p>属性：描述一类实体的 common 特性，实体被属性所标注</p></li></ul><p>The world is not made of strings , but is made of things. 知识图谱旨在描述真实世界中存在的各种实体或概念。即知识图谱实现对客观世界从字符串描述到结构化语义描述，是对客观世界的知识映射（mapping world knowledge）。</p><p><strong>知识图谱的核心：知识库</strong></p><p>通过知识图谱，可以实现 Web 从网页链接向概念链接转变，支持用户按主题而不是字符串检索，从而实现真正的语义检索，基于知识图谱的搜索引擎，能够以图形方式向用户反馈结构化的知识，用户不必浏览大量网页，就可以准确定位和深度获取知识。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlR9F4XsFX9fHSXmzRfR6jpvAJcr1SyHwGicR7vRfU9iaK2YENMzQ6MdNw/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图1 知识图谱示例</center><h3 id="2-知识图谱与人工智能的关系"><a href="#2-知识图谱与人工智能的关系" class="headerlink" title="2. 知识图谱与人工智能的关系"></a>2. 知识图谱与人工智能的关系</h3><p>知识图谱对于人工智能的重要价值在于，知识是人工智能的基石。机器可以模仿人类的视觉、听觉等感知能力，但这种感知能力不是人类的专属，动物也具备感知能力，甚至某些感知能力比人类更强，比如狗的嗅觉。</p><p>而 “认知语言是人区别于其他动物的能力，同时，知识也使人不断地进步，不断地凝练、传承知识，是推动人不断进步的重要基础。” 而知识对于人工智能的价值就在于，让机器具备认知能力。</p><p>知识对于 AI 的价值，有了知识的人工智能会变得更强大，可以做更多的事情。反过来，因为更强大的人工智能，可以帮我们更好地从客观世界中去挖掘、获取和沉淀知识，这些知识和人工智能系统形成正循环，两者共同进步。</p><p>机器通过人工智能技术与用户的互动，从中获取数据、优化算法，更重要的是构建和完善知识图谱，认知和理解世界，进而服务于这个世界，让人类的生活更加美好。</p><h3 id="3-知识图谱构建技术"><a href="#3-知识图谱构建技术" class="headerlink" title="3. 知识图谱构建技术"></a>3. 知识图谱构建技术</h3><p>目前知识大量存在于非结构化的文本数据、大量半结构化的表格和网页以及生产系统的结构化数据中。构建知识图谱的主要目的是获取大量的、让计算机可读的知识。</p><p>为了阐述如何构建知识图谱，本节首先给出了构建知识图谱的技术图，如下图所示：</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlNhtic5dS1nTF84Ka36GiaG0KwbUF9LH2qyTEwfoRjttN0AL56iaZHRztQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图2 构建知识图谱技术图</center><p>整个技术图主要分为三个部分：</p><ul><li><p>知识获取：如何从非结构化、半结构化以及结构化数据中获取知识 ；</p></li><li><p>数据融合：如何将不同数据源获取的知识进行融合构建数据之间的关联；</p></li><li><p>知识计算及应用：基于知识图谱计算功能以及知识图谱的应用。</p></li></ul><h4 id="3-1-知识获取"><a href="#3-1-知识获取" class="headerlink" title="3.1 知识获取"></a>3.1 知识获取</h4><p>在处理<strong>非结构化数据</strong>方面，常见的非结构化数据主要是文本类的文章，因此需要通过自然语言技术识别文章中的实体。常见的实体识别方法有两种，分别是：</p><ul><li><p>用户本身有一个知识库则可以使用实体链接到用户的知识库上；</p></li><li><p>当用户没有知识库则需要命名实体识别技术识别文章中的实体。</p></li></ul><p>当用户获得实体后，则需要关注实体间的关系，即实体关系识别。其中有些实体关系识别的方法会利用到句法结构来帮助确定两个实体的关系，因此有些算法中会利用依存分析或者语义解析。</p><p>如果用户不仅仅想获取实体间的关系，还想获取一个事件的详细内容，那么则需要确定事件的触发词并获取事件相应描述的句子，同时识别事件描述句子中实体对应事件的角色。</p><p>在处理<strong>半结构化数据</strong>方面，主要的工作是通过包装器学习半结构化数据的抽取规则。</p><p>由于半结构化数据具有大量的重复性的结构，因此对数据进行少量的标注，可以让机器学出一定的规则进而在整个站点下使用规则对同类型或者符合某种关系的数据进行抽取。</p><p>最后当用户的数据存储在生产系统的数据库中时，需要通过 ETL 工具对用户生产系统下的数据进行重新组织、清洗、检测最后得到符合用户使用目的数据。</p><h4 id="3-2-知识融合"><a href="#3-2-知识融合" class="headerlink" title="3.2 知识融合"></a>3.2 知识融合</h4><p>当知识从各个数据源下获取时需要提供统一的术语将各个数据源获取的知识融合成一个庞大的知识库。</p><p>提供统一术语的结构或者数据被称为<strong>本体</strong>，本体不仅提供了统一的术语字典，还构建了各个术语间的关系以及限制。本体可以让用户非常方便和灵活的根据自己的业务建立或者修改数据模型。</p><p>通过数据映射技术建立本体中术语和不同数据源抽取知识中词汇的映射关系，进而将不同数据源的数据融合在一起。</p><p>同时不同源的实体可能会指向现实世界的同一个客体，这时需要使用实体匹配将不同数据源相同客体的数据进行融合。</p><p>不同本体间也会存在某些术语描述同一类数据，那么对这些本体间则需要本体融合技术把不同的本体融合。最后融合而成的知识库需要一个存储、管理的解决方案。</p><p>知识存储和管理的解决方案会根据用户查询场景的不同采用不同的存储架构如 NoSQL 或者关系数据库。同时大规模的知识库也符合大数据的特征，因此需要传统的大数据平台如 Spark 或者 Hadoop 提供高性能计算能力，支持快速运算。</p><h4 id="3-3-知识计算及应用"><a href="#3-3-知识计算及应用" class="headerlink" title="3.3 知识计算及应用"></a>3.3 知识计算及应用</h4><p>知识计算主要是根据图谱提供的信息得到更多隐含的知识，如通过本体或者规则推理技术可以获取数据中存在的隐含知识；而链接预测则可预测实体间隐含的关系；同时使用社会计算的不同算法在知识网络上计算获取知识图谱上存在的社区，提供知识间关联的路径；通过不一致检测技术发现数据中的噪声和缺陷。</p><p>通过知识计算知识图谱可以产生大量的智能应用如可以提供精确的用户画像为精准营销系统提供潜在的客户；</p><p>提供领域知识给专家系统提供决策数据，给律师、医生、公司 CEO 等提供辅助决策的意见；</p><p>提供更智能的检索方式，使用户可以通过自然语言进行搜索；当然知识图谱也是问答必不可少的重要组建。</p><h3 id="4-知识图谱的在行业中的典型应用"><a href="#4-知识图谱的在行业中的典型应用" class="headerlink" title="4. 知识图谱的在行业中的典型应用"></a>4. 知识图谱的在行业中的典型应用</h3><p>目前，随着人工智能的不断发展，知识图谱已经在搜索引擎、聊天机器人、问答系统、临床决策支持等方面有了一些应用。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlIU7JUUKULcJg3VrB0f4GdR77RzpC767jjBsgXSmYib5zHm0TJsTGAQA/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图3 知识图谱在商业中的应用</center><p>同时为了应对大数据应用的不同挑战，借助知识图谱，实现不同的业务需求。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlXZ8WotiaAMMdiclDib8fianE4otfq2pY7A1efL2aVM1TyeLQvSzKejI74w/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图4 知图谱助力数据分析实现商业智能</center><h4 id="4-1-金融领域"><a href="#4-1-金融领域" class="headerlink" title="4.1 金融领域"></a>4.1 金融领域</h4><ul><li>反欺诈</li></ul><p>通过融合来自不同数据源的信息构成知识图谱，同时引入领域专家建立业务专家规则。我们通过数据不一致性检测，利用绘制出的知识图谱可以识别潜在的欺诈风险。</p><p>比如借款人 UserC 和借款人 UserA 填写信息为同事，但是两个人填写的公司名却不一样, 以及同一个电话号码属于两个借款人，这些不一致性很可能有欺诈行为 。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIld8Xx74zYNC1fMXRAoicEAFj4kkEHDvKjhxELUgib2SsHaBhYPs5qV6iaw/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图5 知图谱在反欺诈中的应用</center><ul><li>智能投顾</li></ul><p>通过知识图谱相关技术从招股书、年报、公司公告、券商研究报告、新闻等半结构化表格和非结构化文本数据中批量自动抽取公司的股东、子公司、供应商、客户、合作伙伴、竞争对手等信息，构建出公司的知识图谱。</p><p>在某个宏观经济事件或者企业相关事件发生的时候，券商分析师、交易员、基金公司基金经理等投资研究人员可以通过此图谱做更深层次的分析和更好的投资决策。</p><p>比如在美国限制向中兴通讯出口的消息发布之后，如果我们有中兴通讯的客户供应商、合作伙伴以及竞争对手的关系图谱，就能在中兴通讯停牌的情况下快速地筛选出受影响的国际国内上市公司从而挖掘投资机会或者进行投资组合风险控制。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlLRRZLK9emjj89VQfa209Vf0q90d8TvicQHibpwdqgxThIxfvmZicT2OrQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图6 知图谱在智能投顾中的应用</center><h4 id="4-2-商业搜索引擎的应用：如百度、搜狗等，国外谷歌"><a href="#4-2-商业搜索引擎的应用：如百度、搜狗等，国外谷歌" class="headerlink" title="4.2 商业搜索引擎的应用：如百度、搜狗等，国外谷歌"></a>4.2 商业搜索引擎的应用：如百度、搜狗等，国外谷歌</h4><ul><li>查询理解</li></ul><p>搜索引擎借助知识图谱来识别查询中涉及到的实体（概念）及其属性等，并根据实体的重要性展现相应的知识卡片。</p><p>搜索引擎并非展现实体的全部属性，而是根据当前输入的查询自动选择最相关的属性及属性值来显示。</p><p>此外，搜索引擎仅当知识卡片所涉及的知识的正确性很高（通常超过 95%，甚至达到 99%）时，才会展现。当要展现的实体被选中之后，利用相关实体挖掘来推荐其他用户可能感兴趣的实体供进一步浏览。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlAvmGNiajAYR9RXBMcEoOGwYm1NWVSg1ux7fZQoX5affPCJQA1QVicibag/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图7 知图谱在搜索引擎（百度）中的应用</center><h4 id="4-3-问答系统的应用：苹果的-Siri"><a href="#4-3-问答系统的应用：苹果的-Siri" class="headerlink" title="4.3 问答系统的应用：苹果的 Siri"></a>4.3 问答系统的应用：苹果的 Siri</h4><p>自动问答目前也是一个非常热门的方向，这可能是面向应用最直接的方式，目前不管是学术界还是工业界都在做相关的研究，这里有两个例子，左边是百度的度秘，右边是苹果的 Siri，可以看到自然语言问答的结果。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlABTG3K16ibLUxKE76U0ta4hLHtGat2xibx2lhricwSYU3icvHcI26lNvVQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图8 知图谱在问答系统中的应用</center><h4 id="4-4-社交网络运用：FB"><a href="#4-4-社交网络运用：FB" class="headerlink" title="4.4 社交网络运用：FB"></a>4.4 社交网络运用：FB</h4><p>社交网站 Facebook 于 2013 年推出了 GraphSearch 产品，其核心技术就是通过知识图谱将人、地点、事情等联系在一起，并以直观的方式支持精确的自然语言查询。</p><p>例如输入查询式：“我朋友喜欢的餐厅”“住在纽约并且喜欢篮球和中国电影的朋友”等，知识图谱会帮助用户在庞大的社交网络中找到与自己最具相关性的人、照片、地点和兴趣等。</p><p>Graph Search 提供的上述服务贴近个人的生活，满足了用户发现知识以及寻找最具相关性的人的需求。</p><p>其中主要功能就是<strong>兴趣推荐</strong>和<strong>用户聚类</strong>。</p><p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/0vU1ia3htaaMuUibzXmF4xKgxkCM3k1wIlXOkdDmicpsZDBKuZJMDb6yqYtHgz6BbP3zf7ZpnxQpa716qYFjRNAMQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p><center>图9 知识图谱在社交领域中的应用</center><h4 id="4-5-电商平台运用：淘宝"><a href="#4-5-电商平台运用：淘宝" class="headerlink" title="4.5 电商平台运用：淘宝"></a>4.5 电商平台运用：淘宝</h4><p>电商网站的主要目的之一就是通过对商品的文字描述、图片展示、相关信息罗列等可视化的知识展现，为消费者提供最满意的购物服务与体验。通过知识图谱，可以提升电商平台的技术性、易用性、交互性等影响用户体验的因素。</p><p>阿里巴巴是应用知识图谱的代表电商网站之一，它旗下的一淘网不仅包含了淘宝数亿的商品，更建立了商品间关联的信息以及从互联网抽取的相关信息，通过整合所有信息，形成了阿里巴巴知识库和产品库，构建了它自身的知识图谱。</p><p>当用户输入关键词查看商品时，知识图谱会为用户提供此次购物方面最相关的信息，包括整合后分类罗列的商品结果、使用建议、搭配等。</p><h4 id="4-6-其他领域"><a href="#4-6-其他领域" class="headerlink" title="4.6 其他领域"></a>4.6 其他领域</h4><p>如教育科研，医疗，生物医疗以及需要进行大数据分析的一些行业。这些行业对整合性和关联性的资源需求迫切，知识图谱可以为其提供更加精确规范的行业数据以及丰富的表达，帮助用户更加便捷地获取行业知识。</p><h3 id="5-知识图谱的总结与展望"><a href="#5-知识图谱的总结与展望" class="headerlink" title="5. 知识图谱的总结与展望"></a>5. 知识图谱的总结与展望</h3><p>知识图谱是知识工程的一个分支，以知识工程中语义网络作为理论基础，并且结合了机器学习，自然语言处理和知识表示和推理的最新成果，在大数据的推动下受到了业界和学术界的广泛关注。</p><p>知识图谱对于解决大数据中文本分析和图像理解问题发挥重要作用。</p><p>当前知识图谱发展还处于初级阶段，面临众多挑战和难题，如：知识库的自动扩展、异构知识处理、推理规则学习、跨语言检索等。</p><p>知识图谱的构建是多学科的结合，需要知识库、自然语言理解，机器学习和数据挖掘等多方面知识的融合。有很多开放性问题需要学术界和业界一起解决。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oracle常用语句合集</title>
      <link href="/2018/06/06/Oracle%E5%B8%B8%E7%94%A8%E8%AF%AD%E5%8F%A5%E5%90%88%E9%9B%86/"/>
      <url>/2018/06/06/Oracle%E5%B8%B8%E7%94%A8%E8%AF%AD%E5%8F%A5%E5%90%88%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="表空间操作"><a href="#表空间操作" class="headerlink" title="表空间操作"></a><center><font color="#dd0000">表空间操作</font></center></h2><h2 id="1、创建表空间"><a href="#1、创建表空间" class="headerlink" title="1、创建表空间"></a>1、创建表空间</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create tablespace asom nologging datafile</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom01.dbf&apos; size 1000M autoextend on next 100M maxsize 20G,</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom02.dbf&apos; size 1000M autoextend on next 100M maxsize 20G,</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom03.dbf&apos; size 1000M autoextend on next 100M maxsize 20G</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="2、新增表空间数据文件"><a href="#2、新增表空间数据文件" class="headerlink" title="2、新增表空间数据文件"></a>2、新增表空间数据文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alter tablespace asom add datafile </span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom04.dbf&apos; size 1000M autoextend on next 100M maxsize 20G,</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom05.dbf&apos; size 1000M autoextend on next 100M maxsize 20G,</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom06.dbf&apos; size 1000M autoextend on next 100M maxsize 20G,</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom07.dbf&apos; size 1000M autoextend on next 100M maxsize 20G,</span><br><span class="line"> &apos;D:\oracle\product\10.2.0\oradata\orcl\asom08.dbf&apos; size 1000M autoextend on next 100M maxsize 20G</span><br></pre></td></tr></table></figure><h2 id="3、查询表空间对应的数据文件"><a href="#3、查询表空间对应的数据文件" class="headerlink" title="3、查询表空间对应的数据文件"></a>3、查询表空间对应的数据文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select tablespace_name,file_id,bytes,file_name from dba_data_files</span><br></pre></td></tr></table></figure><h2 id="4、查询表空间利用率"><a href="#4、查询表空间利用率" class="headerlink" title="4、查询表空间利用率"></a>4、查询表空间利用率</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.tablespace_name as tablespace_name,</span><br><span class="line">       to_char(b.total/1024/1024,999999.99) as Total,</span><br><span class="line">       to_char((b.total-a.free)/1024/1024,999999.99) as Used,</span><br><span class="line">       to_char(a.free/1024/1024,999999.99) as Free,</span><br><span class="line">       to_char(round((total-free)/total,4)*100,999.99) as Used_Rate</span><br><span class="line">FROM (SELECT tablespace_name, sum(bytes) free FROM DBA_FREE_SPACE GROUP BY tablespace_name) a,</span><br><span class="line">     (SELECT tablespace_name, sum(bytes) total FROM DBA_DATA_FILES GROUP BY tablespace_name ) b</span><br><span class="line">WHERE a.tablespace_name=b.tablespace_name</span><br><span class="line">ORDER BY a.tablespace_name;</span><br></pre></td></tr></table></figure><h2 id="5、查询各表的表空间使用情况"><a href="#5、查询各表的表空间使用情况" class="headerlink" title="5、查询各表的表空间使用情况"></a>5、查询各表的表空间使用情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select t.table_name,t.tablespace_name,t.num_rows,a.bytes/1024/1024 &quot;物理空间(M)&quot;, </span><br><span class="line">round(t.num_rows*t.avg_row_len/1024/1024,2) &quot;实际使用(M)&quot;,</span><br><span class="line">a.bytes/1024/1024-round(t.num_rows*t.avg_row_len/1024/1024,2) &quot;差值(M)&quot;</span><br><span class="line">from all_tables t,user_segments a</span><br><span class="line">where t.owner=&apos;ASOM&apos; /*and t.table_name like &apos;MTR%&apos;*/ and t.num_rows&gt;200*10000 </span><br><span class="line">and t.table_name=a.segment_name and a.segment_type = &apos;TABLE&apos;</span><br><span class="line">order by t.table_name;</span><br></pre></td></tr></table></figure><h2 id="6、释放表空间"><a href="#6、释放表空间" class="headerlink" title="6、释放表空间"></a>6、释放表空间</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alter table TABLE_NAME enable row movement;</span><br><span class="line">alter table TABLE_NAME shrink space;</span><br><span class="line"></span><br><span class="line">alter table TABLE_NAME shrink space cascade;   索引也能缩小，效率很低，慎重。</span><br></pre></td></tr></table></figure><h2 id="7、导入表时指定表空间"><a href="#7、导入表时指定表空间" class="headerlink" title="7、导入表时指定表空间"></a>7、导入表时指定表空间</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SQL&gt; create user user01 identified by password default tablespace ts01;</span><br><span class="line"></span><br><span class="line">SQL&gt; grant resource,connect to user01;</span><br><span class="line"></span><br><span class="line">SQL&gt; grant dba to user01;//赋DBA权限</span><br><span class="line"></span><br><span class="line">SQL&gt; revoke unlimited tablespace from user01;//撤销此权限</span><br><span class="line"></span><br><span class="line">SQL&gt; alter user user01 quota 0 on system;//将用户在System表空间的配额置为0</span><br><span class="line"></span><br><span class="line">SQL&gt; alter user user01 quota unlimited on ts01;//设置在用户在myhuang表空间配额不受限。</span><br></pre></td></tr></table></figure><p>经过上述设置后，就可以用imp导入数据，数据将会进入指定的ts01表空间<br><br><br></p><hr><p><br></p><h2 id="物化视图操作"><a href="#物化视图操作" class="headerlink" title="物化视图操作"></a><center><font color="#dd0000">物化视图操作</font></center></h2><h2 id="1、创建物化视图"><a href="#1、创建物化视图" class="headerlink" title="1、创建物化视图"></a>1、创建物化视图</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create materialized view V_MTR_STATIONEQU</span><br><span class="line">refresh force on demand</span><br><span class="line">start with to_date(&apos;04-06-2013 10:27:06&apos;, &apos;dd-mm-yyyy hh24:mi:ss&apos;) next sysdate + 3/(24*6) </span><br><span class="line">as</span><br><span class="line">select * from table_name;</span><br></pre></td></tr></table></figure><h2 id="2、手动刷新物化视图"><a href="#2、手动刷新物化视图" class="headerlink" title="2、手动刷新物化视图"></a>2、手动刷新物化视图</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exec dbms_mview.refresh(&apos;v_mtr_stationequ&apos;);</span><br></pre></td></tr></table></figure><h2 id="3、删除物化视图"><a href="#3、删除物化视图" class="headerlink" title="3、删除物化视图"></a>3、删除物化视图</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop materialized view V_MTR_STATIONEQU</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 原创 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git知识（四）：细节篇</title>
      <link href="/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E7%BB%86%E8%8A%82%E7%AF%87/"/>
      <url>/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E7%BB%86%E8%8A%82%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h1 id="文件状态"><a href="#文件状态" class="headerlink" title="文件状态"></a>文件状态</h1><p>　　现在我们手上已经有了一个真实项目的 Git 仓库，并从这个仓库中取出了所有文件的工作拷贝。接下来，对这些文件作些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。</p><p>　　请记住，工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指本来就被纳入版本控制管理的文件，在上次快照中有它们的记 录，工作一段时间后，它们的状态可能是未更新，已修改或者已放入暂存区。而所有其他文件都属于未跟踪文件。它们既没有上次更新时的快照，也不在当前的暂存 区域。初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改。<br><a id="more"></a><br>　　在编辑过某些文件之后，Git 将这些文件标为已修改。我们逐步把这些修改过的文件放到暂存区域，直到最后一次性提交所有这些暂存起来的文件，如此重复。所以使用 Git 时的文件状态变化周期如图 2-1 所示。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121432_845.png" alt="Git详解之二 Git基础 "></p><p>　　　　　　　　　　　　<center>图. 文件的状态变化周期</center></p><h2 id="检查当前文件状态"><a href="#检查当前文件状态" class="headerlink" title="检查当前文件状态"></a>检查当前文件状态</h2><p>　　要确定哪些文件当前处于什么状态，可以用 git status 命令。如果在克隆仓库之后立即执行此命令，会看到类似这样的输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line">nothing to commit (working directory clean)</span><br></pre></td></tr></table></figure></p><p>　　这说明你现在的工作目录相当干净。换句话说，当前没有任何跟踪着的文件，也没有任何文件在上次提交后更改过。此外，上面的信息还表明，当前目录下没 有出现任何处于未跟踪的新文件，否则 Git 会在这里列出来。最后，该命令还显示了当前所在的分支是 master，这是默认的分支名称，实际是可以修改的，现在先不用考虑。下一章我们就会详细讨论分支和引用。</p><p>　　现在让我们用 vim 编辑一个新文件 README，保存退出后运行 <code>git status</code> 会看到该文件出现在未跟踪文件列表中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vim README</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Untracked files:</span><br><span class="line">#   (use &quot;git add ...&quot; to include in what will be committed)</span><br><span class="line">#</span><br><span class="line">#    README</span><br><span class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</span><br></pre></td></tr></table></figure><h2 id="跟踪新文件"><a href="#跟踪新文件" class="headerlink" title="跟踪新文件"></a>跟踪新文件</h2><p>　　使用命令 <code>git add</code> 开始跟踪一个新文件。所以，要跟踪 README 文件，运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git add README</span><br></pre></td></tr></table></figure></p><p>　　此时再运行 <code>git status</code> 命令，会看到 README 文件已被跟踪，并处于暂存状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    new file:   README</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　只要在 “Changes to be committed” 这行下面的，就说明是已暂存状态。如果此时提交，那么该文件此时此刻的版本将被留存在历史记录中。你可能会想起之前我们使用<code>git init</code> 后就运行了 <code>git add</code> 命令，开始跟踪当前目录下的文件。在 <code>git add</code> 后面可以指明要跟踪的文件或目录路径。如果是目录的话，就说明要递归跟踪该目录下的所有文件。（译注：其实<code>git add</code> 的潜台词就是把目标文件快照放入暂存区域，也就是 add file into staged area，同时未曾跟踪过的文件标记为需要跟踪。这样就好理解后续 add 操作的实际意义了。）</p><h2 id="暂存已修改文件"><a href="#暂存已修改文件" class="headerlink" title="暂存已修改文件"></a>暂存已修改文件</h2><p>　　现在我们修改下之前已跟踪过的文件 <code>benchmarks.rb</code>，然后再次运行 <code>status</code> 命令，会看到这样的状态报告：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    new file:   README</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add ...&quot; to update what will be committed)</span><br><span class="line">#</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　文件 benchmarks.rb 出现在 “Changed but not updated” 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。要暂存这次更新，需要运行<code>git add</code> 命令（这是个多功能命令，根据目标文件的状态不同，此命令的效果也不同：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等）。现在让我们运行<code>git add</code> 将 benchmarks.rb 放到暂存区，然后再看看 <code>git status</code> 的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git add benchmarks.rb</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    new file:   README</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　现在两个文件都已暂存，下次提交时就会一并记录到仓库。假设此时，你想要在 benchmarks.rb 里再加条注释，重新编辑存盘后，准备好提交。不过且慢，再运行<code>git status</code> 看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ vim benchmarks.rb </span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    new file:   README</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add ...&quot; to update what will be committed)</span><br><span class="line">#</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　怎么回事？benchmarks.rb 文件出现了两次！一次算未暂存，一次算已暂存，这怎么可能呢？好吧，实际上 Git 只不过暂存了你运行 git add 命令时的版本，如果现在提交，那么提交的是添加注释前的版本，而非当前工作目录中的版本。所以，运行了<code>git add</code> 之后又作了修订的文件，需要重新运行 <code>git add</code> 把最新版本重新暂存起来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git add benchmarks.rb</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    new file:   README</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><h1 id="忽略某些文件"><a href="#忽略某些文件" class="headerlink" title="忽略某些文件"></a>忽略某些文件</h1><p>　　一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。来看一个实际的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat .gitignore </span><br><span class="line">*.[oa] </span><br><span class="line">*~</span><br></pre></td></tr></table></figure></p><p>　　第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的，我们用不着跟踪它们的版本。第二行告诉 Git 忽略所有以波浪符（<code>~</code>）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。</p><p>　　文件 .gitignore 的格式规范如下：</p><ul><li>所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。</li><li>可以使用标准的 glob 模式匹配。 <em> 匹配模式最后跟反斜杠（<code>/</code>）说明要忽略的是目录。 </em> 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（<code>!</code>）取反。</li></ul><p>　　所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。星号（<code>*</code>）匹配零个或多个任意字符；<code>[abc]</code> 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（<code>?</code>）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如<code>[0-9]</code> 表示匹配所有 0 到 9 的数字）。</p><p>我们再看一个 .gitignore 文件的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 此为注释 – 将被 Git 忽略 </span><br><span class="line">*.a       # 忽略所有 .a 结尾的文件 </span><br><span class="line">!lib.a    # 但 lib.a 除外 </span><br><span class="line">/TODO     # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO</span><br><span class="line">build/    # 忽略 build/ 目录下的所有文件</span><br><span class="line">doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt</span><br></pre></td></tr></table></figure></p><h2 id="查看已暂存和未暂存的更新"><a href="#查看已暂存和未暂存的更新" class="headerlink" title="查看已暂存和未暂存的更新"></a>查看已暂存和未暂存的更新</h2><p>　　实际上 <code>git status</code> 的显示比较简单，仅仅是列出了修改过的文件，如果要查看具体修改了什么地方，可以用 <code>git diff</code> 命令。稍后我们会详细介绍<code>git diff</code>，不过现在，它已经能回答我们的两个问题了：当前做的哪些更新还没有暂存？有哪些更新已经暂存起来准备好了下次提交？ <code>git diff</code> 会使用文件补丁的格式显示具体添加和删除的行。</p><p>　　假如再次修改 README 文件后暂存，然后编辑 benchmarks.rb 文件后先别暂存，运行 <code>status</code>命令，会看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    new file:   README</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add ...&quot; to update what will be committed)</span><br><span class="line">#</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 <code>git diff</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git diff</span><br><span class="line">diff --git a/benchmarks.rb b/benchmarks.rb</span><br><span class="line">index 3cb747f..da65585 100644</span><br><span class="line">--- a/benchmarks.rb +++ b/benchmarks.rb</span><br><span class="line">@@ -36,6 +36,10 @@ def main</span><br><span class="line">           @commit.parents[0].parents[0].parents[0]</span><br><span class="line">         end +        run_code(x, &apos;commits 1&apos;) do</span><br><span class="line">+ git.commits.size + end + run_code(x, &apos;commits 2&apos;) do log = git.commits(&apos;master&apos;, 15)</span><br><span class="line">           log.size</span><br></pre></td></tr></table></figure><p>　　此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。</p><p>　　若要看已经暂存起来的文件和上次提交时的快照之间的差异，可以用 <code>git diff --cached</code> 命令。（Git 1.6.1 及更高版本还允许使用<code>git diff --staged</code>，效果是相同的，但更好记些。）来看看实际的效果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git diff --cached diff --git a/README b/README</span><br><span class="line">new file mode 100644 index 0000000..03902a1 --- /dev/null</span><br><span class="line">+++ b/README2</span><br><span class="line">@@ -0,0 +1,5 @@ +grit + by Tom Preston-Werner, Chris Wanstrath + http://github.com/mojombo/grit</span><br><span class="line">+</span><br><span class="line">+Grit is a Ruby library for extracting information from a Git repository</span><br></pre></td></tr></table></figure><p>　　请注意，单单 <code>git diff</code> 不过是显示还没有暂存起来的改动，而不是这次工作和上次提交之间的差异。所以有时候你一下子暂存了所有更新过的文件后，运行<code>git diff</code> 后却什么也没有，就是这个原因。</p><p>　　像之前说的，暂存 benchmarks.rb 后再编辑，运行 <code>git status</code> 会看到暂存前后的两个版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ git add benchmarks.rb</span><br><span class="line">$ echo &apos;# test line&apos; &gt;&gt; benchmarks.rb</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line">#</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　现在运行 <code>git diff</code> 看暂存前后的变化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git diff</span><br><span class="line">diff --git a/benchmarks.rb b/benchmarks.rb</span><br><span class="line">index e445e28..86b2f7c 100644</span><br><span class="line">--- a/benchmarks.rb +++ b/benchmarks.rb</span><br><span class="line">@@ -127,3 +127,4 @@ end</span><br><span class="line"> main()</span><br><span class="line"> </span><br><span class="line"> ##pp Grit::GitRuby.cache_client.stats </span><br><span class="line"> +# test line</span><br></pre></td></tr></table></figure><p>然后用 <code>git diff --cached</code> 查看已经暂存起来的变化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git diff --cached diff --git a/benchmarks.rb b/benchmarks.rb</span><br><span class="line">index 3cb747f..e445e28 100644</span><br><span class="line">--- a/benchmarks.rb +++ b/benchmarks.rb</span><br><span class="line">@@ -36,6 +36,10 @@ def main</span><br><span class="line">          @commit.parents[0].parents[0].parents[0]</span><br><span class="line">        end +        run_code(x, &apos;commits 1&apos;) do</span><br><span class="line">+ git.commits.size + end + run_code(x, &apos;commits 2&apos;) do log = git.commits(&apos;master&apos;, 15)</span><br><span class="line">          log.size</span><br></pre></td></tr></table></figure><h3 id="提交更新"><a href="#提交更新" class="headerlink" title="提交更新"></a>提交更新</h3><p>　　现在的暂存区域已经准备妥当可以提交了。在此之前，请一定要确认还有什么修改过的或新建的文件还没有 <code>git add</code> 过，否则提交的时候不会记录这些还没暂存起来的变化。所以，每次准备提交前，先用<code>git status</code> 看下，是不是都已暂存起来了，然后再运行提交命令 <code>git commit</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit</span><br></pre></td></tr></table></figure><p>　　这种方式会启动文本编辑器以便输入本次提交的说明。（默认会启用 shell 的环境变量 <code>$EDITOR</code> 所指定的软件，一般都是 vim 或 emacs。当然也可以按照第一章介绍的方式，使用<code>git config --global core.editor</code> 命令设定你喜欢的编辑软件。）</p><p>编辑器会显示类似下面的文本信息（本例选用 Vim 的屏显方式展示）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Please enter the commit message for your changes. Lines starting</span><br><span class="line"># with &apos;#&apos; will be ignored, and an empty message aborts the commit.</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       new file:   README</span><br><span class="line">#       modified:   benchmarks.rb ~</span><br><span class="line">~</span><br><span class="line">~</span><br><span class="line">&quot;.git/COMMIT_EDITMSG&quot; 10L, 283C</span><br></pre></td></tr></table></figure><p>可以看到，默认的提交消息包含最后一次运行 <code>git status</code> 的输出，放在注释行里，另外开头还有一空行，供你输入提交说明。你完全可以去掉这些注释行，不过留着也没关系，多少能帮你回想起这次更新的内容有哪些。（如果觉得这还不够，可以用<code>-v</code> 选项将修改差异的每一行都包含到注释中来。）退出编辑器时，Git 会丢掉注释行，将说明内容和本次更新提交到仓库。</p><p>另外也可以用 -m 参数后跟提交说明的方式，在一行命令中提交更新：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;Story 182: Fix benchmarks for speed&quot; </span><br><span class="line">[master]: created 463dc4f: &quot;Fix benchmarks for speed&quot;</span><br><span class="line"> 2 files changed, 3 insertions(+), 0 deletions(-)</span><br><span class="line"> create mode 100644 README</span><br></pre></td></tr></table></figure></p><p>　　好，现在你已经创建了第一个提交！可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（<code>463dc4f</code>），以及在本次提交中，有多少文件修订过，多少行添改和删改过。</p><p>　　记住，提交时记录的是放在暂存区域的快照，任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。</p><h2 id="跳过使用暂存区域"><a href="#跳过使用暂存区域" class="headerlink" title="跳过使用暂存区域"></a>跳过使用暂存区域</h2><p>　　尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。Git 提供了一个跳过使用暂存区域的方式，只要在提交的时候，给 <code>git commit</code> 加上<code>-a</code> 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 <code>git add</code> 步骤：</p><p>　　看到了吗？提交之前不再需要 <code>git add</code> 文件 benchmarks.rb 了。</p><h2 id="移除文件"><a href="#移除文件" class="headerlink" title="移除文件"></a>移除文件</h2><p>　　要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用 <code>git rm</code> 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。</p><p>如果只是简单地从工作目录中手工删除文件，运行 <code>git status</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#</span><br><span class="line">#    modified:   benchmarks.rb</span><br><span class="line">#</span><br><span class="line">$ git commit -a -m &apos;added new benchmarks&apos; </span><br><span class="line">[master 83e38c7] added new benchmarks </span><br><span class="line">1 files changed, 5 insertions(+), 0 deletions(-)</span><br></pre></td></tr></table></figure><p>　　时就会在 “Changed but not updated” 部分（也就是<em>未暂存</em>清单）看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ rm grit.gemspec</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add/rm ...&quot; to update what will be committed)</span><br><span class="line">#</span><br><span class="line">#       deleted:    grit.gemspec</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　然后再运行 <code>git rm</code> 记录此次移除文件的操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git rm grit.gemspec rm &apos;grit.gemspec&apos; $ git status</span><br><span class="line"># On branch master</span><br><span class="line">#</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       deleted:    grit.gemspec</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　最后提交的时候，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 <code>-f</code>（译注：即 force 的首字母），以防误删除文件后丢失修改的内容。</p><p>　　另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。换句话说，仅是从跟踪清单中删除。比如一些大型日志文件或者一堆<code>.a</code> 编译文件，不小心纳入仓库后，要移除跟踪但不删除文件，以便稍后在 <code>.gitignore</code> 文件中补上，用 <code>--cached</code> 选项即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rm --cached readme.txt</span><br></pre></td></tr></table></figure></p><p>后面可以列出文件或者目录的名字，也可以使用 glob 模式。比方说：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rm log/\*.log</span><br></pre></td></tr></table></figure></p><p>　　注意到星号 <code>*</code> 之前的反斜杠 <code>\</code>，因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开（译注：实际上不加反斜杠也可以运行，只不过按照 shell 扩展的话，仅仅删除指定目录下的文件而不会递归匹配。上面的例子本来就指定了目录，所以效果等同，但下面的例子就会用递归方式匹配，所以必须加反斜 杠。）。此命令删除所有<code>log/</code> 目录下扩展名为 <code>.log</code> 的文件。类似的比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rm \*~</span><br></pre></td></tr></table></figure></p><p>　　会递归删除当前目录及其子目录中所有 <code>~</code> 结尾的文件。</p><h2 id="移动文件"><a href="#移动文件" class="headerlink" title="移动文件"></a>移动文件</h2><p>　　不像其他的 VCS 系统，Git 并不跟踪文件移动操作。如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。不过 Git 非常聪明，它会推断出究竟发生了什么，至于具体是如何做到的，我们稍后再谈。</p><p>　　既然如此，当你看到 Git 的 <code>mv</code> 命令时一定会困惑不已。要在 Git 中对文件改名，可以这么做：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git mv file_from file_to</span><br></pre></td></tr></table></figure></p><p>　　它会恰如预期般正常工作。实际上，即便此时查看状态信息，也会明白无误地看到关于重命名操作的说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ git mv README.txt README</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Your branch is ahead of &apos;origin/master&apos; by 1 commit.</span><br><span class="line">#</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD..&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       renamed:    README.txt -&gt; README</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　其实，运行 <code>git mv</code> 就相当于运行了下面三条命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mv README.txt README</span><br><span class="line">$ git rm README.txt</span><br><span class="line">$ git add README</span><br></pre></td></tr></table></figure></p><p>　　如此分开操作，Git 也会意识到这是一次改名，所以不管何种方式都一样。当然，直接用 <code>git mv</code>轻便得多，不过有时候用其他工具批处理改名的话，要记得在提交前删除老的文件名，再添加新的文件名。</p><h1 id="查看提交历史"><a href="#查看提交历史" class="headerlink" title="查看提交历史"></a>查看提交历史</h1><p>　　在提交了若干更新之后，又或者克隆了某个项目，想回顾下提交历史，可以使用 <code>git log</code> 命令查看。</p><p>　　接下来的例子会用我专门用于演示的 simplegit 项目，运行下面的命令获取该项目源代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git://github.com/schacon/simplegit-progit.git</span><br></pre></td></tr></table></figure></p><p>　　然后在此项目中运行 <code>git log</code>，应该会看到下面的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ git log</span><br><span class="line">commit ca82a6dff817ec66f44342007202690a93763949</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Mon Mar 17 21:52:11 2008 -0700 </span><br><span class="line">    changed the version number</span><br><span class="line"></span><br><span class="line">commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sat Mar 15 16:40:33 2008 -0700 </span><br><span class="line">    removed unnecessary test code</span><br><span class="line"></span><br><span class="line">commit a11bef06a3f659402fe7563abf99ad00de2209e6</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sat Mar 15 10:31:28 2008 -0700 </span><br><span class="line">    first commit</span><br></pre></td></tr></table></figure><p>　　默认不用任何参数的话，<code>git log</code> 会按提交时间列出所有的更新，最近的更新排在最上面。看到了吗，每次更新都有一个 SHA-1 校验和、作者的名字和电子邮件地址、提交时间，最后缩进一个段落显示提交说明。</p><p><code>git log</code> 有许多选项可以帮助你搜寻感兴趣的提交，接下来我们介绍些最常用的。</p><p>　　我们常用 <code>-p</code> 选项展开显示每次提交的内容差异，用 <code>-2</code> 则仅显示最近的两次更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ git log -p -2 commit ca82a6dff817ec66f44342007202690a93763949</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Mon Mar 17 21:52:11 2008 -0700 </span><br><span class="line">    changed the version number </span><br><span class="line">diff --git a/Rakefile b/Rakefile</span><br><span class="line">index a874b73..8f94139 100644</span><br><span class="line">--- a/Rakefile +++ b/Rakefile</span><br><span class="line">@@ -5,7 +5,7 @@ require &apos;rake/gempackagetask&apos; </span><br><span class="line">spec = Gem::Specification.new do |s|</span><br><span class="line">-    s.version   =   &quot;0.1.0&quot;</span><br><span class="line">+    s.version   =   &quot;0.1.1&quot; s.author =   &quot;Scott Chacon&quot; </span><br><span class="line">commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sat Mar 15 16:40:33 2008 -0700 </span><br><span class="line">    removed unnecessary test code </span><br><span class="line">diff --git a/lib/simplegit.rb b/lib/simplegit.rb</span><br><span class="line">index a0a60ae..47c6340 100644</span><br><span class="line">--- a/lib/simplegit.rb +++ b/lib/simplegit.rb</span><br><span class="line">@@ -18,8 +18,3 @@ class SimpleGit</span><br><span class="line">     end</span><br><span class="line"></span><br><span class="line"> end -</span><br><span class="line">-if $0 == __FILE__ </span><br><span class="line">-  git = SimpleGit.new </span><br><span class="line">- puts git.show </span><br><span class="line">-end</span><br><span class="line">\ No newline at end of file</span><br></pre></td></tr></table></figure><p> 在做代码审查，或者要快速浏览其他协作者提交的更新都作了哪些改动时，就可以用这个选项。此外，还有许多摘要选项可以用，比如 –stat，仅显示简要的增改行数统计：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> $ git log --stat </span><br><span class="line"> commit ca82a6dff817ec66f44342007202690a93763949</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Mon Mar 17 21:52:11 2008 -0700 </span><br><span class="line">    changed the version number</span><br><span class="line"></span><br><span class="line"> Rakefile |    2 +-</span><br><span class="line"> 1 files changed, 1 insertions(+), 1 deletions(-)</span><br><span class="line"></span><br><span class="line">commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sat Mar 15 16:40:33 2008 -0700 </span><br><span class="line">    removed unnecessary test code</span><br><span class="line"></span><br><span class="line"> lib/simplegit.rb |    5 -----</span><br><span class="line"> 1 files changed, 0 insertions(+), 5 deletions(-)</span><br><span class="line"></span><br><span class="line">commit a11bef06a3f659402fe7563abf99ad00de2209e6</span><br><span class="line">Author: Scott Chacon </span><br><span class="line">    &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sat Mar 15 10:31:28 2008 -0700 </span><br><span class="line">    first commit</span><br><span class="line"></span><br><span class="line"> README |    6 ++++++ </span><br><span class="line"> Rakefile |   23 +++++++++++++++++++++++ </span><br><span class="line"> lib/simplegit.rb |   25 +++++++++++++++++++++++++</span><br><span class="line"> 3 files changed, 54 insertions(+), 0 deletions(-)</span><br></pre></td></tr></table></figure><p>　　每个提交都列出了修改过的文件，以及其中添加和移除的行数，并在最后列出所有增减行数小计。还有个常用的 <code>--pretty</code> 选项，可以指定使用完全不同于默认格式的方式展示提交历史。比如用<code>oneline</code> 将每个提交放在一行显示，这在提交数很大时非常有用。另外还有 <code>short</code>，<code>full</code> 和<code>fuller</code> 可以用，展示的信息或多或少有些不同，请自己动手实践一下看看效果如何。</p><p>　　但最有意思的是 <code>format</code>，可以定制要显示的记录格式，这样的输出便于后期编程提取分析，像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git log --pretty=format:&quot;%h - %an, %ar : %s&quot;</span><br><span class="line">$ git log --pretty=oneline</span><br><span class="line">ca82a6dff817ec66f44342007202690a93763949 changed the version number</span><br><span class="line">085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary test code</span><br><span class="line">a11bef06a3f659402fe7563abf99ad00de2209e6 first commit  </span><br><span class="line">ca82a6d - Scott Chacon, 11 months ago : changed the version number</span><br><span class="line">085bb3b - Scott Chacon, 11 months ago : removed unnecessary test code</span><br><span class="line">a11bef0 - Scott Chacon, 11 months ago : first commit</span><br></pre></td></tr></table></figure><p>　　表 列出了常用的格式占位符写法及其代表的意义。</p><p>　你一定奇怪<em>作者（author）</em>和<em>提交者（committer）</em>之间究竟有何差别，其实作者指的是实际作出修改的人，提交者指的是最后将此 工作成果提交到仓库的人。所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。我们会在第五章 再详细介绍两者之间的细微差别。</p><p>　　用 oneline 或 format 时结合 <code>--graph</code> 选项，可以看到开头多出一些 ASCII 字符串表示的简单图形，形象地展示了每个提交所在的分支及其分化衍合情况。在我们之前提到的 Grit 项目仓库中可以看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ git log --pretty=format:&quot;%h %s&quot; --graph</span><br><span class="line">* 2d3acf9 ignore errors from SIGCHLD on trap</span><br><span class="line">*  5e3ee11 Merge branch &apos;master&apos; of git://github.com/dustin/grit</span><br><span class="line">|\</span><br><span class="line">| * 420eac9 Added a method for getting the current branch.</span><br><span class="line">* | 30e367c timeout code and tests</span><br><span class="line">* | 5a09431 add timeout protection to grit</span><br><span class="line">* | e1193f8 support for heads with slashes in them</span><br><span class="line">|/</span><br><span class="line">* d6016bc require time for xmlschema</span><br><span class="line">*  11d191e Merge b</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">选项     说明 </span><br><span class="line">%H    提交对象（commit）的完整哈希字串 </span><br><span class="line">%h    提交对象的简短哈希字串 </span><br><span class="line">%T    树对象（tree）的完整哈希字串 </span><br><span class="line">%t    树对象的简短哈希字串 </span><br><span class="line">%P    父对象（parent）的完整哈希字串 </span><br><span class="line">%p    父对象的简短哈希字串 </span><br><span class="line">%an   作者（author）的名字 </span><br><span class="line">%ae   作者的电子邮件地址 </span><br><span class="line">%ad   作者修订日期（可以用 -date= 选项定制格式） </span><br><span class="line">%ar   作者修订日期，按多久以前的方式显示 </span><br><span class="line">%cn   提交者(committer)的名字 </span><br><span class="line">%ce   提交者的电子邮件地址 </span><br><span class="line">%cd   提交日期 </span><br><span class="line">%cr   提交日期，按多久以前的方式显示 </span><br><span class="line">%s    提交说明</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ranch &apos;defunkt&apos; into local</span><br></pre></td></tr></table></figure><p>以上只是简单介绍了一些 <code>git log</code> 命令支持的选项。表 2-2 还列出了一些其他常用的选项及其释义。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">选项 说明 </span><br><span class="line">-p 按补丁格式显示每个更新之间的差异。 </span><br><span class="line">--stat 显示每次更新的文件修改统计信息。 </span><br><span class="line">--shortstat 只显示 </span><br><span class="line">--stat 中最后的行数修改添加移除统计。 </span><br><span class="line">--name-only 仅在提交信息后显示已修改的文件清单。 </span><br><span class="line">--name-status 显示新增、修改、删除的文件清单。 </span><br><span class="line">--abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 </span><br><span class="line">--relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 </span><br><span class="line">--graph 显示 ASCII 图形表示的分支合并历史。 </span><br><span class="line">--pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。</span><br></pre></td></tr></table></figure><h3 id="限制输出长度"><a href="#限制输出长度" class="headerlink" title="限制输出长度"></a>限制输出长度</h3><p>　　除了定制输出格式的选项之外，<code>git log</code> 还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。之前我们已经看到过 <code>-2</code> 了，它只显示最近的两条提交，实际上，这是 <code>-</code> 选项的写法，其中的 <code>n</code> 可以是任何自然数，表示仅显示最近的若干条提交。不过实践中我们是不太用这个选项的，Git 在输出所有提交时会自动调用分页程序（less），要看更早的更新只需翻到下页即可。</p><p>　　另外还有按照时间作限制的选项，比如 <code>--since</code> 和 <code>--until</code>。下面的命令列出所有最近两周内的提交：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log --since=2.weeks</span><br></pre></td></tr></table></figure><p>　　你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。</p><p>　　还可以给出若干搜索条件，列出符合的提交。用 <code>--author</code> 选项显示指定作者的提交，用 <code>--grep</code>选项搜索提交说明中的关键字。（请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用<code>--all-match</code> 选项。）</p><p>　　如果只关心某些文件或者目录的历史提交，可以在 <code>git log</code> 选项的最后指定它们的路径。因为是放在最后位置上的选项，所以用两个短划线（<code>--</code>）隔开之前的选项和后面限定的路径名。</p><p>表 2-3 还列出了其他常用的类似选项。</p><p>选项 说明 -(n)    仅显示最近的 n 条提交 –since, –after 仅显示指定时间之后的提交。 –until, –before 仅显示指定时间之前的提交。 –author 仅显示指定作者相关的提交。 –committer 仅显示指定提交者相关的提交。</p><p>　　来看一个实际的例子，如果要查看 Git 仓库中，2008 年 10 月期间，Junio Hamano 提交的但未合并的测试脚本（位于项目的 t/ 目录下的文件），可以用下面的查询命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git log --pretty=&quot;%h - %s&quot; --author=gitster --since=&quot;2008-10-01&quot; \ </span><br><span class="line">   --before=&quot;2008-11-01&quot; --no-merges -- t/ </span><br><span class="line">5610e3b - Fix testcase failure when extended attribute</span><br><span class="line">acd3b9e - Enhance hold_lock_file_for_&#123;update,append&#125;()</span><br><span class="line">f563754 - demonstrate breakage of detached checkout wi</span><br><span class="line">d1a43f2 - reset --hard/read-tree --reset -u: remove un</span><br><span class="line">51a94af - Fix &quot;checkout --track -b newbranch&quot; on detac</span><br><span class="line">b0ad11e - pull: allow &quot;git pull origin $something:$cur</span><br></pre></td></tr></table></figure><p>　　Git 项目有 20,000 多条提交，但我们给出搜索选项后，仅列出了其中满足条件的 6 条。</p><h3 id="使用图形化工具查阅提交历史"><a href="#使用图形化工具查阅提交历史" class="headerlink" title="使用图形化工具查阅提交历史"></a>使用图形化工具查阅提交历史</h3><p>　　有时候图形化工具更容易展示历史提交的变化，随 Git 一同发布的 gitk 就是这样一种工具。它是用 Tcl/Tk 写成的，基本上相当于 <code>git log</code> 命令的可视化版本，凡是<code>git log</code> 可以用的选项也都能用在 gitk 上。在项目工作目录中输入 gitk 命令后，就会启动图 2-2 所示的界面。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121433_829.png" alt="Git详解之二 Git基础 "></p><p>　　　　　　　　　　　　　　　　　　图 gitk 的图形界面</p><p>上半个窗口显示的是历次提交的分支祖先图谱，下半个窗口显示当前点选的提交对应的具体差异。</p><h1 id="撤消操作"><a href="#撤消操作" class="headerlink" title="撤消操作"></a>撤消操作</h1><p>　　任何时候，你都有可能需要撤消刚才所做的某些操作。接下来，我们会介绍一些基本的撤消操作相关的命令。请注意，有些操作并不总是可以撤消的，所以请务必谨慎小心，一旦失误，就有可能丢失部分工作成果。</p><h3 id="修改最后一次提交"><a href="#修改最后一次提交" class="headerlink" title="修改最后一次提交"></a>修改最后一次提交</h3><p>　　有时候我们提交完了才发现漏掉了几个文件没有加，或者提交信息写错了。想要撤消刚才的提交操作，可以使用 <code>--amend</code> 选项重新提交：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git commit --amend</span><br></pre></td></tr></table></figure></p><p>　　此命令将使用当前的暂存区域快照提交。如果刚才提交完没有作任何改动，直接运行此命令的话，相当于有机会重新编辑提交说明，但将要提交的文件快照和之前的一样。</p><p>　　启动文本编辑器后，会看到上次提交时的说明，编辑它确认没问题后保存退出，就会使用新的提交说明覆盖刚才失误的提交。</p><p>　　如果刚才提交时忘了暂存某些修改，可以先补上暂存操作，然后再运行 <code>--amend</code> 提交：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &apos;initial commit&apos; $ git add forgotten_file</span><br><span class="line">$ git commit --amend</span><br></pre></td></tr></table></figure></p><p>　　上面的三条命令最终只是产生一个提交，第二个提交命令修正了第一个的提交内容。</p><h3 id="取消已经暂存的文件"><a href="#取消已经暂存的文件" class="headerlink" title="取消已经暂存的文件"></a>取消已经暂存的文件</h3><p>　　接下来的两个小节将演示如何取消暂存区域中的文件，以及如何取消工作目录中已修改的文件。不用担心，查看文件状态的时候就提示了该如何撤消，所以不需要死记硬背。来看下面的例子，有两个修改过的文件，我们想要分开提交，但不小心用<code>git add .</code> 全加到了暂存区域。该如何撤消暂存其中的一个文件呢？其实，<code>git status</code> 的命令输出已经告诉了我们该怎么做：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       modified:   README.txt</span><br><span class="line">#       modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　就在 “Changes to be committed” 下面，括号中有提示，可以使用 <code>git reset HEAD ...</code> 的方式取消暂存。好吧，我们来试试取消暂存 benchmarks.rb 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ git reset HEAD benchmarks.rb</span><br><span class="line">benchmarks.rb: locally modified</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       modified:   README.txt</span><br><span class="line">#</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add ...&quot; to update what will be committed)</span><br><span class="line">#   (use &quot;git checkout -- ...&quot; to discard changes in working directory)</span><br><span class="line">#</span><br><span class="line">#       modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　这条命令看起来有些古怪，先别管，能用就行。现在 benchmarks.rb 文件又回到了之前已修改未暂存的状态。</p><h3 id="取消对文件的修改"><a href="#取消对文件的修改" class="headerlink" title="取消对文件的修改"></a>取消对文件的修改</h3><p>　　如果觉得刚才对 benchmarks.rb 的修改完全没有必要，该如何取消修改，回到之前的状态（也就是修改之前的版本）呢？<code>git status</code> 同样提示了具体的撤消方法，接着上面的例子，现在未暂存区域看起来像这样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add ...&quot; to update what will be committed)</span><br><span class="line">#   (use &quot;git checkout -- ...&quot; to discard changes in working directory)</span><br><span class="line">#</span><br><span class="line">#       modified:   benchmarks.rb</span><br><span class="line">#</span><br></pre></td></tr></table></figure></p><p>　　在第二个括号中，我们看到了抛弃文件修改的命令（至少在 Git 1.6.1 以及更高版本中会这样提示，如果你还在用老版本，我们强烈建议你升级，以获取最佳的用户体验），让我们试试看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -- benchmarks.rb</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       modified:   README.txt</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　可以看到，该文件已经恢复到修改前的版本。你可能已经意识到了，这条命令有些危险，所有对文件的修改都没有了，因为我们刚刚把之前版本的文件复制过 来重写了此文件。所以在用这条命令前，请务必确定真的不再需要保留刚才的修改。如果只是想回退版本，同时保留刚才的修改以便将来继续工作，可以用下章介绍 的 stashing 和分支来处理，应该会更好些。</p><p>　　记住，任何已经提交到 Git 的都可以被恢复。即便在已经删除的分支中的提交，或者用 <code>--amend</code>重新改写的提交，都可以被恢复（关于数据恢复的内容见第九章）。所以，你可能失去的数据，仅限于没有提交过的，对 Git 来说它们就像从未存在过一样。</p><h1 id="远程仓库的使用"><a href="#远程仓库的使用" class="headerlink" title="远程仓库的使用"></a>远程仓库的使用</h1><p>　　要参与任何一个 Git 项目的协作，必须要了解该如何管理远程仓库。远程仓库是指托管在网络上的项目仓库，可能会有好多个，其中有些你只能读，另外有些可以写。同他人协作开发某 个项目时，需要管理这些远程仓库，以便推送或拉取数据，分享各自的工作进展。管理远程仓库的工作，包括添加远程库，移除废弃的远程库，管理各式远程库分 支，定义是否跟踪这些分支，等等。本节我们将详细讨论远程库的管理和使用。</p><h2 id="查看当前的远程库"><a href="#查看当前的远程库" class="headerlink" title="查看当前的远程库"></a>查看当前的远程库</h2><p>　　要查看当前配置有哪些远程仓库，可以用 <code>git remote</code> 命令，它会列出每个远程库的简短名字。在克隆完某个项目后，至少可以看到一个名为 origin 的远程库，Git 默认使用这个名字来标识你所克隆的原始仓库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git clone git://github.com/schacon/ticgit.git</span><br><span class="line">Initialized empty Git repository in /private/tmp/ticgit/.git/ remote: Counting objects: 595, done.</span><br><span class="line">remote: Compressing objects: 100% (269/269), done.</span><br><span class="line">remote: Total 595 (delta 255), reused 589 (delta 253)</span><br><span class="line">Receiving objects: 100% (595/595), 73.31 KiB | 1 KiB/s, done.</span><br><span class="line">Resolving deltas: 100% (255/255), done.</span><br><span class="line">$ cd ticgit</span><br><span class="line">$ git remote</span><br><span class="line">origin</span><br></pre></td></tr></table></figure><p>　　也可以加上 <code>-v</code> 选项（译注：此为 <code>--verbose</code> 的简写，取首字母），显示对应的克隆地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git remote -v</span><br><span class="line">origin    git://github.com/schacon/ticgit.git</span><br></pre></td></tr></table></figure></p><p>　　如果有多个远程仓库，此命令将全部列出。比如在我的 Grit 项目中，可以看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cd grit</span><br><span class="line">$ git remote -v</span><br><span class="line">bakkdoor  git://github.com/bakkdoor/grit.git</span><br><span class="line">cho45     git://github.com/cho45/grit.git</span><br><span class="line">defunkt   git://github.com/defunkt/grit.git</span><br><span class="line">koke      git://github.com/koke/grit.git</span><br><span class="line">origin    git@github.com:mojombo/grit.git</span><br></pre></td></tr></table></figure><p>　　这样一来，我就可以非常轻松地从这些用户的仓库中，拉取他们的提交到本地。请注意，上面列出的地址只有 origin 用的是 SSH URL 链接，所以也只有这个仓库我能推送数据上去（我们会在第四章解释原因）。</p><h2 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h2><p>　　要添加一个新的远程仓库，可以指定一个简单的名字，以便将来引用，运行 <code>git remote add [shortname] [url]</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git remote</span><br><span class="line">origin</span><br><span class="line">$ git remote add pb git://github.com/paulboone/ticgit.git</span><br><span class="line">$ git remote -v</span><br><span class="line">origin    git://github.com/schacon/ticgit.git</span><br><span class="line">pb    git://github.com/paulboone/ticgit.git</span><br></pre></td></tr></table></figure></p><p>　　现在可以用字串 pb 指代对应的仓库地址了。比如说，要抓取所有 Paul 有的，但本地仓库没有的信息，可以运行 <code>git fetch pb</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git fetch pb</span><br><span class="line">remote: Counting objects: 58, done.</span><br><span class="line">remote: Compressing objects: 100% (41/41), done.</span><br><span class="line">remote: Total 44 (delta 24), reused 1 (delta 0)</span><br><span class="line">Unpacking objects: 100% (44/44), done.</span><br><span class="line">From git://github.com/paulboone/ticgit</span><br><span class="line"> * [new branch]      master     -&gt; pb/master </span><br><span class="line"> * [new branch]      ticgit     -&gt; pb/ticgit</span><br></pre></td></tr></table></figure><p>　　现在，Paul 的主干分支（master）已经完全可以在本地访问了，对应的名字是 <code>pb/master</code>，你可以将它合并到自己的某个分支，或者切换到这个分支，看看有些什么有趣的更新。</p><h2 id="从远程仓库抓取数据"><a href="#从远程仓库抓取数据" class="headerlink" title="从远程仓库抓取数据"></a>从远程仓库抓取数据</h2><p>　　正如之前所看到的，可以用下面的命令从远程仓库抓取数据到本地：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git fetch [remote-name]</span><br></pre></td></tr></table></figure></p><p>　　此命令会到远程仓库中拉取所有你本地仓库中还没有的数据。运行完成后，你就可以在本地访问该远程仓库中的所有分支，将其中某个分支合并到本地，或者只是取出某个分支，一探究竟。（我们会在第三章详细讨论关于分支的概念和操作。）</p><p>　　如果是克隆了一个仓库，此命令会自动将远程仓库归于 origin 名下。所以，<code>git fetch origin</code> 会抓取从你上次克隆以来别人上传到此远程仓库中的所有更新（或是上次 fetch 以来别人提交的更新）。有一点很重要，需要记住，fetch 命令只是将远端的数据拉到本地仓库，并不自动合并到当前工作分支，只有当你确实准备好了，才能手工合并。</p><p>　　如果设置了某个分支用于跟踪某个远端仓库的分支（参见下节及第三章的内容），可以使用 <code>git pull</code> 命令自动抓取数据下来，然后将远端分支自动合并到本地仓库中当前分支。在日常工作中我们经常这么用，既快且好。实际上，默认情况下<code>git clone</code> 命令本质上就是自动创建了本地的 master 分支用于跟踪远程仓库中的 master 分支（假设远程仓库确实有 master 分支）。所以一般我们运行<code>git pull</code>，目的都是要从原始克隆的远端仓库中抓取数据后，合并到工作目录中的当前分支。</p><h2 id="推送数据到远程仓库"><a href="#推送数据到远程仓库" class="headerlink" title="推送数据到远程仓库"></a>推送数据到远程仓库</h2><p>　　项目进行到一个阶段，要同别人分享目前的成果，可以将本地仓库中的数据推送到远程仓库。实现这个任务的命令很简单： <code>git push [remote-name] [branch-name]</code>。如果要把本地的 master 分支推送到<code>origin</code> 服务器上（再次说明下，克隆操作会自动使用默认的 master 和 origin 名字），可以运行下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure></p><p>　　只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。如果在你推数据前，已经有其他人推送了若干更新，那 你的推送操作就会被驳回。你必须先把他们的更新抓取到本地，合并到自己的项目中，然后才可以再次推送。有关推送数据到远程仓库的详细内容见第三章。</p><h2 id="查看远程仓库信息"><a href="#查看远程仓库信息" class="headerlink" title="查看远程仓库信息"></a>查看远程仓库信息</h2><p>　　我们可以通过命令 <code>git remote show [remote-name]</code> 查看某个远程仓库的详细信息，比如要看所克隆的 <code>origin</code> 仓库，可以运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git remote show origin * remote origin</span><br><span class="line">  URL: git://github.com/schacon/ticgit.git</span><br><span class="line">  Remote branch merged with &apos;git pull&apos; while on branch master</span><br><span class="line">    master</span><br><span class="line">  Tracked remote branches</span><br><span class="line">    master</span><br><span class="line">    ticgit</span><br></pre></td></tr></table></figure><p>　　除了对应的克隆地址外，它还给出了许多额外的信息。它友善地告诉你如果是在 master 分支，就可以用 <code>git pull</code> 命令抓取数据合并到本地。另外还列出了所有处于跟踪状态中的远端分支。</p><p>上面的例子非常简单，而随着使用 Git 的深入，<code>git remote show</code> 给出的信息可能会像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ git remote show origin * remote origin</span><br><span class="line">  URL: git@github.com:defunkt/github.git</span><br><span class="line">  Remote branch merged with &apos;git pull&apos; while on branch issues</span><br><span class="line">    issues</span><br><span class="line">  Remote branch merged with &apos;git pull&apos; while on branch master</span><br><span class="line">    master</span><br><span class="line">  New remote branches (next fetch will store in remotes/origin)</span><br><span class="line">    caching</span><br><span class="line">  Stale tracking branches (use &apos;git remote prune&apos;)</span><br><span class="line">    libwalker</span><br><span class="line">    walker2</span><br><span class="line">  Tracked remote branches</span><br><span class="line">    acl</span><br><span class="line">    apiv2</span><br><span class="line">    dashboard2</span><br><span class="line">    issues</span><br><span class="line">    master</span><br><span class="line">    postgres</span><br><span class="line">  Local branch pushed with &apos;git push&apos; master:master</span><br></pre></td></tr></table></figure><p>　　它告诉我们，运行 <code>git push</code> 时缺省推送的分支是什么（译注：最后两行）。它还显示了有哪些远端分支还没有同步到本地（译注：第六行的<code>caching</code> 分支），哪些已同步到本地的远端分支在远端服务器上已被删除（译注：<code>Stale tracking branches</code> 下面的两个分支），以及运行<code>git pull</code> 时将自动合并哪些分支（译注：前四行中列出的 <code>issues</code> 和 <code>master</code> 分支）。</p><h2 id="远程仓库的删除和重命名"><a href="#远程仓库的删除和重命名" class="headerlink" title="远程仓库的删除和重命名"></a>远程仓库的删除和重命名</h2><p>　　在新版 Git 中可以用 <code>git remote rename</code> 命令修改某个远程仓库在本地的简短名称，比如想把 <code>pb</code>改成<code>paul</code>，可以这么运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git remote rename pb paul</span><br><span class="line">$ git remote</span><br><span class="line">origin</span><br><span class="line">paul</span><br></pre></td></tr></table></figure></p><p>　　注意，对远程仓库的重命名，也会使对应的分支名称发生变化，原来的 <code>pb/master</code> 分支现在成了 <code>paul/master</code>。</p><p>　　碰到远端仓库服务器迁移，或者原来的克隆镜像不再使用，又或者某个参与者不再贡献代码，那么需要移除对应的远端仓库，可以运行 <code>git remote rm</code> 命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git remote rm paul</span><br><span class="line">$ git remote</span><br><span class="line">origin</span><br></pre></td></tr></table></figure></p><h1 id="打标签"><a href="#打标签" class="headerlink" title="打标签"></a>打标签</h1><p>　　同大多数 VCS 一样，Git 也可以对某一时间点上的版本打上标签。人们在发布某个软件版本（比如 v1.0 等等）的时候，经常这么做。本节我们一起来学习如何列出所有可用的标签，如何新建标签，以及各种不同类型标签之间的差别。</p><h2 id="列显已有的标签"><a href="#列显已有的标签" class="headerlink" title="列显已有的标签"></a>列显已有的标签</h2><p>　　列出现有标签的命令非常简单，直接运行 <code>git tag</code> 即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git tag</span><br><span class="line">v0.1 </span><br><span class="line">v1.3</span><br></pre></td></tr></table></figure></p><p>　　显示的标签按字母顺序排列，所以标签的先后并不表示重要程度的轻重。</p><p>　　我们可以用特定的搜索模式列出符合条件的标签。在 Git 自身项目仓库中，有着超过 240 个标签，如果你只对 1.4.2 系列的版本感兴趣，可以运行下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git tag -l &apos;v1.4.2.*&apos; </span><br><span class="line">v1.4.2.1 </span><br><span class="line">v1.4.2.2 </span><br><span class="line">v1.4.2.3 </span><br><span class="line">v1.4.2.4</span><br></pre></td></tr></table></figure></p><h2 id="新建标签"><a href="#新建标签" class="headerlink" title="新建标签"></a>新建标签</h2><p>　　Git 使用的标签有两种类型：轻量级的（lightweight）和含附注的（annotated）。轻量级标签就像是个不会变化的分支，实际上它就是个指向特 定提交对象的引用。而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标 签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。一般我们都建议使用含附注型的标签，以便保留相关信息；当然，如果只是临时性加注标签，或者不需要旁注额外信息，用轻量级标签也没问题。</p><h2 id="含附注的标签"><a href="#含附注的标签" class="headerlink" title="含附注的标签"></a>含附注的标签</h2><p>　　创建一个含附注类型的标签非常简单，用 <code>-a</code> （译注：取 <code>annotated</code> 的首字母）指定标签名字即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git tag -a v1.4 -m &apos;my version 1.4&apos; </span><br><span class="line">$ git tag</span><br><span class="line">v0.1 </span><br><span class="line">v1.3 </span><br><span class="line">v1.4</span><br></pre></td></tr></table></figure></p><p>　　而 <code>-m</code> 选项则指定了对应的标签说明，Git 会将此说明一同保存在标签对象中。如果没有给出该选项，Git 会启动文本编辑软件供你输入标签说明。</p><p>　　可以使用 <code>git show</code> 命令查看相应标签的版本信息，并连同显示打标签时的提交对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git show v1.4 </span><br><span class="line">tag v1.4 </span><br><span class="line">Tagger: Scott Chacon &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Mon Feb 9 14:45:11 2009 -0800 </span><br><span class="line">my version 1.4 </span><br><span class="line">commit 15027957951b64cf874c3557a0f3547bd83b3ff6</span><br><span class="line">Merge: 4a447f7... a6b4c97...</span><br><span class="line">Author: Scott Chacon &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sun Feb 8 19:02:46 2009 -0800 Merge branch &apos;experiment&apos;</span><br></pre></td></tr></table></figure><p>　　我们可以看到在提交对象信息上面，列出了此标签的提交者和提交时间，以及相应的标签说明。</p><h2 id="轻量级标签"><a href="#轻量级标签" class="headerlink" title="轻量级标签"></a>轻量级标签</h2><p>轻量级标签实际上就是一个保存着对应提交对象的校验和信息的文件。要创建这样的标签，一个 <code>-a</code>，<code>-s</code> 或 <code>-m</code> 选项都不用，直接给出标签名字即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git tag v1.4-lw</span><br><span class="line">$ git tag</span><br><span class="line">v0.1 </span><br><span class="line">v1.3 </span><br><span class="line">v1.4 </span><br><span class="line">v1.4-lw</span><br><span class="line">v1.5</span><br></pre></td></tr></table></figure><p>现在运行 <code>git show</code> 查看此标签信息，就只有相应的提交对象摘要：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git show v1.4-lw</span><br><span class="line">commit 15027957951b64cf874c3557a0f3547bd83b3ff6</span><br><span class="line">Merge: 4a447f7... a6b4c97...</span><br><span class="line">Author: Scott Chacon &lt;schacon@gee-mail.com&gt; </span><br><span class="line">Date:   Sun Feb 8 19:02:46 2009 -0800 </span><br><span class="line">Merge branch &apos;experiment&apos;</span><br></pre></td></tr></table></figure><h2 id="验证标签"><a href="#验证标签" class="headerlink" title="验证标签"></a>验证标签</h2><p>可以使用 <code>git tag -v [tag-name]</code> （译注：取 <code>verify</code> 的首字母）的方式验证已经签署的标签。此命令会调用 GPG 来验证签名，所以你需要有签署者的公钥，存放在 keyring 中，才能验证：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git tag -v v1.4.2.1</span><br><span class="line">object 883653babd8ee7ea23e6a5c392bb739348b1eb61</span><br><span class="line">type commit</span><br><span class="line">tag v1.4.2.1</span><br><span class="line">tagger Junio C Hamano &lt;junkio@cox.net&gt; 1158138501 -0700 GIT 1.4.2.1Minor fixes since 1.4.2, including git-mv and git-http with alternates.</span><br><span class="line">gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID F3119B9A</span><br><span class="line">gpg: Good signature from &quot;Junio C Hamano &lt;junkio@cox.net&gt;&quot;</span><br><span class="line">gpg:                 aka &quot;[jpeg image of size 1513]&quot;</span><br><span class="line">Primary key fingerprint: 3565 2A26 2040 E066 C9A7  4A7D C0C6 D9A4 F311 9B9A</span><br></pre></td></tr></table></figure><h2 id="分享标签"><a href="#分享标签" class="headerlink" title="分享标签"></a>分享标签</h2><p>默认情况下，<code>git push</code> 并不会把标签传送到远端服务器上，只有通过显式命令才能分享标签到远端仓库。其命令格式如同推送分支，运行<code>git push origin [tagname]</code> 即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin v1.5 Counting objects: 50, done.</span><br><span class="line">Compressing objects: 100% (38/38), done.</span><br><span class="line">Writing objects: 100% (44/44), 4.56 KiB, done.</span><br><span class="line">Total 44 (delta 18), reused 8 (delta 1)</span><br><span class="line">To git@github.com:schacon/simplegit.git </span><br><span class="line">* [new tag]         v1.5 -&gt; v1.5</span><br></pre></td></tr></table></figure><p>如果要一次推送所有本地新增的标签上去，可以使用 <code>--tags</code> 选项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin --tags</span><br><span class="line">Counting objects: 50, done.</span><br><span class="line">Compressing objects: 100% (38/38), done.</span><br><span class="line">Writing objects: 100% (44/44), 4.56 KiB, done.</span><br><span class="line">Total 44 (delta 18), reused 8 (delta 1)</span><br><span class="line">To git@github.com:schacon/simplegit.git </span><br><span class="line"> * [new tag]         v0.1 -&gt; v0.1</span><br><span class="line"> * [new tag]         v1.2 -&gt; v1.2</span><br><span class="line"> * [new tag]         v1.4 -&gt; v1.4</span><br><span class="line"> * [new tag]         v1.4-lw -&gt; v1.4-lw </span><br><span class="line"> * [new tag]         v1.5 -&gt; v1.5</span><br></pre></td></tr></table></figure><p>现在，其他人克隆共享仓库或拉取数据同步后，也会看到这些标签。</p><h1 id="技巧和窍门"><a href="#技巧和窍门" class="headerlink" title="技巧和窍门"></a>技巧和窍门</h1><p>在结束本章之前，我还想和大家分享一些 Git 使用的技巧和窍门。很多使用 Git 的开发者可能根本就没用过这些技巧，我们也不是说在读过本书后非得用这些技巧不可，但至少应该有所了解吧。说实话，有了这些小窍门，我们的工作可以变得更简单，更轻松，更高效。</p><h2 id="自动补全"><a href="#自动补全" class="headerlink" title="自动补全"></a>自动补全</h2><p>如果你用的是 Bash shell，可以试试看 Git 提供的自动完成脚本。下载 Git 的源代码，进入 <code>contrib/completion</code> 目录，会看到一个<code>git-completion.bash</code> 文件。将此文件复制到你自己的用户主目录中（译注：按照下面的示例，还应改名加上点：<code>cp git-completion.bash ~/.git-completion.bash</code>），并把下面一行内容添加到你的<code>.bashrc</code> 文件中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.git-completion.bash</span><br></pre></td></tr></table></figure><p>也可以为系统上所有用户都设置默认使用此脚本。Mac 上将此脚本复制到 <code>/opt/local/etc/bash_completion.d</code> 目录中，Linux 上则复制到<code>/etc/bash_completion.d/</code> 目录中。这两处目录中的脚本，都会在 Bash 启动时自动加载。</p><p>如果在 Windows 上安装了 msysGit，默认使用的 Git Bash 就已经配好了这个自动完成脚本，可以直接使用。</p><p>在输入 Git 命令的时候可以敲两次跳格键（Tab），就会看到列出所有匹配的可用命令建议：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git co</span><br><span class="line">    commit config</span><br></pre></td></tr></table></figure><p>此例中，键入 git co 然后连按两次 Tab 键，会看到两个相关的建议（命令） commit 和 config。继而输入 <code>m</code> 会自动完成<code>git commit</code> 命令的输入。</p><p>命令的选项也可以用这种方式自动完成，其实这种情况更实用些。比如运行 <code>git log</code> 的时候忘了相关选项的名字，可以输入开头的几个字母，然后敲 Tab 键看看有哪些匹配的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git log --s</span><br><span class="line">    </span><br><span class="line">      --shortstat  --since=  --src-prefix=  --stat   --summary</span><br></pre></td></tr></table></figure><p>这个技巧不错吧，可以节省很多输入和查阅文档的时间。</p><h2 id="Git-命令别名"><a href="#Git-命令别名" class="headerlink" title="Git 命令别名"></a>Git 命令别名</h2><p>Git 并不会推断你输入的几个字符将会是哪条命令，不过如果想偷懒，少敲几个命令的字符，可以用 <code>git config</code> 为命令设置别名。来看看下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global alias.co checkout</span><br><span class="line">$ git config --global alias.br branch</span><br><span class="line">$ git config --global alias.ci commit</span><br><span class="line">$ git config --global alias.st status</span><br></pre></td></tr></table></figure></p><p>现在，如果要输入 <code>git commit</code> 只需键入 <code>git ci</code> 即可。而随着 Git 使用的深入，会有很多经常要用到的命令，遇到这种情况，不妨建个别名提高效率。</p><p>使用这种技术还可以创造出新的命令，比方说取消暂存文件时的输入比较繁琐，可以自己设置一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global alias.unstage &apos;reset HEAD --&apos;</span><br></pre></td></tr></table></figure></p><p>这样一来，下面的两条命令完全等同：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git unstage fileA</span><br><span class="line">$ git reset HEAD fileA</span><br></pre></td></tr></table></figure></p><p>显然，使用别名的方式看起来更清楚。另外，我们还经常设置 <code>last</code> 命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global alias.last &apos;log -1 HEAD&apos;</span><br></pre></td></tr></table></figure></p><p>然后要看最后一次的提交信息，就变得简单多了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git last</span><br><span class="line">commit 66938dae3329c7aebe598c2246a8e6af90d04646</span><br><span class="line">Author: Josh Goebel &lt;dreamer3@example.com&gt;</span><br><span class="line">Date:   Tue Aug 26 19:48:51 2008 +0800</span><br></pre></td></tr></table></figure><p>　　可以看出，实际上 Git 只是简单地在命令中替换了你设置的别名。不过有时候我们希望运行某个外部命令，而非 Git 的附属工具，这个好办，只需要在命令前加上 <code>!</code> 就行。如果你自己写了些处理 Git 仓库信息的脚本的话，就可以用这种技术包装起来。作为演示，我们可以设置用 <code>git visual</code>启动<code>gitk</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global alias.visual &quot;!gitk&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git知识（三）：分支篇</title>
      <link href="/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%88%86%E6%94%AF%E7%AF%87/"/>
      <url>/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%88%86%E6%94%AF%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h1 id="Git-分支"><a href="#Git-分支" class="headerlink" title="Git 分支"></a>Git 分支</h1><p>　　几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。在很多版本控制系统中，这是个昂贵的过程，常常需要创建一个源代码目录的完整副本，对大型项目来说会花费很长时间。</p><p>　　有人把 Git 的分支模型称为“必杀技特性”，而正是因为它，将 Git 从版本控制系统家族里区分出来。Git 有何特别之处呢？Git 的分支可谓是难以置信的轻量级，它的新建操作几乎可以在瞬间完成，并且在不同分支间切换起来也差不多一样快。和许多其他版本控制系统不同，Git 鼓励在工作流程中频繁使用分支与合并，哪怕一天之内进行许多次都没有关系。理解分支的概念并熟练运用后，你才会意识到为什么 Git 是一个如此强大而独特的工具，并从此真正改变你的开发方式。<br><a id="more"></a></p><h1 id="3-1-何谓分支"><a href="#3-1-何谓分支" class="headerlink" title="3.1 何谓分支"></a>3.1 何谓分支</h1><p>　　为了理解 Git 分支的实现方式，我们需要回顾一下 Git 是如何储存数据的。或许你还记得第一章的内容，Git 保存的不是文件差异或者变化量，而只是一系列文件快照。</p><p>　　在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次提交的作者等相关附属信息，包含零个或多个指向该提交对 象的父对象指针：首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。</p><p>　　为直观起见，我们假设在工作目录中有三个文件，准备将它们暂存后提交。暂存操作会对每一个文件计算校验和（即第一章中提到的 SHA-1 哈希字串），然后把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 类型的对象存储这些快照），并将校验和加入暂存区域：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git add README test.rb LICENSE </span><br><span class="line">$ git commit -m &apos;initial commit of my project&apos;</span><br></pre></td></tr></table></figure></p><p>　　当使用 <code>git commit</code> 新建一个提交对象前，Git 会先计算每一个子目录（本例中就是项目根目录）的校验和，然后在 Git 仓库中将这些目录保存为树（tree）对象。之后 Git 创建的提交对象，除了包含相关提交信息以外，还包含着指向这个树对象（项目根目录）的指针，如此它就可以在将来需要的时候，重现此次快照的内容了。</p><p>　　现在，Git 仓库中有五个对象：三个表示文件快照内容的 blob 对象；一个记录着目录树内容及其中各个文件对应 blob 对象索引的 tree 对象；以及一个包含指向 tree 对象（根目录）的索引和其他提交信息元数据的 commit 对象。概念上来说，仓库中的各个对象保存的数据和相互关系看起来如图 3-1 所示：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121719_123.png" alt="Git详解之三 Git分支 "></p><center>图 3-1. 单个提交对象在仓库中的数据结构</center><p>　　作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针（译注：即下图中的 parent 对象）。两次提交后，仓库历史会变成图 3-2 的样子：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121721_705.png" alt="Git详解之三 Git分支 "></p><center>图 3-2. 多个提交对象之间的链接关系</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　现在来谈分支。Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针。Git 会使用 master 作为分支的默认名字。在若干次提交后，你其实已经有了一个指向最后一次提交对象的 master 分支，它在每次提交的时候都会自动向前移动。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121722_450.png" alt="Git详解之三 Git分支 "></p><center>图 3-3. 分支其实就是从某个提交对象往回看的历史</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　那么，Git 又是如何创建一个新的分支的呢？答案很简单，创建一个新的分支指针。比如新建一个 testing 分支，可以使用 <code>git branch</code> 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">　　$ git branch testing</span><br></pre></td></tr></table></figure><p>　　这会在当前 commit 对象上新建一个分支指针（见图 3-4）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121722_624.png" alt="Git详解之三 Git分支 "></p><center>图 3-4. 多个分支指向提交数据的历史</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　那么，Git 是如何知道你当前在哪个分支上工作的呢？其实答案也很简单，它保存着一个名为 HEAD 的特别指针。请注意它和你熟知的许多其他版本控制系统（比如 Subversion 或 CVS）里的 HEAD 概念大不相同。在 Git 中，它是一个指向你正在工作中的本地分支的指针（译注：将 HEAD 想象为当前分支的别名。）。运行<code>git branch</code> 命令，仅仅是建立了一个新的分支，但不会自动切换到这个分支中去，所以在这个例子中，我们依然还在 master 分支里工作（参考图 3-5）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121723_520.png" alt="Git详解之三 Git分支 "></p><center>图 3-5. HEAD 指向当前所在的分支</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　要切换到其他分支，可以执行 <code>git checkout</code> 命令。我们现在转换到新建的 testing 分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">　　$ git checkout testing</span><br></pre></td></tr></table></figure><p>　　这样 HEAD 就指向了 testing 分支（见图3-6）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121724_229.png" alt="Git详解之三 Git分支 "></p><center>图 3-6. HEAD 在你转换分支时指向新的分支</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　这样的实现方式会给我们带来什么好处呢？好吧，现在不妨再提交一次：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">　　$ vim test.rb</span><br><span class="line">　　$ git commit -a -m &apos;made a change&apos;</span><br></pre></td></tr></table></figure><p>　　图 3-7 展示了提交后的结果。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121724_283.png" alt="Git详解之三 Git分支 "></p><center>图 3-7. 每次提交后 HEAD 随着分支一起向前移动</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　非常有趣，现在 testing 分支向前移动了一格，而 master 分支仍然指向原先 <code>git checkout</code> 时所在的 commit 对象。现在我们回到 master 分支看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">　　$ git checkout master</span><br></pre></td></tr></table></figure><p>　　图 3-8 显示了结果。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121725_226.png" alt="Git详解之三 Git分支 "></p><center>图 3-8. HEAD 在一次 checkout 之后移动到了另一个分支</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　这条命令做了两件事。它把 HEAD 指针移回到 master 分支，并把工作目录中的文件换成了 master 分支所指向的快照内容。也就是说，现在开始所做的改动，将始于本项目中一个较老的版本。它的主要作用是将 testing 分支里作出的修改暂时取消，这样你就可以向另一个方向进行开发。</p><p>　　我们作些修改后再次提交：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">　　$ vim test.rb</span><br><span class="line">　　$ git commit -a -m &apos;made other changes&apos;</span><br></pre></td></tr></table></figure><p>　　现在我们的项目提交历史产生了分叉（如图 3-9 所示），因为刚才我们创建了一个分支，转换到其中进行了一些工作，然后又回到原来的主分支进行了另外一些工作。这些改变分别孤立在不同的分支里：我们可以 在不同分支里反复切换，并在时机成熟时把它们合并到一起。而所有这些工作，仅仅需要<code>branch</code> 和 <code>checkout</code> 这两条命令就可以完成。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121725_89.png" alt="Git详解之三 Git分支 "></p><center>图 3-9. 不同流向的分支历史</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　由于 Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，所以创建和销毁一个分支就变得非常廉价。说白了，新建一个分支就是向一个文件写入 41 个字节（外加一个换行符）那么简单，当然也就很快了。</p><p>　　这和大多数版本控制系统形成了鲜明对比，它们管理分支大多采取备份所有项目文件到特定目录的方式，所以根据项目文件数量和大小不同，可能花费的时间 也会有相当大的差别，快则几秒，慢则数分钟。而 Git 的实现与项目复杂度无关，它永远可以在几毫秒的时间内完成分支的创建和切换。同时，因为每次提交时都记录了祖先信息（译注：即<code>parent</code> 对象），将来要合并分支时，寻找恰当的合并基础（译注：即共同祖先）的工作其实已经自然而然地摆在那里了，所以实现起来非常容易。Git 鼓励开发者频繁使用分支，正是因为有着这些特性作保障。</p><p>　　接下来看看，我们为什么应该频繁使用分支。</p><h1 id="3-2-分支的新建与合并"><a href="#3-2-分支的新建与合并" class="headerlink" title="3.2 分支的新建与合并"></a>3.2 分支的新建与合并</h1><p>　　现在让我们来看一个简单的分支与合并的例子，实际工作中大体也会用到这样的工作流程：</p><p>　　1. 开发某个网站。</p><p>　　2. 为实现某个新的需求，创建一个分支。</p><p>　　3. 在这个分支上开展工作。</p><p>　　假设此时，你突然接到一个电话说有个很严重的问题需要紧急修补，那么可以按照下面的方式处理：</p><p>　　 1. 返回到原先已经发布到生产服务器上的分支。</p><p>　　 2. 为这次紧急修补建立一个新分支，并在其中修复问题。</p><p>　　 3. 通过测试后，回到生产服务器所在的分支，将修补分支合并进来，然后再推送到生产服务器上。</p><p>　　 4. 切换到之前实现新需求的分支，继续工作。</p><h2 id="分支的新建与切换"><a href="#分支的新建与切换" class="headerlink" title="分支的新建与切换"></a>分支的新建与切换</h2><p>　　首先，我们假设你正在项目中愉快地工作，并且已经提交了几次更新（见图 3-10）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121726_3.png" alt="Git详解之三 Git分支 "></p><p>　　<center>图 3-10. 一个简短的提交历史</center></p><p>　　现在，你决定要修补问题追踪系统上的 #53 问题。顺带说明下，Git 并不同任何特定的问题追踪系统打交道。这里为了说明要解决的问题，才把新建的分支取名为 iss53。要新建并切换到该分支，运行<code>git checkout</code> 并加上 <code>-b</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">　　$ git checkout -b iss53</span><br><span class="line">　　Switched to a new branch &quot;iss53&quot;</span><br></pre></td></tr></table></figure><p>　　这相当于执行下面这两条命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">　　$ git branch iss53</span><br><span class="line">　　$ git checkout iss53</span><br></pre></td></tr></table></figure><p>　　 图 3-11 示意该命令的执行结果。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121726_846.png" alt="Git详解之三 Git分支 "></p><center>图 3-11. 创建了一个新分支的指针</center><p>接着你开始尝试修复问题，在提交了若干次更新后，<code>iss53</code> 分支的指针也会随着向前推进，因为它就是当前分支（换句话说，当前的 <code>HEAD</code> 指针正指向 <code>iss53</code>，见图 3-12）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vim index.html</span><br><span class="line">$ git commit -a -m &apos;added a new footer [issue 53]&apos;</span><br></pre></td></tr></table></figure><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121727_65.png" alt="Git详解之三 Git分支 "></p><center>图 3-12. iss53 分支随工作进展向前推进</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　现在你就接到了那个网站问题的紧急电话，需要马上修补。有了 Git ，我们就不需要同时发布这个补丁和 <code>iss53</code> 里作出的修改，也不需要在创建和发布该补丁到服务器之前花费大力气来复原这些修改。唯一需要的仅仅是切换回<code>master</code> 分支。</p><p>　　不过在此之前，留心你的暂存区或者工作目录里，那些还没有提交的修改，它会和你即将检出的分支产生冲突从而阻止 Git 为你切换分支。切换分支的时候最好保持一个清洁的工作区域。稍后会介绍几个绕过这种问题的办法（分别叫做 stashing 和 commit amending）。目前已经提交了所有的修改，所以接下来可以正常转换到<code>master</code> 分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">　　$ git checkout master</span><br><span class="line">　　Switched to branch &quot;master&quot;</span><br></pre></td></tr></table></figure><p>　　此时工作目录中的内容和你在解决问题 #53 之前一模一样，你可以集中精力进行紧急修补。这一点值得牢记：Git 会把工作目录的内容恢复为检出某分支时它所指向的那个提交对象的快照。它会自动添加、删除和修改文件以确保目录的内容和你当时提交时完全一样。</p><p>　　接下来，你得进行紧急修补。我们创建一个紧急修补分支 <code>hotfix</code> 来开展工作，直到搞定（见图 3-13）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b &apos;hotfix&apos; </span><br><span class="line">Switched to a new branch &quot;hotfix&quot; </span><br><span class="line">$ vim index.html</span><br><span class="line">$ git commit -a -m &apos;fixed the broken email address&apos; </span><br><span class="line">[hotfix]: created 3a0874c: &quot;fixed the broken email address&quot;</span><br><span class="line"> 1 files changed, 0 insertions(+), 1 deletions(-)</span><br></pre></td></tr></table></figure></p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121727_499.png" alt="Git详解之三 Git分支 "></p><center>图 3-13. hotfix 分支是从 master 分支所在点分化出来的</center><p>　　有必要作些测试，确保修补是成功的，然后回到 <code>master</code> 分支并把它合并进来，然后发布到生产服务器。用 <code>git merge</code> 命令来进行合并：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">$ git merge hotfix</span><br><span class="line">Updating f42c576..3a0874c</span><br><span class="line">Fast forward</span><br><span class="line"> README |    1 -</span><br><span class="line"> 1 files changed, 0 insertions(+), 1 deletions(-)</span><br></pre></td></tr></table></figure></p><p>　　请注意，合并时出现了“Fast forward”的提示。由于当前 <code>master</code> 分支所在的提交对象是要并入的 <code>hotfix</code> 分支的直接上游，Git 只需把<code>master</code> 分支指针直接右移。换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么 Git 在合并两者时，只会简单地把指针右移，因为这种单线的历史分支不存在任何需要解决的分歧，所以这种合并过程可以称为快进（Fast forward）。</p><p>　　现在最新的修改已经在当前 <code>master</code> 分支所指向的提交对象中了，可以部署到生产服务器上去了（见图 3-14）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121728_242.png" alt="Git详解之三 Git分支 "></p><center>图 3-14. 合并之后，master 分支和 hotfix 分支指向同一位置。</center><p>　　在那个超级重要的修补发布以后，你想要回到被打扰之前的工作。由于当前 <code>hotfix</code> 分支和 <code>master</code> 都指向相同的提交对象，所以<code>hotfix</code> 已经完成了历史使命，可以删掉了。使用 <code>git branch</code> 的 <code>-d</code> 选项执行删除操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">　　$ git branch -d hotfix</span><br><span class="line">　　Deleted branch hotfix (3a0874c).</span><br></pre></td></tr></table></figure><p>　　现在回到之前未完成的 #53 问题修复分支上继续工作（图 3-15）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout iss53</span><br><span class="line">Switched to branch &quot;iss53&quot; $ vim index.html</span><br><span class="line">$ git commit -a -m &apos;finished the new footer [issue 53]&apos; </span><br><span class="line">[iss53]: created ad82d7a: &quot;finished the new footer [issue 53]&quot;</span><br><span class="line"> 1 files changed, 1 insertions(+), 0 deletions(-)</span><br></pre></td></tr></table></figure></p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121729_2.png" alt="Git详解之三 Git分支 "></p><center>图 3-15. iss53 分支可以不受影响继续推进。</center><p>　　不用担心之前 <code>hotfix</code> 分支的修改内容尚未包含到 <code>iss53</code> 中来。如果确实需要纳入此次修补，可以用<code>git merge master</code> 把 master 分支合并到 <code>iss53</code>；或者等 <code>iss53</code> 完成之后，再将<code>iss53</code>分支中的更新并入 <code>master</code>。</p><h3 id="分支的合并"><a href="#分支的合并" class="headerlink" title="分支的合并"></a>分支的合并</h3><p>　　在问题 #53 相关的工作完成之后，可以合并回 <code>master</code> 分支。实际操作同前面合并 <code>hotfix</code> 分支差不多，只需回到<code>master</code> 分支，运行 <code>git merge</code> 命令指定要合并进来的分支：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">$ git merge iss53</span><br><span class="line">Merge made by recursive.</span><br><span class="line"> README |    1 +</span><br><span class="line"> 1 files changed, 1 insertions(+), 0 deletions(-)</span><br></pre></td></tr></table></figure></p><p>　　请注意，这次合并操作的底层实现，并不同于之前 <code>hotfix</code> 的并入方式。因为这次你的开发历史是从更早的地方开始分叉的。由于当前 <code>master</code> 分支所指向的提交对象（C4）并不是 <code>iss53</code> 分支的直接祖先，Git 不得不进行一些额外处理。就此例而言，Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算。图 3-16 用红框标出了 Git 用于合并的三个提交对象：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121729_46.png" alt="Git详解之三 Git分支 "></p><center>图 3-16. Git 为分支合并自动识别出最佳的同源合并点。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　这次，Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）（见图 3-17）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。</p><p>　　值得一提的是 Git 可以自己裁决哪个共同祖先才是最佳合并基础；这和 CVS 或 Subversion（1.5 以后的版本）不同，它们需要开发者手工指定合并基础。所以此特性让 Git 的合并操作比其他系统都要简单不少。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121730_974.png" alt="Git详解之三 Git分支 "></p><center>图 3-17. Git 自动创建了一个包含了合并结果的提交对象。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　既然之前的工作成果已经合并到 <code>master</code> 了，那么 <code>iss53</code> 也就没用了。你可以就此删除它，并在问题追踪系统里关闭该问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">　　$ git branch -d iss53</span><br></pre></td></tr></table></figure><h2 id="Checkout-历史版本"><a href="#Checkout-历史版本" class="headerlink" title="Checkout 历史版本"></a>Checkout 历史版本</h2><h3 id="从某个历史版本创建新的分支"><a href="#从某个历史版本创建新的分支" class="headerlink" title="从某个历史版本创建新的分支"></a>从某个历史版本创建新的分支</h3><p>在 Git 中从当前分支创建并检出新分支的命令是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b name-of-new-branch</span><br></pre></td></tr></table></figure></p><p>这个命令实际上是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b name-of-new-branch current-branch</span><br></pre></td></tr></table></figure></p><p>的简写形式。也就是说，当我们不指定 checkout 起点时，Git 默认从当前活动分支开始创建新的分支。</p><p>Git 的每个提交都有一个 SHA1 散列值（Hash 值）作为 ID。我们可以在 <code>checkout</code> 命令中使用这些 ID 作为起点。比如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b name-of-new-branch 169d2dc</span><br></pre></td></tr></table></figure></p><p>这样，Git 的活动分支会切换到 <code>name-of-new-branch</code> 这个分支上，而它的内容与 <code>169d2dc</code> 这个分支一致。</p><p>注意：SHA1 的散列值有 40 个字母，相当长。所以 Git 允许我们在不引起歧义的情况下，使用散列值的前几位作为缩写。</p><p>提示：你也可以用 <code>git branch name-of-new-branch 169d2dc</code> 来创建一个历史分支，而不切换到该分支。</p><h3 id="将某个历史版本-checkout-到工作区"><a href="#将某个历史版本-checkout-到工作区" class="headerlink" title="将某个历史版本 checkout 到工作区"></a>将某个历史版本 checkout 到工作区</h3><p>首先说明，这样做会产生一个分离的 HEAD 指针，所以个人不推荐这么做。</p><p>如果我们工作在 <code>master</code> 分支上，希望 checkout 到 <code>dev</code> 分支上，我们会这么做：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout dev</span><br></pre></td></tr></table></figure></p><p>这里 <code>dev</code> 实际上是一个指针的别名，其本质也是一个 SHA1 散列值。所以，我们很自然地可以用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout &lt;sha1-of-a-commit&gt;</span><br></pre></td></tr></table></figure></p><p>将某个历史版本 checkout 到工作区。</p><h3 id="将某个文件的历史版本-checkout-到工作区"><a href="#将某个文件的历史版本-checkout-到工作区" class="headerlink" title="将某个文件的历史版本 checkout 到工作区"></a>将某个文件的历史版本 checkout 到工作区</h3><p>大多数时候，我们可能只需要对某一个文件做细小的修补，因此只 checkout 该文件就行了，并不需要操作整个 commit 或分支。</p><p>上一节我们介绍了如何将某个历史版本完整地 checkout 到工作区。实际上，我们只需要在上一节的命令之后加上需要 checkout 的文件即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout &lt;sha1-of-a-commit&gt; &lt;/path/to/your/file&gt;</span><br></pre></td></tr></table></figure></p><p>当然，有时候你需要将某个文件的历史版本 checkout 出来，并以一个新的名字保存。这时候可以这么做：</p><h2 id="遇到冲突时的分支合并"><a href="#遇到冲突时的分支合并" class="headerlink" title="遇到冲突时的分支合并"></a>遇到冲突时的分支合并</h2><p>　　有时候合并操作并不会如此顺利。如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（译注：逻辑上说，这种问题只能由人来裁决。）。如果你在解决问题 #53 的过程中修改了<code>hotfix</code> 中修改的部分，将得到类似下面的结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git merge iss53</span><br><span class="line">Auto-merging index.html</span><br><span class="line">CONFLICT (content): Merge conflict in index.html</span><br><span class="line">Automatic merge failed; fix conflicts and then commit the result.</span><br></pre></td></tr></table></figure></p><p>　　Git 作了合并，但没有提交，它会停下来等你解决冲突。要看看哪些文件在合并时发生冲突，可以用 <code>git status</code> 查阅：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[master*]$ git status</span><br><span class="line">index.html: needs merge</span><br><span class="line"># On branch master</span><br><span class="line"># Changed but not updated:</span><br><span class="line">#   (use &quot;git add </span><br><span class="line">     ...&quot; to update what will be committed)</span><br><span class="line">#   (use &quot;git checkout -- </span><br><span class="line"> ...&quot; to discard changes in working directory)</span><br><span class="line">#</span><br><span class="line">#    unmerged:   index.html</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　任何包含未解决冲突的文件都会以未合并（unmerged）的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记，可以通过它们来手工定位并解决这些冲突。可以看到此文件包含类似下面这样的部分：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html</span><br><span class="line">contact : email.support@github.com </span><br><span class="line">======= </span><br><span class="line">please contact us at support@github.com </span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html</span><br></pre></td></tr></table></figure></p><p>　　可以看到 <code>=======</code> 隔开的上半部分，是 <code>HEAD</code>（即 <code>master</code> 分支，在运行<code>merge</code> 命令时所切换到的分支）中的内容，下半部分是在 <code>iss53</code> 分支中的内容。解决冲突的办法无非是二者选其一或者由你亲自整合到一起。比如你可以通过把这段内容替换为下面这样来解决：</p><p>　　please  contact  us  at  <a href="mailto:email.support@github.com" target="_blank" rel="noopener">email.support@github.com</a></p><p>　　这个解决方案各采纳了两个分支中的一部分内容，而且我还删除了 <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>，<code>=======</code> 和 <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code> 这些行。在解决了所有文件里的所有冲突后，运行 <code>git add</code> 将把它们标记为已解决状态（译注：实际上就是来一次快照保存到暂存区域。）。因为一旦暂存，就表示冲突已经解决。如果你想用一个有图形界面的工具来解决这些问题，不妨运行<code>git mergetool</code>，它会调用一个可视化的合并工具并引导你解决所有冲突：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git mergetool</span><br><span class="line">merge tool candidates: kdiff3 tkdiff xxdiff meld gvimdiff opendiff emerge vimdiff</span><br><span class="line">Merging the files: index.html</span><br><span class="line"></span><br><span class="line">Normal merge conflict for &apos;index.html&apos;:</span><br><span class="line">  &#123;local&#125;: modified</span><br><span class="line">  &#123;remote&#125;: modified</span><br><span class="line">Hit return to start merge resolution tool (opendiff):</span><br></pre></td></tr></table></figure><p>　　如果不想用默认的合并工具（Git 为我默认选择了 <code>opendiff</code>，因为我在 Mac 上运行了该命令），你可以在上方”merge tool candidates”里找到可用的合并工具列表，输入你想用的工具名。我们将在第七章讨论怎样改变环境中的默认值。</p><p>　　退出合并工具以后，Git 会询问你合并是否成功。如果回答是，它会为你把相关文件暂存起来，以表明状态为已解决。</p><p>　　再运行一次 <code>git status</code> 来确认所有冲突都已解决：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD </span><br><span class="line"> ...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#    modified:   index.html</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　如果觉得满意了，并且确认所有冲突都已解决，也就是进入了暂存区，就可以用 <code>git commit</code> 来完成这次合并提交。提交的记录差不多是这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Merge branch &apos;iss53&apos; Conflicts:</span><br><span class="line">  index.html</span><br><span class="line">#</span><br><span class="line"># It looks like you may be committing a MERGE.</span><br><span class="line"># If this is not correct, please remove the file </span><br><span class="line"># .git/MERGE_HEAD</span><br><span class="line"># and try again.</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p>　　如果想给将来看这次合并的人一些方便，可以修改该信息，提供更多合并细节。比如你都作了哪些改动，以及这么做的原因。有时候裁决冲突的理由并不直接或明显，有必要略加注解。</p><h1 id="3-3-分支的管理"><a href="#3-3-分支的管理" class="headerlink" title="3.3 分支的管理"></a>3.3 分支的管理</h1><p>　　到目前为止，你已经学会了如何创建、合并和删除分支。除此之外，我们还需要学习如何管理分支，在日后的常规工作中会经常用到下面介绍的管理命令。</p><p><code>git branch</code> 命令不仅仅能创建和删除分支，如果不加任何参数，它会给出当前所有分支的清单：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">  iss53 * master</span><br><span class="line">  testing</span><br></pre></td></tr></table></figure></p><p>注意看 <code>master</code> 分支前的 <code>*</code> 字符：它表示当前所在的分支。也就是说，如果现在提交更新，<code>master</code> 分支将随着开发进度前移。若要查看各个分支最后一个提交对象的信息，运行<code>git branch -v</code>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -v</span><br><span class="line">  iss53   93b412c fix javascript issue * </span><br><span class="line">  master  7a98805 Merge branch &apos;iss53&apos; </span><br><span class="line">  testing 782fd34 add scott to the author list in the readmes</span><br></pre></td></tr></table></figure></p><p>要从该清单中筛选出你已经（或尚未）与当前分支合并的分支，可以用 <code>--merge</code> 和 <code>--no-merged</code>选项（Git 1.5.6 以上版本）。比如用<code>git branch --merge</code> 查看哪些分支已被并入当前分支（译注：也就是说哪些分支是当前分支的直接上游。）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git branch --merged</span><br><span class="line">  iss53 </span><br><span class="line">* master</span><br></pre></td></tr></table></figure></p><p>之前我们已经合并了 <code>iss53</code>，所以在这里会看到它。一般来说，列表中没有 <code>*</code> 的分支通常都可以用 <code>git branch -d</code> 来删掉。原因很简单，既然已经把它们所包含的工作整合到了其他分支，删掉也不会损失什么。</p><p>另外可以用 <code>git branch --no-merged</code> 查看尚未合并的工作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git branch --no-merged</span><br><span class="line">  testing</span><br></pre></td></tr></table></figure><p>它会显示还未合并进来的分支。由于这些分支中还包含着尚未合并进来的工作成果，所以简单地用 <code>git branch -d</code> 删除该分支会提示错误，因为那样做会丢失数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -d testing</span><br><span class="line">error: The branch &apos;testing&apos; is not an ancestor of your current HEAD.</span><br><span class="line">If you are sure you want to delete it, run &apos;git branch -D testing&apos;.</span><br></pre></td></tr></table></figure></p><p>不过，如果你确实想要删除该分支上的改动，可以用大写的删除选项 <code>-D</code> 强制执行，就像上面提示信息中给出的那样。</p><h1 id="3-4-利用分支进行开发的工作流程"><a href="#3-4-利用分支进行开发的工作流程" class="headerlink" title="3.4 利用分支进行开发的工作流程"></a>3.4 利用分支进行开发的工作流程</h1><p>　　现在我们已经学会了新建分支和合并分支，可以（或应该）用它来做点什么呢？在本节，我们会介绍一些利用分支进行开发的工作流程。而正是由于分支管理的便捷，才衍生出了这类典型的工作模式，你可以根据项目的实际情况选择一种用用看。</p><h3 id="长期分支"><a href="#长期分支" class="headerlink" title="长期分支"></a>长期分支</h3><p>　　由于 Git 使用简单的三方合并，所以就算在较长一段时间内，反复多次把某个分支合并到另一分支，也不是什么难事。也就是说，你可以同时拥有多个开放的分支，每个分支用于完成特定的任务，随着开发的推进，你可以随时把某个特性分支的成果并到其他分支中。</p><p>　　许多使用 Git 的开发者都喜欢用这种方式来开展工作，比如仅在 <code>master</code> 分支中保留完全稳定的代码，即已经发布或即将发布的代码。与此同时，他们还有一个名为<code>develop</code> 或 <code>next</code> 的平行分支，专门用于后续的开发，或仅用于稳定性测试 — 当然并不是说一定要绝对稳定，不过一旦进入某种稳定状态，便可以把它合并到<code>master</code> 里。这样，在确保这些已完成的特性分支（短期分支，比如之前的 <code>iss53</code> 分支）能够通过所有测试，并且不会引入更多错误之后，就可以并到主干分支中，等待下一次的发布。</p><p>　　本质上我们刚才谈论的，是随着提交对象不断右移的指针。稳定分支的指针总是在提交历史中落后一大截，而前沿分支总是比较靠前（见图 3-18）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121730_93.png" alt="Git详解之三 Git分支 "></p><center>图 3-18. 稳定分支总是比较老旧。</center><p>或者把它们想象成工作流水线，或许更好理解一些，经过测试的提交对象集合被遴选到更稳定的流水线（见图 3-19）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121731_16.png" alt="Git详解之三 Git分支 "></p><center>图 3-19. 想象成流水线可能会容易点。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　你可以用这招维护不同层次的稳定性。某些大项目还会有个 <code>proposed</code>（建议）或 <code>pu</code>（proposed updates，建议更新）分支，它包含着那些可能还没有成熟到进入<code>next</code> 或 <code>master</code> 的内容。这么做的目的是拥有不同层次的稳定性：当这些分支进入到更稳定的水平时，再把它们合并到更高层分支中去。再次说明下，使用多个长期分支的做法并非必需，不过一般来说，对于特大型项目或特复杂的项目，这么做确实更容易管理。</p><h3 id="特性分支"><a href="#特性分支" class="headerlink" title="特性分支"></a>特性分支</h3><p>　　在任何规模的项目中都可以使用特性（Topic）分支。一个特性分支是指一个短期的，用来实现单一特性或与其相关工作的分支。可能你在以前的版本控 制系统里从未做过类似这样的事情，因为通常创建与合并分支消耗太大。然而在 Git 中，一天之内建立、使用、合并再删除多个分支是常见的事。</p><p>　　我们在上节的例子里已经见过这种用法了。我们创建了 <code>iss53</code> 和 <code>hotfix</code> 这两个特性分支，在提交了若干更新后，把它们合并到主干分支，然后删除。该技术允许你迅速且完全的进行语境切换 — 因为你的工作分散在不同的流水线里，每个分支里的改变都和它的目标特性相关，浏览代码之类的事情因而变得更简单了。你可以把作出的改变保持在特性分支中几 分钟，几天甚至几个月，等它们成熟以后再合并，而不用在乎它们建立的顺序或者进度。</p><p>　　现在我们来看一个实际的例子。请看图 3-20，由下往上，起先我们在 <code>master</code> 工作到 C1，然后开始一个新分支 <code>iss91</code> 尝试修复 91 号缺陷，提交到 C6 的时候，又冒出一个解决该问题的新办法，于是从之前 C4 的地方又分出一个分支<code>iss91v2</code>，干到 C8 的时候，又回到主干 <code>master</code> 中提交了 C9 和 C10，再回到 <code>iss91v2</code> 继续工作，提交 C11，接着，又冒出个不太确定的想法，从 <code>master</code> 的最新提交 C10 处开了个新的分支<code>dumbidea</code> 做些试验。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121731_919.png" alt="Git详解之三 Git分支 "></p><center>图 3-20. 拥有多个特性分支的提交历史。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　现在，假定两件事情：我们最终决定使用第二个解决方案，即 <code>iss91v2</code> 中的办法；另外，我们把 <code>dumbidea</code> 分支拿给同事们看了以后，发现它竟然是个天才之作。所以接下来，我们准备抛弃原来的<code>iss91</code> 分支（实际上会丢弃 C5 和 C6），直接在主干中并入另外两个分支。最终的提交历史将变成图 3-21 这样：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121732_963.png" alt="Git详解之三 Git分支 "></p><center>图 3-21. 合并了 dumbidea 和 iss91v2 后的分支历史。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>请务必牢记这些分支全部都是本地分支，这一点很重要。当你在使用分支及合并的时候，一切都是在你自己的 Git 仓库中进行的 — 完全不涉及与服务器的交互。</p><h1 id="3-5-远程分支"><a href="#3-5-远程分支" class="headerlink" title="3.5 远程分支"></a>3.5 远程分支</h1><p>　　远程分支（remote branch）是对远程仓库中的分支的索引。它们是一些无法移动的本地分支；只有在 Git 进行网络交互时才会更新。远程分支就像是书签，提醒着你上次连接远程仓库时上面各分支的位置。</p><p>　　我们用 <code>(远程仓库名)/(分支名)</code> 这样的形式表示远程分支。比如我们想看看上次同 <code>origin</code> 仓库通讯时<code>master</code> 的样子，就应该查看 <code>origin/master</code> 分支。如果你和同伴一起修复某个问题，但他们先推送了一个<code>iss53</code> 分支到远程仓库，虽然你可能也有一个本地的 <code>iss53</code> 分支，但指向服务器上最新更新的却应该是 <code>origin/iss53</code> 分支。</p><p>　　可能有点乱，我们不妨举例说明。假设你们团队有个地址为 <code>git.ourcompany.com</code> 的 Git 服务器。如果你从这里克隆，Git 会自动为你将此远程仓库命名为<code>origin</code>，并下载其中所有的数据，建立一个指向它的 <code>master</code> 分支的指针，在本地命名为 <code>origin/master</code>，但你无法在本地更改其数据。接着，Git 建立一个属于你自己的本地<code>master</code> 分支，始于 <code>origin</code> 上 <code>master</code> 分支相同的位置，你可以就此开始工作（见图 3-22）：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121733_745.png" alt="Git详解之三 Git分支 "></p><center>图 3-22. 一次 Git 克隆会建立你自己的本地分支 master 和远程分支 origin/master，它们都指向 origin/master 分支的最后一次提交。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　如果你在本地 <code>master</code> 分支做了些改动，与此同时，其他人向 <code>git.ourcompany.com</code> 推送了他们的更新，那么服务器上的<code>master</code> 分支就会向前推进，而于此同时，你在本地的提交历史正朝向不同方向发展。不过只要你不和服务器通讯，你的 <code>origin/master</code> 指针仍然保持原位不会移动（见图 3-23）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121734_669.png" alt="Git详解之三 Git分支 "></p><center>图 3-23. 在本地工作的同时有人向远程仓库推送内容会让提交历史开始分流。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>可以运行 <code>git fetch origin</code> 来同步远程服务器上的数据到本地。该命令首先找到 <code>origin</code> 是哪个服务器（本例为<code>git.ourcompany.com</code>），从上面获取你尚未拥有的数据，更新你本地的数据库，然后把 <code>origin/master</code> 的指针移到它最新的位置上（见图 3-24）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121735_607.png" alt="Git详解之三 Git分支 "></p><center>图 3-24. git fetch 命令会更新 remote 索引。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　为了演示拥有多个远程分支（在不同的远程服务器上）的项目是如何工作的，我们假设你还有另一个仅供你的敏捷开发小组使用的内部服务器 <code>git.team1.ourcompany.com</code>。可以用第二章中提到的<code>git remote add</code> 命令把它加为当前项目的远程分支之一。我们把它命名为 <code>teamone</code>，以便代替原始的 Git 地址（见图 3-25）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121736_936.png" alt="Git详解之三 Git分支 "></p><center>图 3-25. 把另一个服务器加为远程仓库</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　现在你可以用 <code>git fetch teamone</code> 来获取小组服务器上你还没有的数据了。由于当前该服务器上的内容是你 <code>origin</code> 服务器上的子集，Git 不会下载任何数据，而只是简单地创建一个名为<code>teamone/master</code> 的分支，指向 <code>teamone</code> 服务器上 <code>master</code> 分支所在的提交对象<code>31b8e</code>（见图 3-26）。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121737_674.png" alt="Git详解之三 Git分支 "></p><center>图 3-26. 你在本地有了一个指向 teamone 服务器上 master 分支的索引。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><h3 id="推送本地分支"><a href="#推送本地分支" class="headerlink" title="推送本地分支"></a>推送本地分支</h3><p>　　要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你的本地分支不会被自动同步到你引入的远程服务器上，除非你明确执行推送操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。</p><p>如果你有个叫 <code>serverfix</code> 的分支需要和他人一起开发，可以运行 <code>git push (远程仓库名) (分支名)</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin serverfix</span><br><span class="line">Counting objects: 20, done.</span><br><span class="line">Compressing objects: 100% (14/14), done.</span><br><span class="line">Writing objects: 100% (15/15), 1.74 KiB, done.</span><br><span class="line">Total 15 (delta 5), reused 0 (delta 0)</span><br><span class="line">To git@github.com:schacon/simplegit.git </span><br><span class="line"> * [new branch]      serverfix -&gt; serverfix</span><br></pre></td></tr></table></figure><p>　　这其实有点像条捷径。Git 自动把 <code>serverfix</code> 分支名扩展为 <code>refs/heads/serverfix:refs/heads/serverfix</code>，意为“取出我在本地的 serverfix 分支，推送到远程仓库的 serverfix 分支中去”。我们将在第九章进一步介绍<code>refs/heads/</code> 部分的细节，不过一般使用的时候都可以省略它。也可以运行 <code>git push origin serverfix:serferfix</code> 来实现相同的效果，它的意思是“上传我本地的 serverfix 分支到远程仓库中去，仍旧称它为 serverfix 分支”。通过此语法，你可以把本地分支推送到某个命名不同的远程分支：若想把远程分支叫作<code>awesomebranch</code>，可以用 <code>git push origin serverfix:awesomebranch</code> 来推送数据。</p><p>　　接下来，当你的协作者再次从服务器上获取数据时，他们将得到一个新的远程分支 <code>origin/serverfix</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git fetch origin</span><br><span class="line">remote: Counting objects: 20, done.</span><br><span class="line">remote: Compressing objects: 100% (14/14), done.</span><br><span class="line">remote: Total 15 (delta 5), reused 0 (delta 0)</span><br><span class="line">Unpacking objects: 100% (15/15), done.</span><br><span class="line">From git@github.com:schacon/simplegit </span><br><span class="line"> * [new branch]      serverfix    -&gt; origin/serverfix</span><br></pre></td></tr></table></figure><p>　　值得注意的是，在 <code>fetch</code> 操作下载好新的远程分支之后，你仍然无法在本地编辑该远程仓库中的分支。换句话说，在本例中，你不会有一个新的<code>serverfix</code> 分支，有的只是一个你无法移动的 <code>origin/serverfix</code> 指针。</p><p>　　如果要把该内容合并到当前分支，可以运行 <code>git merge origin/serverfix</code>。如果想要一份自己的 <code>serverfix</code> 来开发，可以在远程分支的基础上分化出一个新的分支来：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b serverfix origin/serverfix</span><br><span class="line">Branch serverfix set up to track remote branch refs/remotes/origin/serverfix.</span><br><span class="line">Switched to a new branch &quot;serverfix&quot;</span><br></pre></td></tr></table></figure></p><p>这会切换到新建的 <code>serverfix</code> 本地分支，其内容同远程分支 <code>origin/serverfix</code> 一致，这样你就可以在里面继续开发了。</p><h3 id="跟踪远程分支"><a href="#跟踪远程分支" class="headerlink" title="跟踪远程分支"></a>跟踪远程分支</h3><p>　　从远程分支 <code>checkout</code> 出来的本地分支，称为<em>跟踪分支(tracking branch)</em>。跟踪分支是一种和远程分支有直接联系的本地分支。在跟踪分支里输入<code>git push</code>，Git 会自行推断应该向哪个服务器的哪个分支推送数据。反过来，在这些分支里运行 <code>git pull</code> 会获取所有远程索引，并把它们的数据都合并到本地分支中来。</p><p>　　在克隆仓库时，Git 通常会自动创建一个名为 <code>master</code> 的分支来跟踪 <code>origin/master</code>。这正是<code>git push</code> 和 <code>git pull</code> 一开始就能正常工作的原因。当然，你可以随心所欲地设定为其它跟踪分支，比如<code>origin</code> 上除了 <code>master</code> 之外的其它分支。刚才我们已经看到了这样的一个例子：<code>git checkout -b [分支名] [远程名]/[分支名]</code>。如果你有 1.6.2 以上版本的 Git，还可以用<code>--track</code> 选项简化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout --track origin/serverfix</span><br><span class="line">Branch serverfix set up to track remote branch refs/remotes/origin/serverfix.</span><br><span class="line">Switched to a new branch &quot;serverfix&quot;</span><br></pre></td></tr></table></figure></p><p>　　要为本地分支设定不同于远程分支的名字，只需在前个版本的命令里换个名字：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b sf origin/serverfix</span><br><span class="line">Branch sf set up to track remote branch refs/remotes/origin/serverfix.</span><br><span class="line">Switched to a new branch &quot;sf&quot;</span><br></pre></td></tr></table></figure></p><p>　　现在你的本地分支 <code>sf</code> 会自动向 <code>origin/serverfix</code> 推送和抓取数据了。</p><h3 id="删除远程分支"><a href="#删除远程分支" class="headerlink" title="删除远程分支"></a>删除远程分支</h3><p>如果不再需要某个远程分支了，比如搞定了某个特性并把它合并进了远程的 <code>master</code> 分支（或任何其他存放稳定代码的地方），可以用这个非常无厘头的语法来删除它：<code>git push [远程名] :[分支名]</code>。如果想在服务器上删除<code>serverfix</code> 分支，运行下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin :serverfix</span><br><span class="line">To git@github.com:schacon/simplegit.git </span><br><span class="line">- [deleted]         serverfix</span><br></pre></td></tr></table></figure></p><p>咚！服务器上的分支没了。你最好特别留心这一页，因为你一定会用到那个命令，而且你很可能会忘掉它的语法。有种方便记忆这条命令的方法：记住我们不久前见过的 <code>git push [远程名] [本地分支]:[远程分支]</code> 语法，如果省略 <code>[本地分支]</code>，那就等于是在说“在这里提取空白然后把它变成<code>[远程分支]</code>”。</p><h1 id="3-6-分支的衍合"><a href="#3-6-分支的衍合" class="headerlink" title="3.6 分支的衍合"></a>3.6 分支的衍合</h1><p>　　把一个分支整合到另一个分支的办法有两种：<code>merge</code> 和 <code>rebase</code>（译注：<code>rebase</code> 的翻译暂定为“衍合”，大家知道就可以了。）。在本章我们会学习什么是衍合，如何使用衍合，为什么衍合操作如此富有魅力，以及我们应该在什么情况下使用衍合。</p><h3 id="基本的衍合操作"><a href="#基本的衍合操作" class="headerlink" title="基本的衍合操作"></a>基本的衍合操作</h3><p>　　请回顾之前有关合并的一节（见图 3-27），你会看到开发进程分叉到两个不同分支，又各自提交了更新。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121738_258.png" alt="Git详解之三 Git分支 "></p><center>图 3-27. 最初分叉的提交历史。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　之前介绍过，最容易的整合分支的方法是 <code>merge</code> 命令，它会把两个分支最新的快照（C3 和 C4）以及二者最新的共同祖先（C2）进行三方合并，合并的结果是产生一个新的提交对象（C5）。如图 3-28 所示：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121738_893.png" alt="Git详解之三 Git分支 "></p><center>图 3-28. 通过合并一个分支来整合分叉了的历史。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　其实，还有另外一个选择：你可以把在 C3 里产生的变化补丁在 C4 的基础上重新打一遍。在 Git 里，这种操作叫做<em>衍合（rebase）</em>。有了 <code>rebase</code> 命令，就可以把在一个分支里提交的改变移到另一个分支里重放一遍。</p><p>　　在上面这个例子中，运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout experiment</span><br><span class="line">$ git rebase master</span><br><span class="line">First, rewinding head to replay your work on top of it...</span><br><span class="line">Applying: added staged command</span><br></pre></td></tr></table></figure></p><p>　　它的原理是回到两个分支最近的共同祖先，根据当前分支（也就是要进行衍合的分支 <code>experiment</code>）后续的历次提交对象（这里只有一个 C3），生成一系列文件补丁，然后以基底分支（也就是主干分支<code>master</code>）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3’），从而改写 <code>experiment</code> 的提交历史，使它成为 <code>master</code> 分支的直接下游，如图 3-29 所示：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121739_386.png" alt="Git详解之三 Git分支 "></p><center>图 3-29. 把 C3 里产生的改变到 C4 上重演一遍。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>现在回到 <code>master</code> 分支，进行一次快进合并（见图 3-30）：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121739_29.png" alt="Git详解之三 Git分支 "></p><center>图 3-30. master 分支的快进。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　现在的 C3’ 对应的快照，其实和普通的三方合并，即上个例子中的 C5 对应的快照内容一模一样了。虽然最后整合得到的结果没有任何区别，但衍合能产生一个更为整洁的提交历史。如果视察一个衍合过的分支的历史记录，看起来会更 清楚：仿佛所有修改都是在一根线上先后进行的，尽管实际上它们原本是同时并行发生的。</p><p>　　一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁 — 比如某些项目你不是维护者，但想帮点忙的话，最好用衍合：先在自己的一个分支里进行开发，当准备向主项目提交补丁的时候，根据最新的<code>origin/master</code> 进行一次衍合操作然后再提交，这样维护者就不需要做任何整合工作（译注：实际上是把解决分支补丁同最新主干代码之间冲突的责任，化转为由提交补丁的人来解决。），只需根据你提供的仓库地址作一次快进合并，或者直接采纳你提交的补丁。</p><p>　　请注意，合并结果中最后一次提交所指向的快照，无论是通过衍合，还是三方合并，都会得到相同的快照内容，只不过提交历史不同罢了。衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。</p><h3 id="有趣的衍合"><a href="#有趣的衍合" class="headerlink" title="有趣的衍合"></a>有趣的衍合</h3><p>　　衍合也可以放到其他分支进行，并不一定非得根据分化之前的分支。以图 3-31 的历史为例，我们为了给服务器端代码添加一些功能而创建了特性分支 <code>server</code>，然后提交 C3 和 C4。然后又从 C3 的地方再增加一个<code>client</code> 分支来对客户端代码进行一些相应修改，所以提交了 C8 和 C9。最后，又回到 <code>server</code> 分支提交了 C10。</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121740_319.png" alt="Git详解之三 Git分支 "></p><center>图 3-31. 从一个特性分支里再分出一个特性分支的历史。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　假设在接下来的一次软件发布中，我们决定先把客户端的修改并到主线中，而暂缓并入服务端软件的修改（因为还需要进一步测试）。这个时候，我们就可以把基于 <code>server</code> 分支而非 <code>master</code> 分支的改变（即 C8 和 C9），跳过 <code>server</code> 直接放到<code>master</code> 分支中重演一遍，但这需要用 <code>git rebase</code> 的 <code>--onto</code> 选项指定新的基底分支<code>master</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rebase --onto master server client</span><br></pre></td></tr></table></figure><p>　　这好比在说：“取出 <code>client</code> 分支，找出 <code>client</code> 分支和 <code>server</code> 分支的共同祖先之后的变化，然后把它们在<code>master</code> 上重演一遍”。是不是有点复杂？不过它的结果如图 3-32 所示，非常酷（译注：虽然 <code>client</code> 里的 C8, C9 在 C3 之后，但这仅表明时间上的先后，而非在 C3 修改的基础上进一步改动，因为<code>server</code> 和 <code>client</code> 这两个分支对应的代码应该是两套文件，虽然这么说不是很严格，但应理解为在 C3 时间点之后，对另外的文件所做的 C8，C9 修改，放到主干重演。）：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121740_891.png" alt="Git详解之三 Git分支 "></p><center>图 3-32. 将特性分支上的另一个特性分支衍合到其他分支。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>现在可以快进 <code>master</code> 分支了（见图 3-33）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">$ git merge client</span><br></pre></td></tr></table></figure><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121741_142.png" alt="Git详解之三 Git分支 "></p><center>图 3-33. 快进 master 分支，使之包含 client 分支的变化。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　现在我们决定把 <code>server</code> 分支的变化也包含进来。我们可以直接把 <code>server</code> 分支衍合到 <code>master</code>，而不用手工切换到 <code>server</code> 分支后再执行衍合操作 — <code>git rebase [主分支] [特性分支]</code>命令会先取出特性分支<code>server</code>，然后在主分支 <code>master</code> 上重演：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git rebase master server</span><br></pre></td></tr></table></figure><p>于是，<code>server</code> 的进度应用到 <code>master</code> 的基础上，如图 3-34 所示：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121742_594.png" alt="Git详解之三 Git分支 "></p><center>图 3-34. 在 master 分支上衍合 server 分支。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>然后就可以快进主干分支 <code>master</code> 了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">$ git merge server</span><br></pre></td></tr></table></figure><p>现在 <code>client</code> 和 <code>server</code> 分支的变化都已经集成到主干分支来了，可以删掉它们了。最终我们的提交历史会变成图 3-35 的样子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -d client</span><br><span class="line">$ git branch -d server</span><br></pre></td></tr></table></figure><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121743_361.png" alt="Git详解之三 Git分支 "></p><center>图 3-35. 最终的提交历史</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><h3 id="衍合的风险"><a href="#衍合的风险" class="headerlink" title="衍合的风险"></a>衍合的风险</h3><p>　　呃，奇妙的衍合也并非完美无缺，要用它得遵守一条准则：</p><p>　　一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行衍合操作。</p><p>　　如果你遵循这条金科玉律，就不会出差错。否则，人民群众会仇恨你，你的朋友和家人也会嘲笑你，唾弃你。</p><p>　　在进行衍合的时候，实际上抛弃了一些现存的提交对象而创造了一些类似但不同的新的提交对象。如果你把原来分支中的提交对象发布出去，并且其他人更新下载后在其基础上开展工作，而稍后你又用<code>git rebase</code> 抛弃这些提交对象，把新的重演后的提交对象发布出去的话，你的合作者就不得不重新合并他们的工作，这样当你再次从他们那里获取内容时，提交历史就会变得一团糟。</p><p>　　下面我们用一个实际例子来说明为什么公开的衍合会带来问题。假设你从一个中央服务器克隆然后在它的基础上搞了一些开发，提交历史类似图 3-36 所示：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121743_370.png" alt="Git详解之三 Git分支 "></p><center>图 3-36. 克隆一个仓库，在其基础上工作一番。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>现在，某人在 C1 的基础上做了些改变，并合并他自己的分支得到结果 C6，推送到中央服务器。当你抓取并合并这些数据到你本地的开发分支中后，会得到合并结果 C7，历史提交会变成图 3-37 这样：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121744_939.png" alt="Git详解之三 Git分支 "></p><center>图 3-37. 抓取他人提交，并入自己主干。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>接下来，那个推送 C6 上来的人决定用衍合取代之前的合并操作；继而又用 <code>git push --force</code> 覆盖了服务器上的历史，得到 C4’。而之后当你再从服务器上下载最新提交后，会得到：</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121745_75.png" alt="Git详解之三 Git分支 "></p><center>图 3-38. 有人推送了衍合后得到的 C4’，丢弃了你作为开发基础的 C4 和 C6。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>下载更新后需要合并，但此时衍合产生的提交对象 C4’ 的 SHA-1 校验值和之前 C4 完全不同，所以 Git 会把它们当作新的提交对象处理，而实际上此刻你的提交历史 C7 中早已经包含了 C4 的修改内容，于是合并操作会把 C7 和 C4’ 合并为 C8（见图 3-39）:</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121746_276.png" alt="Git详解之三 Git分支 "></p><center>图 3-39. 你把相同的内容又合并了一遍，生成一个新的提交 C8。</center><p>　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　</p><p>　　C8 这一步的合并是迟早会发生的，因为只有这样你才能和其他协作者提交的内容保持同步。而在 C8 之后，你的提交历史里就会同时包含 C4 和 C4’，两者有着不同的 SHA-1 校验值，如果用<code>git log</code> 查看历史，会看到两个提交拥有相同的作者日期与说明，令人费解。而更糟的是，当你把这样的历史推送到服务器后，会再次把这些衍合后的提交引入到中央服务 器，进一步困扰其他人（译注：这个例子中，出问题的责任方是那个发布了 C6 后又用衍合发布 C4’ 的人，其他人会因此反馈双重历史到共享主干，从而混淆大家的视听。）。</p><p>　　如果把衍合当成一种在推送之前清理提交历史的手段，而且仅仅衍合那些尚未公开的提交对象，就没问题。如果衍合那些已经公开的提交对象，并且已经有人基于这些提交对象开展了后续开发工作的话，就会出现叫人沮丧的麻烦。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>　　读到这里，你应该已经学会了如何创建分支并切换到新分支，在不同分支间转换，合并本地分支，把分支推送到共享服务器上，使用共享分支与他人协作，以及在分享之前进行衍合。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git知识（二）：实战篇</title>
      <link href="/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AE%9E%E6%88%98%E7%AF%87/"/>
      <url>/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%AE%9E%E6%88%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><p>　　本章介绍开始使用 Git 前的相关知识。我们会先了解一些版本控制工具的历史背景，然后试着让 Git 在你的系统上跑起来，直到最后配置好，可以正常开始开发工作。读完本章，你就会明白为什么 Git 会如此流行，为什么你应该立即开始使用它。</p><h1 id="关于版本控制"><a href="#关于版本控制" class="headerlink" title="关于版本控制"></a>关于版本控制</h1><p>　　什么是版本控制？我真的需要吗？版本控制是一种记录若干文件内容变化，以便将来查阅特定版本修订情况的系统。在本书所展示的例子中，我们仅对保存着软件源代码的文本文件作版本控制管理，但实际上，你可以对任何类型的文件进行版本控制。</p><p>　　如果你是位图形或网页设计师，可能会需要保存某一幅图片或页面布局文件的所有修订版本（这或许是你非常渴望拥有的功能）。采用版本控制系统 （VCS）是个明智的选择。有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态。你可以比较文件的变化细节，查出最 后是谁修改了哪个地方，从而导致出现怪异问题，又是谁在何时报告了某个功能缺陷等等。使用版本控制系统通常还意味着，就算你乱来一气把整个项目中的文件改 的改删的删，你也照样可以轻松恢复到原先的样子。但额外增加的工作量却微乎其微。<br><a id="more"></a></p><h3 id="本地版本控制系统"><a href="#本地版本控制系统" class="headerlink" title="本地版本控制系统"></a>本地版本控制系统</h3><p>　　许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。</p><p>　　为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异</p><p>　　　　　　　　　　　　　　　　　　　　　<img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121201_805.png" alt="Git详解之一 Git起步 "></p><p>　　其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。甚至在流行的 Mac OS X 系统上安装了开发者工具包之后，也可以使用 rcs 命令。它的工作原理基本上就是保存并管理文件补丁（patch）。文件补丁是一种特定格式的文本文件，记录着对应文件修订前后的内容变化。所以，根据每次 修订后的补丁，rcs 可以通过不断打补丁，计算出各个版本的文件内容。</p><h3 id="集中化的版本控制系统"><a href="#集中化的版本控制系统" class="headerlink" title="集中化的版本控制系统"></a>集中化的版本控制系统</h3><p>　　接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？于是，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这 已成为版本控制系统的标准做法（见图 1-2）。</p><p>　　　　　　　　　　　　　　　　　　　　<img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121201_94.png" alt="Git详解之一 Git起步 "></p><p>　　这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。</p><p>　　事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要 是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就还是会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，而被客户端 提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人事先完整提取出来过。本地版本控制系统也存在类似问题，只要整个项 目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。</p><h3 id="分布式版本控制系统"><a href="#分布式版本控制系统" class="headerlink" title="分布式版本控制系统"></a>分布式版本控制系统</h3><p>　　于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜 像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份（见图 1-3）。</p><p>　　　　　　　　　　　　　　　　　　<img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121202_798.png" alt="Git详解之一 Git起步 "></p><p>　　更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。</p><h1 id="Git-基础"><a href="#Git-基础" class="headerlink" title="Git 基础"></a>Git 基础</h1><p>　　那么，简单地说，Git 究竟是怎样的一个系统呢？请注意，接下来的内容非常重要，若是理解了 Git 的思想和基本工作原理，用起来就会知其所以然，游刃有余。在开始学习 Git 的时候，请不要尝试把各种概念和其他版本控制系统（诸如 Subversion 和 Perforce 等）相比拟，否则容易混淆每个操作的实际意义。Git 在保存和处理各种信息的时候，虽然操作起来的命令形式非常相近，但它与其他版本控制系统的做法颇为不同。理解这些差异将有助于你准确地使用 Git 提供的各种工具。</p><h3 id="直接记录快照，而非差异比较"><a href="#直接记录快照，而非差异比较" class="headerlink" title="直接记录快照，而非差异比较"></a>直接记录快照，而非差异比较</h3><p>　　Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统 （CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121203_850.png" alt="Git详解之一 Git起步 "></p><p>　　　　　　　　　　　　　　　　　　　　　<center>图： 其他系统在每个版本中记录着各个文件的具体差异</center></p><p>　　Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照 的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式如图所示</p><p><img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121204_39.png" alt="Git详解之一 Git起步 "></p><p>　　　　　　　　　　　　　　　　　　　　　<center>图： Git 保存每次更新时的文件快照</center></p><p>　　这是 Git 同其他系统的重要区别。它完全颠覆了传统版本控制的套路，并对各个环节的实现方式作了新的设计。Git 更像是个小型的文件系统，但它同时还提供了许多以此为基础的超强工具，而不只是一个简单的 VCS。稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。</p><h3 id="近乎所有操作都是本地执行"><a href="#近乎所有操作都是本地执行" class="headerlink" title="近乎所有操作都是本地执行"></a>近乎所有操作都是本地执行</h3><p>　　在 Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。但如果用 CVCS 的话，差不多所有操作都需要连接网络。因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。</p><p>　　举个例子，如果要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。所以任何时候你都可以马上翻阅，无需等待。如果想要看当前版本的文件和一个月 前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算，而不用请求远程服务器来做这件事，或是把老版本的文件拉到本地来作比较。</p><p>　　用 CVCS 的话，没有网络或者断开 VPN 你就无法做任何事情。但用 Git 的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程仓库。同样，在回家的路上，不用连接 VPN 你也可以继续工作。换作其他版本控制系统，这么做几乎不可能，抑或非常麻烦。比如 Perforce，如果不连到服务器，几乎什么都做不了（译注：默认无法发出命令<code>p4 edit file</code> 开始编辑文件，因为 Perforce 需要联网通知系统声明该文件正在被谁修订。但实际上手工修改文件权限可以绕过这个限制，只是完成后还是无法提交更新。）；如果是 Subversion 或 CVS，虽然可以编辑文件，但无法提交更新，因为数据库在网络上。看上去好像这些都不是什么大问题，但实际体验过之后，你就会惊喜地发现，这其实是会带来很大不同的。</p><h3 id="时刻保持数据完整性"><a href="#时刻保持数据完整性" class="headerlink" title="时刻保持数据完整性"></a>时刻保持数据完整性</h3><p>　　在保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。换句话说，不可能在你修改了文件或目录之后，Git 一无所知。这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致文件数据缺失，Git 都能立即察觉。</p><p>Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">24b9da6552252987aa493b52f8696cd6d3b00373</span><br></pre></td></tr></table></figure><p>　　Git 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。</p><h3 id="多数操作仅添加数据"><a href="#多数操作仅添加数据" class="headerlink" title="多数操作仅添加数据"></a>多数操作仅添加数据</h3><p>　　常用的 Git 操作大多仅仅是把数据添加到数据库。因为任何一种不可逆的操作，比如删除数据，都会使回退或重现历史版本变得困难重重。在别的 VCS 中，若还未提交更新，就有可能丢失或者混淆一些修改的内容，但在 Git 里，一旦提交快照之后就完全不用担心丢失数据，特别是养成定期推送到其他仓库的习惯的话。</p><p>这种高可靠性令我们的开发工作安心不少，尽管去做各种试验性的尝试好了，再怎样也不会弄丢数据。至于 Git 内部究竟是如何保存和恢复数据的，我们会在第九章讨论 Git 内部原理时再作详述。</p><h3 id="文件的三种状态"><a href="#文件的三种状态" class="headerlink" title="文件的三种状态"></a>文件的三种状态</h3><p>　　好，现在请注意，接下来要讲的概念非常重要。对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）。已提交表示该文件已经被安全地保存在本地数据库 中了；已修改表示修改了某个文件，但还没有提交保存；已暂存表示把已修改的文件放在下次提交时要保存的清单中。</p><p>由此我们看到 Git 管理项目时，文件流转的三个工作区域：Git 的工作目录，暂存区域，以及本地仓库。</p><p>　　　　　　　　　　　　　　　　　　<img src="http://static.open-open.com/lib/uploadImg/20120201/20120201121205_151.png" alt="Git详解之一 Git起步 "></p><p>　　　　　　　　　　　　　　　　　　　　　　<center>图： 工作目录，暂存区域，以及本地仓库</center></p><p>　　每个项目都有一个 Git 目录（译注：如果 <code>git clone</code> 出来的话，就是其中 <code>.git</code> 的目录；如果<code>git clone --bare</code> 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。</p><p>　　从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。</p><p>所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。</p><h1 id="Git-工作流程如下"><a href="#Git-工作流程如下" class="headerlink" title="Git 工作流程如下"></a>Git 工作流程如下</h1><p>　　1. 在工作目录中修改某些文件。</p><p>　　2. 对修改后的文件进行快照，然后保存到暂存区域。</p><p>　　3. 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。</p><p>　　所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修</p><p>改但还没有放到暂存区域，就 是已修改状态。到第二章的时候，我们会进一步了解其中细节，并学会如何根据文件状态实施后续操作，以及怎样跳过暂存直接提交。</p><p>在正式使用前，我们还需要弄清楚Git的三种重要模式，分别是<strong>已提交、已修改、已暂存</strong></p><p><strong>　　　　　　　　　　　　<img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174201623-1776071686.png" alt=""></strong></p><p>　　已提交(committed):表示数据文件已经顺利提交到Git数据库中。</p><p>　　已修改(modified):表示数据文件已经被修改，但未被保存到Git数据库中。</p><p>　　已暂存(staged):表示数据文件已经被修改，并会在下次提交时提交到Git数据库中。</p><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><ol><li><p>先创建一个工程的目录mkdir test_project</p></li><li><p>cd test_project</p></li><li><p>git init 初始化git工作目录（git init –bare功能相同）</p></li></ol><p>git init的结果（这个隐藏的git目录里面的内容和–bare创建的相同）</p><p><img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174416513-612893373.png" alt=""></p><p>git init –bare 路径</p><p><img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174424076-543342440.png" alt=""></p><ol start="4"><li><p>touch readme 创建一个文件</p></li><li><p>git status 查看状态</p></li></ol><p>　　第一次查看，这个文件还没有添加到暂存区的</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174449216-6923784.png" alt=""></p><p>6.git add readme 将readme文件添加到暂存区</p><p>　　既然有添加，那就有删除（此处说的是暂存区的操作，不会删除文件）</p><p>　　git rm –cached readme 　　</p><p>7.git status 再次查看暂存区的状态</p><p>　　将readme添加到暂存区后的状态</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174509185-269205270.png" alt=""></p><ol start="8"><li><p>git commit -m “first commit” 提交到自己的中央仓库（init就是创建自己的中央仓库）</p></li><li><p>git log查看日志（相当与svn的提交日志）</p></li></ol><p>　　　　 <img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174535123-619959998.png" alt=""></p><p>　　到目前为止自己本地仓库就提交结束了</p><p>　　之后就是提交到远程仓库了</p><p>10 git remote –v 查看本地存储的远程仓库信息，如果是clone出来的工程这个结果如下</p><p>　　　　 <img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174555513-2058573015.png" alt=""></p><p>　　origin 表示的是远程仓库的别名（默认为origin，也可以自己起，fetch更新类似于update，push推数据相当于commit）</p><p>　　如果不是clone的工程，就不会有任何结果，要自己添加，命令如下：</p><p>　　git remote add test ssh:<a href="mailto://root@10.0.0.5" target="_blank" rel="noopener">//root@10.0.0.5</a>/usr/GitData/DingDang/.git</p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174606123-1903416899.png" alt=""></p><p>　　　　<img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174622435-717896173.png" alt=""></p><p>11.做完这步然后就是远程推数据了（必须保证本地仓库里面有提交，注意是本地仓库而不是暂存区）</p><p>　　git push test</p><p>　　到此自己创建的文件就推到了远程的git仓库了</p><p>12 还有一个功能比较重要，本地仓库的版本回退</p><p>　　git reset –hard HEAD^ #还原历史提交版本上一次</p><p>　　git reset –hard 版本号 #就是上图黄色的部分，仅需要前7位即可</p><p>　如果回退过头了，log是看不到未来的版本号的，想看可以用git reflog查看</p><p>相关参考文档：<a href="https://docs.gitlab.com/" target="_blank" rel="noopener">https://docs.gitlab.com</a></p><p>分支参考文档：<a href="http://www.cnblogs.com/haore147/p/3604464.html" target="_blank" rel="noopener">http://www.cnblogs.com/haore147/p/3604464.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git知识（一）：基础篇</title>
      <link href="/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
      <url>/2018/06/05/Git%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>　　Git是一个分布式的版本控制工具，本篇文章从介绍Git开始，重点在于介绍Git的基本命令和使用技巧，让你尝试使用Git的同时，体验到原来一个版 本控制工具可以对开发产生如此之多的影响，文章分为两部分，第一部分介绍Git的一些常用命令，其中穿插介绍Git的基本概念和原理，第二篇重点介绍 Git的使用技巧，最后会在Git Hub上创建一个开源项目开启你的Git实战之旅</p><h1 id="Git是什么"><a href="#Git是什么" class="headerlink" title="Git是什么"></a>Git是什么</h1><p>Git在Wikipedia上的定义：它是一个免费的、分布式的版本控制工具，或是一个强调了速度快的源代码管理工具。Git最初被Linus Torvalds开发出来用于管理Linux内核的开发。每一个Git的工作目录都是一个完全独立的代码库，并拥有完整的历史记录和版本追踪能力，不依赖 于网络和中心服务器。<br><a id="more"></a><br>Git的出现减轻了许多开发者和开源项目对于管理分支代码的压力，由于对分支的良好控制，更鼓励开发者对自己感兴趣的项目做出贡献。其实许多开源项目 包括Linux kernel, Samba, X.org Server, Ruby on Rails，都已经过渡到使用Git作为自己的版本控制工具。对于我们这些喜欢写代码的开发者嘛，有两点最大的好处，我们可以在任何地点(在上班的地铁 上)提交自己的代码和查看代码版本;我们可以开许许多多个分支来实践我们的想法，而合并这些分支的开销几乎可以忽略不计。</p><h1 id="Git-初始化"><a href="#Git-初始化" class="headerlink" title="Git 初始化"></a>Git 初始化</h1><p>现在进入本篇文章真正的主题，介绍一下Git的基本命令和操作，会从Git的版本库的初始化，基本操作和独有的常用命令三部分着手，让大家能够开始使用Git。</p><p>Git通常有两种方式来进行初始化:</p><p><strong>git clone</strong>: 这是较为简单的一种初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份，例如’git clone git://github.com/someone/some_project.git some_project’命令就是将’git://github.com/someone/some_project.git’这个URL地址的远程版 本库完全克隆到本地some_project目录下面</p><p><strong>git init</strong>和<strong>git remote</strong>：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用 git init 命令进行初始化，Git以后就会对该目录下的文件进行版本控制，这时候如果你需要将它放到远程服务器上，可以在远程服务器上创建一个目录，并把 可访问的URL记录下来，此时你就可以利用 git remote add 命令来增加一个远程服务器端，例如’git remote add origin git://github.com/someone/another_project.git’这条命令就会增加URL地址为’git: //github.com/someone/another_project.git’，名称为origin的远程服务器，以后提交代码的时候只需要使用 origin别名即可</p><h1 id="Git-基本命令"><a href="#Git-基本命令" class="headerlink" title="Git 基本命令"></a>Git 基本命令</h1><p>现在我们有了本地和远程的版本库，让我们来试着用用Git的基本命令吧：</p><p><strong>git pull</strong>：从版本库(既可以是远程的也可以是本地的)将代码更新到本地，例如：’git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝，该功能类似于SVN的update</p><p><strong>git add</strong>：将所有改动的文件（新增和有变动的）放在暂存区，由git进行管理</p><p><strong>git rm</strong>：从当前的工作空间中和索引中删除文件，例如’git rm app/model/user.rb’，移除暂存区</p><p><strong>git commit</strong>：提交当前工作空间的修改内容，类似于SVN的commit命令，例如’git commit -m  “story #3, add user model”‘，提交的时候必须用-m来输入一条提交信息</p><p><strong>git push</strong>：将本地commit的代码更新到远程版本库中，例如’git push origin branchname’就会将本地的代码更新到名为orgin的远程版本库中</p><p><strong>git log</strong>：查看历史日志</p><p><strong>git revert</strong>：还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值、</p><p>上面的命令几乎都是每个版本控制工具所公有的，下面就开始尝试一下Git独有的一些命令：</p><h1 id="Git-独有命令"><a href="#Git-独有命令" class="headerlink" title="Git 独有命令"></a>Git 独有命令</h1><p><strong>git branch</strong>：对分支的增、删、查等操作，例如 git branch new_branch 会从当前的工作版本创建一个叫做new_branch的新分支，git branch -D new_branch 就会强制删除叫做new_branch的分支，git branch 就会列出本地所有的分支</p><p><strong>git checkout</strong>：Git的checkout有两个作用，其一是在 不同的branch之间进行切换，例如 ‘git checkout new_branch’就会切换到new_branch的分支上去;另一个功能是 还原代码的作用，例如git checkout app/model/user.rb 就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚</p><p><strong>git rebase</strong>：用下面两幅图解释会比较清楚一些，rebase命令执行后，实际上是将分支点从C移到了G，这样分支也就具有了从C到G的功能 （使历史更加简洁明了）</p><p><img src="http://static.open-open.com/lib/uploadImg/20120328/20120328111440_202.png" alt="Git使用基础篇 "></p><p><strong>git reset</strong>：回滚到指定的版本号，我们有A-G提交的版本，其中C 的版本号是 bbaf6fb，我们执行了’git reset bbaf6fb’那么结果就只剩下了A-C三个提交的版本</p><p><img src="http://static.open-open.com/lib/uploadImg/20120328/20120328111443_111.png" alt="Git使用基础篇 "></p><p><strong>git stash</strong>：将当前未提交的工作存入Git工作栈中，时机成熟的时候再应用回来，这里暂时提一下这个命令的用法，后面在技巧篇会重点讲解</p><p><strong>git config</strong>：新增、更改Git的各种设置，例如：git config branch.master.remote origin 就将master的远程版本库设置为别名叫做origin版本库</p><p><strong>git tag</strong>：将某个版本打上一个标签，例如：git tag revert_version bbaf6fb50 来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了</p><h1 id="Git-其他命令"><a href="#Git-其他命令" class="headerlink" title="Git 其他命令"></a>Git 其他命令</h1><p><strong>add</strong> #添加文件内容至索引</p><p><strong>branch</strong> #列出、创建或删除分支</p><p><strong>checkout</strong> #检出一个分支或路径到工作区</p><p><strong>clone</strong> #克隆一个版本库到一个新目录</p><p><strong>commit</strong> #最近一次的提交，–amend修改最近一次提交说明</p><p><strong>diff</strong> #显示提交之间、提交和工作区之间等的差异　　</p><p><strong>fetch</strong> #从另外一个版本库下载对象和引用　</p><p><strong>init</strong> #创建一个空的 Git 版本库或重新初始化一个已存在的版本库</p><p><strong>log</strong> #显示提交日志 –stat 具体文件的改动</p><p><strong>reflog</strong>#记录丢失的历史</p><p><strong>merge</strong> #合并两个或更多开发历史，–squash 把分支所有提交合并成一个提交</p><p><strong>mv</strong> #移动或重命名一个文件、目录或符号链接</p><p><strong>pull</strong> #获取并合并另外的版本库或一个本地分支（相当于git fetch和git merge）</p><p><strong>push</strong> #更新远程引用和相关的对象　　</p><p><strong>rebase</strong> #本地提交转移至更新后的上游分支中</p><p><strong>reset</strong> #重置当前HEAD到指定状态</p><p><strong>rm</strong> #从工作区和索引中删除文件</p><p><strong>show</strong> #显示各种类型的对象</p><p><strong>status</strong> #显示工作区状态</p><p><strong>tag</strong> #创建、列出、删除或校验一个GPG签名的 tag 对象</p><p><strong>cherry-pick </strong>#从其他分支复制指定的提交，然后导入到现在的分支</p><p><strong>git merge –squash issue1</strong></p><h1 id="git分支命令"><a href="#git分支命令" class="headerlink" title="git分支命令"></a>git分支命令</h1><p>创建分支：</p><p>git branch linux #创建分支</p><p>git checkout linux #切换分支</p><p>git branch #查看当前分支情况,当前分支前有*号</p><p>git add readme.txt #提交到暂存区</p><p>git commit -m “new branch” #提交到git版本仓库</p><p>git checkout master #我们在提交文件后再切回master分支  </p><p>分支合并：（合并前必须保证在master主干上）</p><p><strong>git branch</strong> 　　　　　　 #查看在哪个位置</p><p><strong>git merge Linux</strong>　　　　#合并创建的Linux分支（–no–ff默认情况下，Git执行”快进式合并”（fast-farward merge），会直接将Master分支指向Develop分支。使用–no–ff参数后，会执行正常合并，在Master分支上生成一个新节点。）</p><p><strong>git branch -d linux</strong>　　 #确认合并后删除分支</p><p>如果有冲突：</p><p><strong>git merge linux</strong> 　　　　#合并Linux分支(冲突)</p><p>Auto-merging readme.txt</p><p>CONFLICT (content): Merge conflict in readme.txt</p><p>Automatic merge failed; fix conflicts and then commit the result.</p><p>　　#那么此时，我们在master与linux分支上都分别对中readme文件进行了修改并提交了，那这种情况下Git就没法再为我们自动的快速合并了，它只能告诉我们readme文件的内容有冲突，需要手工处理冲突的内容后才能继续合并</p><p>自己修改完readme.txt文件后再次提交</p><center class="half"><br>　　<img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174759701-1329948263.png" alt=""><br>    <img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174805904-1910142845.png" alt=""><br></center><br>　　　　　　<center>–no–ff原理图</center><h1 id="git全局配置"><a href="#git全局配置" class="headerlink" title="git全局配置"></a>git全局配置</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum install git                                       #安装Git</span><br><span class="line">git config –global user.name “xubusi”                 #配置git使用用户</span><br><span class="line">git config –global user.email “xubusi@mail.com”       #配置git使用邮箱</span><br><span class="line">git config –global color.ui true                      #加颜色</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">git config –list                                      #所有配置的信息（上面的结果）</span><br><span class="line">user.name=xubusi</span><br><span class="line">user.email=xubusi@mail.com</span><br><span class="line">color.ui=true</span><br></pre></td></tr></table></figure><h1 id="git目录结构"><a href="#git目录结构" class="headerlink" title=".git目录结构"></a><strong>.git目录结构</strong></h1><p>Git之所以能够提供方便的本地分支等特性，是与它的文件存储机制有关的。Git存储版本控制信息时使用它自己定义的一套文件系统存储机制，在代码根目录下有一个.git文件夹，会有如下这样的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HEAD</span><br><span class="line">branches/ config</span><br><span class="line">description</span><br><span class="line">hooks/ index info/ objects/ refs/</span><br></pre></td></tr></table></figure><p><img src="http://static.open-open.com/lib/uploadImg/20120328/20120328111443_875.png" alt="Git使用基础篇 "></p><p>有几个比较重要的文件和目录需要解释一下：</p><p>　　 <strong>HEAD</strong>：文件存放根节点的信息，其实目录结构就表示一个树型结构，Git采用这种树形结构来存储版本信息， 那么HEAD就表示根;</p><p>　　 <strong>refs</strong>：目录存储了你在当前版本控制目录下的各种不同引用(引用指的是你本地和远程所用到的各个树分支的信息)，它有heads、 remotes、stash、tags四个子目录，分别存储对不同的根、远程版本库、Git栈和标签的四种引用，你可以通过命令’git show-ref’更清晰地查看引用信息;</p><p>　　 <strong>logs</strong>：目录根据不同的引用存储了日志信息。因此，Git只需要代码根目录下的这一个.git目录就可以记录完 整的版本控制信息，而不是像SVN那样根目录和子目录下都有.svn目录。那么下面就来看一下Git与SVN的区别吧</p><p>　　 <strong>.gitigmore:</strong> 放一些不需要git管理的文件（例：IDE的工作目录 .idea，）<a href="https://sysrepo.byted.org/devops/idchelper/chamber-fe/blob/dev-duyichen/.gitignore" target="_blank" rel="noopener">  </a></p><h1 id="Git与SVN的不同"><a href="#Git与SVN的不同" class="headerlink" title="Git与SVN的不同"></a>Git与SVN的不同</h1><p>SVN(Subversion)是当前使用最多的版本控制工具。与它相比较，Git最大的优势在于两点：<strong>易于本地增加分支和分布式的特性</strong>。</p><p>下面两幅图可以形象的展示Git与SVN的不同之处</p><p><img src="http://static.open-open.com/lib/uploadImg/20120328/20120328111443_802.png" alt="Git使用基础篇 "> <img src="http://static.open-open.com/lib/uploadImg/20120328/20120328111444_659.png" alt="Git使用基础篇 "></p><p>GIT对于易于本地增加分支，图中Git本地和服务器端结构都很灵活，所有版本都存储在一个目录中，你只需要进行分支的切换即可达到在某个分支工作的效果。</p><p>　　SVN则完全不同，如果你需要在本地试验一些自己的代码，只能本地维护多个不同的拷贝，每个拷贝对应一个SVN服务器地址。</p><p>　　分布式对于Git而言，你可以本地提交代码，所以在上面的图中，Git有利于将一个大任务分解，进行本地的多次提交，而SVN只能在本地进行大量的一 次性更改，导致将来合并到主干上造成巨大的风险。Git的代码日志是在本地的，可以随时查看。SVN的日志在服务器上的，每次查看日志需要先从服务器上下 载下来。我工作的小组，代码服务器在美国，每次查看小组几年前所做的工作时，日志下载就需要十分钟，这不能不说是一个痛苦。后来我们迁移到Git上，利用 Git日志在本地的特性，我用Ruby编写了一个Rake脚本，可以查看某个具体任务的所有代码历史，每次只需要几秒钟，大大方便我的工作。当然分布式并 不是说用了Git就不需要一个代码中心服务器，如果你工作在一个团队里，还是需要一个服务器来保存所有的代码的。</p><p><strong>实际的例子：</strong></p><p>　　以前我所 在的小组使用SVN作为版本控制工具，当我正在试图增强一个模块，工作做到一半，由于会改变原模块的行为导致代码服务器上许多测试的失败，所以并没有提交 代码。这时候上级对我说，现在有一个很紧急的Bug需要处理， 必须在两个小时内完成。我只好将本地的所有修改diff，并输出成为一个patch文件，然后回滚有关当前任务的所有代码，再开始修改Bug的任务，等到 修改好后，在将patch应用回来。前前后后要完成多个繁琐的步骤，这还不计中间代码发生冲突所要进行的工作量。</p><p>　　可是如果使用Git， 我们只需要开一个分支或者转回到主分支上，就可以随时开始Bug修改的任务，完成之后，只要切换到原来的分支就可以优雅的继续以前的任务。只要你愿意，每 一个新的任务都可以开一个分支，完成后，再将它合并到主分支上，轻松而优雅。</p><h1 id="gitlab介绍"><a href="#gitlab介绍" class="headerlink" title="gitlab介绍"></a>gitlab介绍</h1><h2 id="安装服务相关命令"><a href="#安装服务相关命令" class="headerlink" title="安装服务相关命令"></a>安装服务相关命令</h2><p>安装有可能的依赖：</p><p>yum install openssh-server</p><p>yum install postfix</p><p>yum install cronie</p><p>安装gitlab：</p><p>curl -sS <a href="https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh" target="_blank" rel="noopener">https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh</a> #下载数据源</p><p>yum install gitlab-ce</p><p>安装完成后：</p><p>gitlab-ctl reconfigure #使配置文件生效 但是会初始化除了gitlab.rb之外的所有文件</p><p>gitlab-ctl status #查看状态</p><p>gitlab-ctl stop #停服务</p><p>gitlab-ctl start #起服务</p><p>gitlab-ctl tail #查看日志的命令（Gitlab 默认的日志文件存放在/var/log/gitlab 目录下）</p><p>如下表示启动成功：（全是run，有down表示有的服务没启动成功</p><p><img src="https://images2017.cnblogs.com/blog/1232780/201711/1232780-20171101174826279-1303044367.png" alt=""></p><p>然后打开浏览器输入ip或者域名</p><h2 id="相关目录文件信息"><a href="#相关目录文件信息" class="headerlink" title="相关目录文件信息"></a>相关目录文件信息</h2><h3 id="相关目录"><a href="#相关目录" class="headerlink" title="相关目录"></a>相关目录</h3><p>.git/config #版本库特定的配置设置，可用–file修改</p><p>~/.gitconfig #用户特定的配置设置，可用–global修改</p><p>/var/opt/gitlab/git-data/repositories/root #库默认存储目录</p><p>/opt/gitlab #是gitlab的应用代码和相应的依赖程序</p><p>/var/opt/gitlab #此目录下是运行gitlab-ctl reconfigure命令编译后的应用数据和配置文件，不需要人为修改配置<br>/etc/gitlab #此目录下存放了以omnibus-gitlab包安装方式时的配置文件，这里的配置文件才需要<a href="http://www.07net01.com/tags-%E7%AE%A1%E7%90%86%E5%91%98-0.html" target="_blank" rel="noopener">管理员</a>手动编译配置<br>/var/log/gitlab #此目录下存放了gitlab各个组件产生的日志</p><p>/var/opt/gitlab/backups/ #备份文件生成的目录</p><h3 id="相关文件"><a href="#相关文件" class="headerlink" title="相关文件"></a>相关文件</h3><p>/opt/gitlab/embedded/service/gitlab-rails/config #配置文件（修改clone的ip地址）</p><p>/etc/gitlab/gitlab.rb 　　　　#设置相关选项进行配置（gitlab地址就在这）</p><p>/var/opt/gitlab/git-data 　　　　#Git存储库数据（默认</p><p><strong>总结</strong></p><p>本篇介绍了Git的基本概念、一些常用命令和原理，大家可以尝试动手体会一下，下一篇会重点介绍Git命令的使用技巧，Git附带的工具，最后会在Git Hub上创建一个开源项目。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为什么Kubernetes天然适合微服务（转载）</title>
      <link href="/2018/05/17/%E4%B8%BA%E4%BB%80%E4%B9%88Kubernetes%E5%A4%A9%E7%84%B6%E9%80%82%E5%90%88%E5%BE%AE%E6%9C%8D%E5%8A%A1%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/05/17/%E4%B8%BA%E4%BB%80%E4%B9%88Kubernetes%E5%A4%A9%E7%84%B6%E9%80%82%E5%90%88%E5%BE%AE%E6%9C%8D%E5%8A%A1%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>作者：网易云<br>链接：<a href="https://zhuanlan.zhihu.com/p/35642088" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35642088</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  </p><p>作者：刘超，网易云首席解决方案架构师</p><p>最近总在思考，为什么在支撑容器平台和微服务的竞争中，Kubernetes 会取得最终的胜出，事实上从很多角度出发三大容器平台从功能方面来看，最后简直是一模一样。（可参考<a href="https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA4OTMxODQwNA%3D%3D%26mid%3D2650979463%26idx%3D1%26sn%3Da78fc29e05c239b28aa1d0e49442916f%26chksm%3D8beaa9ecbc9d20fa76dafefbfa1c56b1e53f4ee48c7605f0c6ac19bb14cb524e150903bb9fc6%26scene%3D21%23wechat_redirect" target="_blank" rel="noopener">《容器平台选型的十大模式：Docker、DC/OS、K8S 谁与当先？》</a>）</p><p>经过一段时间的思索，以及采访了从早期就开始实践 Kubernetes 的<a href="https://link.zhihu.com/?target=https%3A//www.163yun.com/product-cloudcompute%3Ftag%3DM_zhihu_35642088" target="_blank" rel="noopener">网易云</a>架构师们后，我把反思所得总结为今天的这篇文章。</p><p><strong>一、从企业上云的三大架构看容器平台的三种视角</strong></p><p><img src="https://pic4.zhimg.com/v2-73c3d9efd91312ce671baad877ce1e75_b.jpg" alt=""><br><a id="more"></a><br>如图所示，企业上云的三大架构为 IT 架构、应用架构和数据架构，在不同的公司，不同的人、不同的角色，关注的重点不同。</p><p>对大部分的企业来讲，上云的诉求是从 IT 部门发起的，发起人往往是运维部门，他们关注计算、网络、存储，试图通过云计算服务来减轻 CAPEX 和 OPEX。</p><p>有的公司有 ToC 的业务，因而累积了大量的用户数据，公司的运营需要通过这部分数据进行大数据分析和数字化运营，因而在这些企业里面往往还需要关注数据架构。</p><p>从事互联网应用的企业，往往首先关注的是应用架构，是否能够满足终端客户的需求，带给客户良好的用户体验。业务量上往往会有短期内出现爆炸式增长的现象，因而关注高并发应用架构，并希望这个架构可以快速迭代，从而抢占风口。</p><p>在容器出现之前，这三种架构往往通过虚拟机云平台的方式解决。当容器出现之后，容器的各种良好的特性让人眼前一亮，它的轻量级、封装、标准、易迁移、易交付的特性，使得容器技术迅速被广泛使用。</p><p><img src="https://pic1.zhimg.com/v2-187eba16b5d56f0681d2fddfee4d88dd_b.jpg" alt=""></p><p>然而一千个人心中有一千个哈姆雷特，由于原来工作的关系，三类角色分别从自身的角度看到了容器的优势给自己带来的便捷。</p><p><strong>对于原来在机房里管计算、网络、存储的 IT 运维工程师来讲</strong>，容器更像是一种轻量级的运维模式，在他们看来，容器和虚拟机的最大的区别就是轻量级，启动速度快，他们往往更愿意推出虚拟机模式的容器。</p><p><strong>对于数据架构来讲</strong>，他们每天都在执行各种各样的数据计算任务，容器相对于原来的 JVM，是一种隔离性较好，资源利用率高的任务执行模式。</p><p><strong>从应用架构的角度出发</strong>，容器是微服务的交付形式，容器不仅仅是做部署的，而且是做交付的，CI/CD 中的 D 的。</p><p>所以这三种视角的人，在使用容器和选择容器平台时方法会不一样。</p><p><strong>二、Kubernetes 才是微服务和 DevOps 的桥梁</strong></p><p><strong>Swarm：IT 运维工程师</strong></p><p><img src="https://pic2.zhimg.com/v2-e09b2fc513cc51ed2652c4ea2410f5e4_b.jpg" alt=""></p><p>从 IT 运维工程师的角度来看：容器主要是轻量级、启动快，并且自动重启，自动关联，弹性伸缩的技术，使得 IT 运维工程师似乎不用再加班。</p><p>Swarm 的设计显然更加符合传统 IT 工程师的管理模式。</p><p>他们希望能够清晰地看到容器在不同机器的分布和状态，可以根据需要很方便地 SSH 到一个容器里面去查看情况。</p><p>容器最好能够原地重启，而非随机调度一个新的容器，这样原来在容器里面安装的一切都是有的。</p><p>可以很方便地将某个运行的容器打一个镜像，而非从 Dockerfile 开始，这样以后启动就可以复用在这个容器里面手动做的 100 项工作。</p><p>容器平台的集成性要好，用这个平台本来是为了简化运维的，如果容器平台本身就很复杂，像 Kubernetes 这种本身就这么多进程，还需要考虑它的高可用和运维成本，这个不划算，一点都没有比原来省事，而且成本还提高了。</p><p>最好薄薄的一层，像一个云管理平台一样，只不过更加方便做跨云管理，毕竟容器镜像很容易跨云迁移。</p><p>Swarm 的使用方式比较让 IT 工程师有熟悉的味道，其实 OpenStack 所做的事情它都能做，速度还快。</p><p><img src="https://pic4.zhimg.com/v2-e281d95bfe2e3c712f099b2f0578a73c_b.jpg" alt=""></p><p><strong>Swarm 的问题</strong></p><p>然而容器作为轻量级虚拟机，暴露出去给客户使用，无论是外部客户，还是公司内的开发，而非 IT 人员自己使用的时候，他们以为和虚拟机一样，但是发现了不一样的部分，就会有很多的抱怨。</p><p>例如自修复功能，重启之后，原来 SSH 进去手动安装的软件不见了，甚至放在硬盘上的文件也不见了，而且应用没有放在 Entrypoint 里面自动启动，自修复之后进程没有跑起来，还需要手动进去启动进程，客户会抱怨你这个自修复功能有啥用？</p><p>例如有的用户会 ps 一下，发现有个进程他不认识，于是直接 kill 掉了，结果是 Entrypoint 的进程，整个容器直接就挂了，客户抱怨你们的容器太不稳定，老是挂。</p><p>容器自动调度的时候，IP 是不保持的，所以往往重启后原来的 IP 就没了，很多用户会提需求，这个能不能保持啊，原来配置文件里面都配置的这个 IP ，挂了重启就变了，这个怎么用啊，还不如用虚拟机，至少没那么容易挂。</p><p>容器的系统盘，也即操作系统的那个盘往往大小是固定的，虽然前期可以配置，后期很难改变，而且没办法每个用户可以选择系统盘的大小。有的用户会抱怨，我们原来本来就很多东西直接放在系统盘的，这个都不能调整，叫什么云计算的弹性啊。</p><p>如果给客户说容器挂载数据盘，容器都启动起来了，有的客户想像云主机一样，再挂载一个盘，容器比较难做到，也会被客户骂。</p><p>如果容器的使用者不知道他们在用容器，当虚拟机来用，他们会觉得很难用，这个平台一点都不好。</p><p>Swarm 上手虽然相对比较容易，但是当出现问题的时候，作为运维容器平台的人，会发现问题比较难解决。</p><p>Swarm 内置的功能太多，都耦合在了一起，一旦出现错误，不容易 debug。如果当前的功能不能满足需求，很难定制化。很多功能都是耦合在 Manager 里面的，对 Manager 的操作和重启影响面太大。</p><p><strong>Mesos：数据运维工程师</strong></p><p><img src="https://pic4.zhimg.com/v2-d3e4775c5dcdf92eb7219d8d72998045_b.jpg" alt=""></p><p>从大数据平台运维的角度来讲，如何更快地调度大数据处理任务，在有限的时间和空间里面，更快地跑更多的任务，是一个非常重要的要素。</p><p>所以当我们评估大数据平台牛不牛的时候，往往以单位时间内跑的任务数目以及能够处理的数据量来衡量。</p><p>从数据运维的角度来讲，Mesos 是一个很好的调度器。既然能够跑任务，也就能够跑容器，Spark 和 Mesos 天然的集成，有了容器之后，可以用更加细粒度的任务执行方式。</p><p>在没有细粒度的任务调度之前，任务的执行过程是这样的。任务的执行需要 Master 的节点来管理整个任务的执行过程，需要 Worker 节点来执行一个个子任务。在整个总任务的一开始，就分配好 Master 和所有的 Work 所占用的资源，将环境配置好，等在那里执行子任务，没有子任务执行的时候，这个环境的资源都是预留在那里的，显然不是每个 Work 总是全部跑满的，存在很多的资源浪费。</p><p>在细粒度的模式下，在整个总任务开始的时候，只会为 Master 分配好资源，不给 Worker 分配任何的资源，当需要执行一个子任务的时候，Master 才临时向 Mesos 申请资源，环境没有准备好怎么办？好在有 Docker，启动一个 Docker，环境就都有了，在里面跑子任务。在没有任务的时候，所有节点上的资源都是可被其他任务使用的，大大提升了资源利用效率。</p><p>这就是 Mesos 最大的优势，在 Mesos 的论文中，最重要阐述的就是资源利用率的提升，而 Mesos 的双层调度算法是核心。</p><p>原来大数据运维工程师出身的，会比较容易选择 Mesos 作为容器管理平台。不过原来是跑短任务，加上 marathon 就能跑长任务。但是后来 Spark 将细粒度的模式 deprecated 掉了，因为效率还是比较差。</p><p><strong>Mesos 的问题</strong></p><p><img src="https://pic4.zhimg.com/v2-f3983b70c19ae7a2ed4089a737803d4c_b.jpg" alt=""></p><p>调度在大数据领域是核心中的核心，在容器平台中是重要的，但不是全部。所以容器还需要编排，需要各种外围组件，让容器跑起来运行长任务，并且相互访问。Marathon 只是万里长征的第一步。</p><p>所以早期用 Marathon + Mesos 的厂商，多是裸用 Marathon 和 Mesos 的，由于周边不全，因而要做各种的封装，各家不同。大家有兴趣可以到社区上去看裸用 Marathon 和 Mesos 的厂商，各有各的负载均衡方案，各有各的服务发现方案。</p><p>所以后来有了 DCOS，也就是在 Marathon 和 Mesos 之外，加了大量的周边组件，补充一个容器平台应有的功能，但是很可惜，很多厂商都自己定制过了，还是裸用 Marathon 和 Mesos 的比较多。</p><p>而且 Mesos 虽然调度牛，但是只解决一部分调度，另一部分靠用户自己写 framework 以及里面的调度，有时候还需要开发 Executor，这个开发起来还是很复杂的，学习成本也比较高。</p><p>虽说后来的 DCOS 功能也比较全了，但是感觉没有如 Kubernetes 一样使用统一的语言，而是采取大杂烩的方式。在 DCOS 的整个生态中，Marathon 是 Scala 写的，Mesos 是 C++ 写的，Admin Router 是 Nginx+lua，Mesos-DNS 是Go，Marathon-lb 是 Python，Minuteman 是 Erlang，这样太复杂了吧，林林总总，出现了 Bug 的话，比较难自己修复。</p><p><strong>Kubernetes</strong></p><p><img src="https://pic1.zhimg.com/v2-b1533bf9aeb79e620041d5309abd4bbe_b.jpg" alt=""></p><p>而 Kubernetes 不同，初看 Kubernetes 的人觉得他是个奇葩所在，容器还没创建出来，概念先来一大堆，文档先读一大把，编排文件也复杂，组件也多，让很多人望而却步。我就想创建一个容器，怎么这么多的前置条件。如果你将 Kubernetes 的概念放在界面上，让客户去创建容器，一定会被客户骂。</p><p>在开发人员角度，使用 Kubernetes 绝对不是像使用虚拟机一样，开发除了写代码，做构建，做测试，还需要知道自己的应用是跑在容器上的，而不是当甩手掌柜。开发人员需要知道，容器是和原来的部署方式不一样的存在，你需要区分有状态和无状态，容器挂了起来，就会按照镜像还原了。开发人员需要写 Dockerfile，需要关心环境的交付，需要了解太多原来不了解的东西。实话实说，一点都不方便。</p><p>在运维人员角度，使用 Kubernetes 也绝对不是像运维虚拟机一样，我交付出来了环境，应用之间互相怎么调用，我才不管，我就管网络通不通。在运维眼中他做了过多不该关心的事情，例如服务的发现，配置中心，熔断降级，这都应该是代码层面关心的事情，应该是 SpringCloud 和 Dubbo 关心的事情，为什么要到容器平台层来关心这个。</p><p><strong>Kubernetes + Docker，却是 Dev 和 Ops 融合的一个桥梁。</strong></p><p>Docker 是微服务的交付工具，微服务之后，服务太多了，单靠运维根本管不过来，而且很容易出错，这就需要研发开始关心环境交付这件事情。例如配置改了什么，创建了哪些目录，如何配置权限，只有开发最清楚，这些信息很难通过文档的方式又及时又准确地同步到运维部门来，就算是同步过来了，运维部门的维护量也非常的大。</p><p>所以，有了容器，最大的改变是环境交付的提前，是每个开发多花 5% 的时间，去换取运维 200% 的劳动，并且提高稳定性。</p><p>而另一方面，本来运维只管交付资源，给你个虚拟机，虚拟机里面的应用如何相互访问我不管，你们爱咋地咋地，有了 Kubernetes 以后，运维层要关注服务发现，配置中心，熔断降级。</p><p>两者融合在了一起。在微服务化的研发的角度来讲，Kubernetes 虽然复杂，但是设计的都是有道理的，符合微服务的思想。</p><p><strong>三、微服务化的十个设计要点</strong></p><p>微服务有哪些要点呢？第一张图是 SpringCloud 的整个生态。</p><p><img src="https://pic3.zhimg.com/v2-2940295fbe535853f2209c939bf72b6a_b.jpg" alt=""></p><p>第二张图是微服务的 12 要素以及在网易云的实践。</p><p><img src="https://pic1.zhimg.com/v2-11e7c3c6f94cb4b61262f21f0522389b_b.jpg" alt=""></p><p>第三张图是构建一个高并发的微服务，需要考虑的所有的点。（打个广告，这是一门课程，即将上线。）</p><p><img src="https://pic1.zhimg.com/v2-b7b51e63f96b905134a21a3bdf82452d_b.jpg" alt=""></p><p>接下来细说微服务的设计要点。</p><p><strong>设计要点一：API 网关。</strong></p><p><img src="https://pic4.zhimg.com/v2-2049df4a0aa21772ba52a52cc4fe7a0c_b.jpg" alt=""></p><p>在实施微服务的过程中，不免要面临服务的聚合与拆分，当后端服务的拆分相对比较频繁的时候，作为手机 App 来讲，往往需要一个统一的入口，将不同的请求路由到不同的服务，无论后面如何拆分与聚合，对于手机端来讲都是透明的。</p><p>有了 API 网关以后，简单的数据聚合可以在网关层完成，这样就不用在手机 App 端完成，从而手机 App 耗电量较小，用户体验较好。</p><p>有了统一的 API 网关，还可以进行统一的认证和鉴权，尽管服务之间的相互调用比较复杂，接口也会比较多，API 网关往往只暴露必须的对外接口，并且对接口进行统一的认证和鉴权，使得内部的服务相互访问的时候，不用再进行认证和鉴权，效率会比较高。</p><p>有了统一的 API 网关，可以在这一层设定一定的策略，进行 A/B 测试，蓝绿发布，预发环境导流等等。API 网关往往是无状态的，可以横向扩展，从而不会成为性能瓶颈。</p><p><strong>设计要点二：无状态化，区分有状态的和无状态的应用。</strong></p><p><img src="https://pic1.zhimg.com/v2-19eebf97bbc7f21af452a6663cfec38d_b.jpg" alt=""></p><p>影响应用迁移和横向扩展的重要因素就是应用的状态，无状态服务，是要把这个状态往外移，将 Session 数据，文件数据，结构化数据保存在后端统一的存储中，从而应用仅仅包含商务逻辑。</p><p>状态是不可避免的，例如 ZooKeeper, DB，Cache 等，把这些所有有状态的东西收敛在一个非常集中的集群里面。</p><p>整个业务就分两部分，一个是无状态的部分，一个是有状态的部分。</p><p>无状态的部分能实现两点，一是跨机房随意地部署，也即迁移性，一是弹性伸缩，很容易地进行扩容。</p><p>有状态的部分，如 DB，Cache，ZooKeeper 有自己的高可用机制，要利用到他们自己高可用的机制来实现这个状态的集群。</p><p>虽说无状态化，但是当前处理的数据，还是会在内存里面的，当前的进程挂掉数据，肯定也是有一部分丢失的，为了实现这一点，服务要有重试的机制，接口要有幂等的机制，通过服务发现机制，重新调用一次后端服务的另一个实例就可以了。</p><p><strong>设计要点三：数据库的横向扩展。</strong></p><p><img src="https://pic2.zhimg.com/v2-91d4ca417ea4bc57fc5f43f73c4c8457_b.jpg" alt=""></p><p>数据库是保存状态，是最重要的也是最容易出现瓶颈的。有了分布式数据库可以使数据库的性能可以随着节点增加线性地增加。</p><p>分布式数据库最最下面是 RDS，是主备的，通过 MySql 的内核开发能力，我们能够实现主备切换数据零丢失，所以数据落在这个 RDS 里面，是非常放心的，哪怕是挂了一个节点，切换完了以后，你的数据也是不会丢的。</p><p>再往上就是横向怎么承载大的吞吐量的问题，上面有一个负载均衡 NLB，用 LVS，HAProxy, Keepalived，下面接了一层 Query Server。Query Server 是可以根据监控数据进行横向扩展的，如果出现了故障，可以随时进行替换的修复，对于业务层是没有任何感知的。</p><p>另外一个就是双机房的部署，DDB 开发了一个数据运河 NDC 的组件，可以使得不同的 DDB 之间在不同的机房里面进行同步，这时候不但在一个数据中心里面是分布式的，在多个数据中心里面也会有一个类似双活的一个备份，高可用性有非常好的保证。</p><p><strong>设计要点四：缓存</strong></p><p><img src="https://pic3.zhimg.com/v2-0f8f843c8d833ad122475e0dd56e5eab_b.jpg" alt=""></p><p>在高并发场景下缓存是非常重要的。要有层次的缓存，使得数据尽量靠近用户。数据越靠近用户能承载的并发量也越大，响应时间越短。</p><p>在手机客户端 App 上就应该有一层缓存，不是所有的数据都每时每刻从后端拿，而是只拿重要的，关键的，时常变化的数据。</p><p>尤其对于静态数据，可以过一段时间去取一次，而且也没必要到数据中心去取，可以通过 CDN，将数据缓存在距离客户端最近的节点上，进行就近下载。</p><p>有时候 CDN 里面没有，还是要回到数据中心去下载，称为回源，在数据中心的最外层，我们称为接入层，可以设置一层缓存，将大部分的请求拦截，从而不会对后台的数据库造成压力。</p><p>如果是动态数据，还是需要访问应用，通过应用中的商务逻辑生成，或者去数据库读取，为了减轻数据库的压力，应用可以使用本地的缓存，也可以使用分布式缓存，如 Memcached 或者 Redis，使得大部分请求读取缓存即可，不必访问数据库。</p><p>当然动态数据还可以做一定的静态化，也即降级成静态数据，从而减少后端的压力。</p><p><strong>设计要点五：服务拆分和服务发现</strong></p><p><img src="https://pic4.zhimg.com/v2-8c582f0e98d0ac592918beb012757559_b.jpg" alt=""></p><p>当系统扛不住，应用变化快的时候，往往要考虑将比较大的服务拆分为一系列小的服务。</p><p>这样第一个好处就是<strong>开发比较独立</strong>，当非常多的人在维护同一个代码仓库的时候，往往对代码的修改就会相互影响，常常会出现我没改什么测试就不通过了，而且代码提交的时候，经常会出现冲突，需要进行代码合并，大大降低了开发的效率。</p><p>另一个好处就是<strong>上线独立</strong>，物流模块对接了一家新的快递公司，需要连同下单一起上线，这是非常不合理的行为，我没改还要我重启，我没改还让我发布，我没改还要我开会，都是应该拆分的时机。</p><p>另外再就是<strong>高并发时段的扩容</strong>，往往只有最关键的下单和支付流程是核心，只要将关键的交易链路进行扩容即可，如果这时候附带很多其他的服务，扩容即是不经济的，也是很有风险的。</p><p>再就是<strong>容灾和降级</strong>，在大促的时候，可能需要牺牲一部分的边角功能，但是如果所有的代码耦合在一起，很难将边角的部分功能进行降级。</p><p>当然拆分完毕以后，应用之间的关系就更加复杂了，因而需要服务发现的机制，来管理应用相互的关系，实现自动的修复，自动的关联，自动的负载均衡，自动的容错切换。</p><p><strong>设计要点六：服务编排与弹性伸缩</strong></p><p><img src="https://pic3.zhimg.com/v2-716f5d35a790b5eb0cf3c378cafb9e70_b.jpg" alt=""></p><p>当服务拆分了，进程就会非常的多，因而需要服务编排来管理服务之间的依赖关系，以及将服务的部署代码化，也就是我们常说的基础设施即代码。这样对于服务的发布，更新，回滚，扩容，缩容，都可以通过修改编排文件来实现，从而增加了可追溯性，易管理性，和自动化的能力。</p><p>既然编排文件也可以用代码仓库进行管理，就可以实现一百个服务中，更新其中五个服务，只要修改编排文件中的五个服务的配置就可以，当编排文件提交的时候，代码仓库自动触发自动部署升级脚本，从而更新线上的环境，当发现新的环境有问题时，当然希望将这五个服务原子性地回滚，如果没有编排文件，需要人工记录这次升级了哪五个服务。有了编排文件，只要在代码仓库里面 revert，就回滚到上一个版本了。所有的操作在代码仓库里都是可以看到的。</p><p><strong>设计要点七：统一配置中心</strong></p><p><img src="https://pic1.zhimg.com/v2-ee65797a5f4293d78775f250f37d2cb7_b.jpg" alt=""></p><p>服务拆分以后，服务的数量非常多，如果所有的配置都以配置文件的方式放在应用本地的话，非常难以管理，可以想象当有几百上千个进程中有一个配置出现了问题，是很难将它找出来的，因而需要有统一的配置中心，来管理所有的配置，进行统一的配置下发。</p><p>在微服务中，配置往往分为几类，一类是几乎不变的配置，这种配置可以直接打在容器镜像里面，第二类是启动时就会确定的配置，这种配置往往通过环境变量，在容器启动的时候传进去，第三类就是统一的配置，需要通过配置中心进行下发，例如在大促的情况下，有些功能需要降级，哪些功能可以降级，哪些功能不能降级，都可以在配置文件中统一配置。</p><p><strong>设计要点八：统一的日志中心</strong></p><p><img src="https://pic1.zhimg.com/v2-88ef3b80e824cc64df8f2c40188b1a31_b.jpg" alt=""></p><p>同样是进程数目非常多的时候，很难对成千上百个容器，一个一个登录进去查看日志，所以需要统一的日志中心来收集日志，为了使收集到的日志容易分析，对于日志的规范，需要有一定的要求，当所有的服务都遵守统一的日志规范的时候，在日志中心就可以对一个交易流程进行统一的追溯。例如在最后的日志搜索引擎中，搜索交易号，就能够看到在哪个过程出现了错误或者异常。</p><p><strong>设计要点九：熔断，限流，降级</strong></p><p><img src="https://pic1.zhimg.com/v2-74fecf0238c7d15d8d1597bc263e425e_b.jpg" alt=""></p><p>服务要有熔断，限流，降级的能力，当一个服务调用另一个服务，出现超时的时候，应及时返回，而非阻塞在那个地方，从而影响其他用户的交易，可以返回默认的托底数据。</p><p>当一个服务发现被调用的服务，因为过于繁忙，线程池满，连接池满，或者总是出错，则应该及时熔断，防止因为下一个服务的错误或繁忙，导致本服务的不正常，从而逐渐往前传导，导致整个应用的雪崩。</p><p>当发现整个系统的确负载过高的时候，可以选择降级某些功能或某些调用，保证最重要的交易流程的通过，以及最重要的资源全部用于保证最核心的流程。</p><p>还有一种手段就是限流，当既设置了熔断策略，又设置了降级策略，通过全链路的压力测试，应该能够知道整个系统的支撑能力，因而就需要制定限流策略，保证系统在测试过的支撑能力范围内进行服务，超出支撑能力范围的，可拒绝服务。当你下单的时候，系统弹出对话框说 “系统忙，请重试”，并不代表系统挂了，而是说明系统是正常工作的，只不过限流策略起到了作用。</p><p><strong>设计要点十：全方位的监控</strong></p><p><img src="https://pic1.zhimg.com/v2-b367fd48938230d8fb2955b84cd93926_b.jpg" alt=""></p><p>当系统非常复杂的时候，要有统一的监控，主要有两个方面，一个是是否健康，一个是性能瓶颈在哪里。当系统出现异常的时候，监控系统可以配合告警系统，及时地发现，通知，干预，从而保障系统的顺利运行。</p><p>当压力测试的时候，往往会遭遇瓶颈，也需要有全方位的监控来找出瓶颈点，同时能够保留现场，从而可以追溯和分析，进行全方位的优化。</p><p><strong>四、Kubernetes 本身就是微服务架构</strong></p><p>基于上面这十个设计要点，我们再回来看 Kubernetes，会发现越看越顺眼。</p><p>首先 Kubernetes 本身就是微服务的架构，虽然看起来复杂，但是容易定制化，容易横向扩展。</p><p><img src="https://pic3.zhimg.com/v2-41add76a0b1cd02074ff99d26402892b_b.jpg" alt=""></p><p>如图黑色的部分是 Kubernetes 原生的部分，而蓝色的部分是网易云为了支撑大规模高并发应用而做的定制化部分。</p><p><img src="https://pic1.zhimg.com/v2-22b05baed189fb9bdaba5e4039e80fba_b.jpg" alt=""></p><p>Kubernetes 的 API Server 更像网关，提供统一的鉴权和访问接口。</p><p>众所周知，Kubernetes 的租户管理相对比较弱，尤其是对于公有云场景，复杂的租户关系的管理，我们只要定制化 API Server，对接 Keystone，就可以管理复杂的租户关系，而不用管其他的组件。</p><p><img src="https://pic1.zhimg.com/v2-8beb43c3363a53274e9c0994ce8eb1e9_b.jpg" alt=""></p><p>在 Kubernetes 中几乎所有的组件都是无状态化的，状态都保存在统一的 etcd 里面，这使得扩展性非常好，组件之间异步完成自己的任务，将结果放在 etcd 里面，互相不耦合。</p><p>例如图中 pod 的创建过程，客户端的创建仅仅是在 etcd 中生成一个记录，而其他的组件监听到这个事件后，也相应异步的做自己的事情，并将处理的结果同样放在 etcd 中，同样并不是哪一个组件远程调用 kubelet，命令它进行容器的创建，而是发现 etcd 中，pod 被绑定到了自己这里，方才拉起。</p><p>为了在公有云中实现租户的隔离性，我们的策略是不同的租户，不共享节点，这就需要 Kubernetes 对于 IaaS 层有所感知，因而需要实现自己的 Controller，Kubernetes 的设计使得我们可以独立创建自己的 Controller，而不是直接改代码。</p><p><img src="https://pic4.zhimg.com/v2-91aec2ae7048c9be4c5d14c2846dddd5_b.jpg" alt=""></p><p>API-Server 作为接入层，是有自己的缓存机制的，防止所有的请求压力直接到后端数据库上。但是当仍然无法承载高并发请求时，瓶颈依然在后端的 etcd 存储上，这和电商应用一摸一样。当然能够想到的方式也是对 etcd 进行分库分表，不同的租户保存在不同的 etcd 集群中。</p><p>有了 API Server 做 API 网关，后端的服务进行定制化，对于 client 和 kubelet 是透明的。</p><p><img src="https://pic1.zhimg.com/v2-25f7e24db4445ba739d9a30a49479beb_b.jpg" alt=""></p><p>如图是定制化的容器创建流程，由于大促和非大促期间，节点的数目相差比较大，因而不能采用事先全部创建好节点的方式，这样会造成资源的浪费，因而中间添加了网易云自己的模块 Controller 和 IaaS 的管理层，使得当创建容器资源不足的时候，动态调用 IaaS 的接口，动态的创建资源。这一切对于客户端和 kubelet 无感知。</p><p><img src="https://pic4.zhimg.com/v2-40320ad412f120448bdf8b5f160c62eb_b.jpg" alt=""></p><p>为了解决超过 3 万个节点的规模问题，网易云需要对各个模块进行优化，由于每个子模块仅仅完成自己的功能，Scheduler 只管调度，Proxy 只管转发，而非耦合在一起，因而每个组件都可以进行独立的优化，这符合微服务中的独立功能，独立优化，互不影响。而且 Kubernetes 的所有组件都是 Go 开发的，更加容易一些。所以 Kubernetes 上手慢，但是一旦需要定制化，会发现更加容易。</p><p><strong>五、Kubernetes 更加适合微服务和 DevOps 的设计</strong></p><p>好了，说了 K8S 本身，接下来说说 K8S 的理念设计，为什么这么适合微服务。</p><p><img src="https://pic2.zhimg.com/v2-2f2d7777c23693b5a8ffaa60bc024afb_b.jpg" alt=""></p><p>前面微服务设计的十大模式，其中一个就是区分无状态和有状态，在 K8S 中，无状态对应 deployment，有状态对应 StatefulSet。</p><p>deployment 主要通过副本数，解决横向扩展的问题。</p><p>而 StatefulSet 通过一致的网络 ID，一致的存储，顺序的升级，扩展，回滚等机制，保证有状态应用，很好地利用自己的高可用机制。因为大多数集群的高可用机制，都是可以容忍一个节点暂时挂掉的，但是不能容忍大多数节点同时挂掉。而且高可用机制虽然可以保证一个节点挂掉后回来，有一定的修复机制，但是需要知道刚才挂掉的到底是哪个节点，StatefulSet 的机制可以让容器里面的脚本有足够的信息，处理这些情况，实现哪怕是有状态，也能尽快修复。</p><p><img src="https://pic4.zhimg.com/v2-7f9799aeeb0cb83b3f97d7833bd670c0_b.jpg" alt=""></p><p>在微服务中，比较推荐使用云平台的 PaaS，例如数据库，消息总线，缓存等。但是配置也是非常复杂的，因为不同的环境需要连接不同的 PaaS 服务。</p><p>K8S 里面的 headless service 是可以很好地解决这个问题的，只要给外部服务创建一个 headless service，指向相应的 PaaS 服务，并且将服务名配置到应用中。由于生产和测试环境分成 Namespace，虽然配置了相同的服务名，但是不会错误访问，简化了配置。</p><p><img src="https://pic3.zhimg.com/v2-859c7a5046a914a1b60347565c63a08f_b.jpg" alt=""></p><p>微服务少不了服务发现，除了应用层可以使用 SpringCloud 或者 Dubbo 进行服务发现，在容器平台层当然是用 Service了，可以实现负载均衡，自修复，自动关联。</p><p><img src="https://pic2.zhimg.com/v2-e06cf0afb975aebac6ae9303381bc517_b.jpg" alt=""></p><p>服务编排，本来 K8S 就是编排的标准，可以将 yml 文件放到代码仓库中进行管理，而通过 deployment 的副本数，可以实现弹性伸缩。</p><p><img src="https://pic2.zhimg.com/v2-b6ab5f1572abe7c857cae91503045f53_b.jpg" alt=""></p><p>对于配置中心，K8S 提供了 configMap，可以在容器启动的时候，将配置注入到环境变量或者 Volume 里面。但是唯一的缺点是，注入到环境变量中的配置不能动态改变了，好在 Volume 里面的可以，只要容器中的进程有 reload 机制，就可以实现配置的动态下发了。</p><p><img src="https://pic3.zhimg.com/v2-80bb4f5a52e3efea59519071e0276dcc_b.jpg" alt=""></p><p>统一日志和监控往往需要在 Node 上部署 Agent，来对日志和指标进行收集，当然每个 Node 上都有，daemonset 的设计，使得更容易实现。</p><p><img src="https://pic3.zhimg.com/v2-1d0829c125a8274f741050563dbdfb05_b.jpg" alt=""></p><p>当然目前最最火的 Service Mesh，可以实现更加精细化的服务治理，进行熔断，路由，降级等策略。Service Mesh 的实现往往通过 sidecar 的方式，拦截服务的流量，进行治理。这也得力于 Pod 的理念，一个 Pod 可以有多个容器，如果当初的设计没有 Pod，直接启动的就是容器，会非常的不方便。</p><p>所以 K8S 的各种设计，看起来非常冗余和复杂，入门门槛比较高，但是一旦想实现真正的微服务，K8S 可以给你各种可能的组合方式。实践过微服务的人，往往会对这一点深有体会。</p><p><strong>六、Kubernetes 常见的使用方式</strong></p><p>关于微服务化的不同阶段，Kubernetes 的使用方式，详见这篇文章：<a href="https://zhuanlan.zhihu.com/p/34852026" target="_blank" rel="noopener">微服务化的不同阶段 Kubernetes 的不同玩法</a></p><p>原文地址：<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/L5imIU7pW6RN0VB2XgQ_kw" target="_blank" rel="noopener">为什么 kubernetes 天然适合微服务</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>相似图片搜索、算法、识别的原理解析（转载）</title>
      <link href="/2018/05/15/%E7%9B%B8%E4%BC%BC%E5%9B%BE%E7%89%87%E6%90%9C%E7%B4%A2%E3%80%81%E7%AE%97%E6%B3%95%E3%80%81%E8%AF%86%E5%88%AB%E7%9A%84%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/05/15/%E7%9B%B8%E4%BC%BC%E5%9B%BE%E7%89%87%E6%90%9C%E7%B4%A2%E3%80%81%E7%AE%97%E6%B3%95%E3%80%81%E8%AF%86%E5%88%AB%E7%9A%84%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>2011年6月，Google把”相似图片搜索”正式放上了首页。你可以用一张图片，搜索互联网上所有与它相似的图片。点击搜索框中照相机的图标。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RogXqA/B2We7.png" alt="google搜图"><br><a id="more"></a><br>一个对话框会出现。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RohS9u/xs1qM.png" alt="google搜图"></p><p>你输入网片的网址，或者直接上传图片，Google就会找出与其相似的图片。下面这张图片是美国女演员Alyson Hannigan。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8Ro715X/8FPGP.jpg" alt="Alyson Hannigan"></p><p>上传后，Google返回如下结果：</p><p><img src="http://pic.yupoo.com/36dsj_v/D8Rohqsg/9GQ7c.jpg" alt="Alyson Hannigan"></p><p>类似的”相似图片搜索引擎”还有不少，TinEye甚至可以找出照片的拍摄背景。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RohLmG/il8eq.jpg" alt="TinEye"></p><p>==========================================================</p><p>这种技术的原理是什么？计算机怎么知道两张图片相似呢？</p><p>根据Neal Krawetz博士的解释，原理非常简单易懂。我们可以用一个快速算法，就达到基本的效果。</p><p>这里的关键技术叫做”感知哈希算法”（Perceptual hash algorithm），它的作用是对每张图片生成一个”指纹”（fingerprint）字符串，然后比较不同图片的指纹。结果越接近，就说明图片越相似。</p><p>下面是一个最简单的实现：</p><p>第一步，缩小尺寸。</p><p>将图片缩小到8×8的尺寸，总共64个像素。这一步的作用是去除图片的细节，只保留结构、明暗等基本信息，摒弃不同尺寸、比例带来的图片差异。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RoiEeL/8gfg6.png" alt="搜图"></p><p>第二步，简化色彩。</p><p>将缩小后的图片，转为64级灰度。也就是说，所有像素点总共只有64种颜色。</p><p>第三步，计算平均值。</p><p>计算所有64个像素的灰度平均值。</p><p>第四步，比较像素的灰度。</p><p>将每个像素的灰度，与平均值进行比较。大于或等于平均值，记为1；小于平均值，记为0。</p><p>第五步，计算哈希值。</p><p>将上一步的比较结果，组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。组合的次序并不重要，只要保证所有图片都采用同样次序就行了。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RoiB6F/1179sk.png" alt="哈希值"></p><p>得到指纹以后，就可以对比不同的图片，看看64位中有多少位是不一样的。在理论上，这等同于计算”汉明距离”（Hamming distance）。如果不相同的数据位不超过5，就说明两张图片很相似；如果大于10，就说明这是两张不同的图片。</p><p>具体的代码实现，可以参见Wote用python语言写的imgHash.py。代码很短，只有53行。使用的时候，第一个参数是基准图片，第二个参数是用来比较的其他图片所在的目录，返回结果是两张图片之间不相同的数据位数量（汉明距离）。</p><p>这种算法的优点是简单快速，不受图片大小缩放的影响，缺点是图片的内容不能变更。如果在图片上加几个文字，它就认不出来了。所以，它的最佳用途是根据缩略图，找出原图。</p><p>实际应用中，往往采用更强大的pHash算法和SIFT算法，它们能够识别图片的变形。只要变形程度不超过25%，它们就能匹配原图。这些算法虽然更复杂，但是原理与上面的简便算法是一样的，就是先将图片转化成Hash字符串，然后再进行比较。</p><hr><h2 id="续："><a href="#续：" class="headerlink" title="续："></a><font color="red">续：</font></h2><p>昨天，我在<a href="http://www.isnowfy.com/similar-image-search/" target="_blank" rel="noopener">isnowfy</a>的网站看到，还有其他两种方法也很简单，这里做一些笔记。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RsxN08/omQCZ.jpg" alt="google搜图"></p><p>一、颜色分布法</p><p>每张图片都可以生成<a href="http://en.wikipedia.org/wiki/Color_histogram" target="_blank" rel="noopener">颜色分布的直方图</a>（color histogram）。如果两张图片的直方图很接近，就可以认为它们很相似。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8RsxGy9/wPZ9L.jpg" alt="颜色直方图"></p><p>任何一种颜色都是由红绿蓝三原色（RGB）构成的，所以上图共有4张直方图（三原色直方图 + 最后合成的直方图）。</p><p>如果每种原色都可以取256个值，那么整个颜色空间共有1600万种颜色（256的三次方）。针对这1600万种颜色比较直方图，计算量实在太大了，因此需要采用简化方法。可以将0～255分成四个区：0～63为第0区，64～127为第1区，128～191为第2区，192～255为第3区。这意味着红绿蓝分别有4个区，总共可以构成64种组合（4的3次方）。</p><p>任何一种颜色必然属于这64种组合中的一种，这样就可以统计每一种组合包含的像素数量。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8Rsn9mm/eOMPz.png" alt="颜色图分解"></p><p>上图是某张图片的颜色分布表，将表中最后一栏提取出来，组成一个64维向量(7414, 230, 0, 0, 8, …, 109, 0, 0, 3415, 53929)。这个向量就是这张图片的特征值或者叫”指纹”。</p><p>于是，寻找相似图片就变成了找出与其最相似的向量。这可以用<a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient" target="_blank" rel="noopener">皮尔逊相关系数</a>或者<a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="noopener">余弦相似度</a>算出。</p><p>二、内容特征法</p><p>除了颜色构成，还可以从比较图片内容的相似性入手。</p><p>首先，将原图转成一张较小的灰度图片，假定为50×50像素。然后，确定一个阈值，将灰度图片转成黑白图片。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8Rwlp9W/Y9fLL.png" alt="灰白照片"></p><p>如果两张图片很相似，它们的黑白轮廓应该是相近的。于是，问题就变成了，第一步如何确定一个合理的阈值，正确呈现照片中的轮廓？</p><p>显然，前景色与背景色反差越大，轮廓就越明显。这意味着，如果我们找到一个值，可以使得前景色和背景色各自的“类内差异最小“（minimizing the intra-class variance），或者”类间差异最大“（maximizing the inter-class variance），那么这个值就是理想的阈值。</p><p>1979年，日本学者大津展之证明了，”类内差异最小”与”类间差异最大”是同一件事，即对应同一个阈值。他提出一种简单的算法，可以求出这个阈值，这被称为”大津法“（Otsu’s method）。下面就是他的计算方法。</p><p>假定一张图片共有n个像素，其中灰度值小于阈值的像素为 n1 个，大于等于阈值的像素为 n2 个（ n1 + n2 = n ）。w1 和 w2 表示这两种像素各自的比重。</p><blockquote><p>　　w1 = n1 / n<br>w2 = n2 / n</p></blockquote><p>再假定，所有灰度值小于阈值的像素的平均值和方差分别为 μ1 和 σ1，所有灰度值大于等于阈值的像素的平均值和方差分别为 μ2 和 σ2。于是，可以得到</p><blockquote><p>类内差异 = w1(σ1的平方) + w2(σ2的平方)</p><p>类间差异 = w1w2(μ1-μ2)^2</p></blockquote><p>可以证明，这两个式子是等价的：得到”类内差异”的最小值，等同于得到”类间差异”的最大值。不过，从计算难度看，后者的计算要容易一些。</p><p>下一步用”穷举法”，将阈值从灰度的最低值到最高值，依次取一遍，分别代入上面的算式。使得”类内差异最小”或”类间差异最大”的那个值，就是最终的阈值。具体的实例和Java算法，请看<a href="http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html" target="_blank" rel="noopener">这里</a>。</p><p><img src="http://pic.yupoo.com/36dsj_v/D8Rsy0Ip/vTd5H.png" alt="图片算法"></p><p>有了50×50像素的黑白缩略图，就等于有了一个50×50的0-1矩阵。矩阵的每个值对应原图的一个像素，0表示黑色，1表示白色。这个矩阵就是一张图片的特征矩阵。</p><p>两个特征矩阵的不同之处越少，就代表两张图片越相似。这可以用”异或运算”实现（即两个值之中只有一个为1，则运算结果为1，否则运算结果为0）。对不同图片的特征矩阵进行”异或运算”，结果中的1越少，就是越相似的图片。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>物联网的概念及其发展前景（转载）</title>
      <link href="/2018/05/14/%E7%89%A9%E8%81%94%E7%BD%91%E7%9A%84%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%85%B6%E5%8F%91%E5%B1%95%E5%89%8D%E6%99%AF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/05/14/%E7%89%A9%E8%81%94%E7%BD%91%E7%9A%84%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%85%B6%E5%8F%91%E5%B1%95%E5%89%8D%E6%99%AF%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>物联网应用的领域太广了，农业、交通、医疗、物流、工业、安防等等等等，如下图：<br><img src="/2018/05/14/物联网的概念及其发展前景（转载）/hd001.jpg"><br><a id="more"></a><br><strong>2.1 物联网概念</strong></p><p><strong>一、世界的物联网</strong></p><p>物联网（Internet of Things，简称IOT）概念始终处于一个动态的、不断拓展的过程。</p><p>最初的物联网概念，国内外普遍公认的是MIT Auto-ID中心Ashton教授1999年在研究RFID时最早提出来的。当时叫传感网。其定义是：通过射频识别（RFID）、红外感应器、全球定位系统、激光扫描器等信息传感设备，按约定的协议，把任何物品与互联网相连接，进行信息交换和通信，以实现智能化识别、定位、跟踪、监控和管理的一种网络概念。</p><p>在2005年国际电信联盟（ITU）发布的同名报告中，物联网的定义和范围已经发生了变化，覆盖范围有了较大的拓展，不再只是指基于RFID技术的物联网，提出任何时刻、任何地点、任何物体之间的互联，无所不在的网络和无所不在计算的发展愿景，除RFID技术外、传感器技术、纳米技术、智能终端等技术将得到更加广泛的应用。</p><p>在国外，物联网概念往往以可视化的形式来深入人心，如图：<br><img src="../物联网的概念及其发展前景（转载）/hd002.jpg" alt=""><br>物联网体系可以形象地比喻成一棵树木，其由三部分构成：</p><p>最底层的是树根，即技术部分。由传感器技术&amp;设备、嵌入式处理器技术&amp;设备、连接技术&amp;设备构成，是整个树木赖以生存和发展的根基。技术&amp;设备的发展程度决定了树干和树冠的茂盛程度。</p><p>传感器技术&amp;设备：压力传感器、温度传感器、湿度传感器等；</p><p>嵌入式处理技术&amp;设备：微控制器MCU、微处理器MPU、网络处理器等；</p><p>连接技术&amp;设备：NFC、Zigbee、GPS、WIFI等。</p><p>树根上面是树干，即软件部分。这是树木的躯干和中枢神经。包括设备驱动软件、服务器端软件和应用客户端软件。</p><p>树干上面是树冠。即应用部分。这是整个物联网体系的成果，可分为工业性应用和民用型应用两部分。</p><p><strong>二、中国的物联网。</strong></p><p>在我国，物联网的覆盖范围与时俱进，已经超越了1999年Ashton教授和2005年ITU报告所指的范围，物联网已被贴上“中国式”标签，其含义为：</p><p>物联网是将无处不在（Ubiquitous）的末端设备（Devices）和设施（Facilities），包括具备“内在智能”的传感器、移动终端、工业系统、楼控系统、家庭智能设施、视频监控系统等、和“外在使能”(Enabled)的，如贴上RFID的各种资产（Assets）、携带无线终端的个人与车辆等等“智能化物件或动物”或“智能尘埃”（Mote），通过各种无线和/或有线的长距离和/或短距离通讯网络实现互联互通（M2M)、应用大集成（Grand Integration)、以及基于云计算的SaaS营运等模式，在内网（Intranet）、专网（Extranet）、和/或互联网（Internet）环境下，采用适当的信息安全保障机制，提供安全可控乃至个性化的实时在线监测、定位追溯、报警联动、调度指挥、预案管理、远程控制、安全防范、远程维保、在线升级、统计报表、决策支持、领导桌面（集中展示的Cockpit Dashboard)等管理和服务功能，实现对“万物”的“高效、节能、安全、环保”的“管、控、营”一体化。</p><p>简单概括可为：把所有物品通过信息传感设备与互联网连接起来，进行信息交换，即物物相息，以实现智能化识别和管理。</p><p><strong>2.2 物联网体系</strong></p><p><strong>一、物联网基本要素</strong></p><p>物联网发展的关键要素包括由感知、网络和应用层组成的网络架构，物联网技术和标准，包括服务业和制造业在内的物联网相关产业，资源体系，隐私和安全以及促进和规范物联网发展的法律、政策和国际治理体系。<br><img src="../物联网的概念及其发展前景（转载）/hd003.jpg" alt=""><br><strong>二、物联网网络架构</strong></p><p>物联网网络架构由感知层、网络层和应用层组成。</p><p>感知层实现对物理世界的智能感知识别、信息采集处理和自动控制，并通过通信模块将物理实体连接到网络层和应用层。</p><p>网络层主要实现信息的传递、路由和控制，包括延伸网、接入网和核心网，网络层可依托公众电信网和互联网，也可以依托行业专用通信网络。</p><p>应用层包括应用基础设施/中间件和各种物联网应用。应用基础设施/中间件为物联网应用提供信息处理、计算等通用基础服务设施、能力及资源调用接口，以此为基础实现物联网在众多领域的各种应用。<br><img src="../物联网的概念及其发展前景（转载）/hd004.jpg" alt=""><br><strong>三、物联网技术体系</strong></p><p>物联网涉及感知、控制、网络通信、微电子、计算机、软件、嵌入式系统、微机电等技术领域，因此物联网涵盖的关键技术也非常多，为了系统分析物联网技术体系，特将物联网技术体系划分为感知关键技术、网络通信关键技术、应用关键技术、共性技术和支撑技术。<br><img src="../物联网的概念及其发展前景（转载）/hd005.jpg" alt=""><br><strong>1.</strong> <strong>感知、网络通信和应用关键技术。</strong></p><p>传感和识别技术是物联网感知物理世界获取信息和实现物体控制的首要环节。传感器将物理世界中的物理量、化学量、生物量转化成可供处理的数字信号。识别技术实现对物联网中物体标识和位置信息的获取。</p><p><strong>2.</strong> <strong>网络通信关键技术。</strong></p><p>网络通信技术主要实现物联网数据信息和控制信息的双向传递、路由和控制，重点包括低速近距离无线通信技术、低功耗路由、自组织通信、无线接入M2M 通信增强、IP 承载技术、网络传送技术、异构网络融合接入技术以及认知无线电技术。</p><p><strong>3.</strong> <strong>应用关键技术。</strong></p><p>海量信息智能处理综合运用高性能计算、人工智能、数据库和模糊计算等技术，对收集的感知数据进行通用处理，重点涉及数据存储、并行计算、数据挖掘、平台服务、信息呈现等。面向服务的体系架构（Service-oriented Architecture ，SOA）是一种松耦合的软件组件技术，它将应用程序的不同功能模块化，并通过标准化的接口和调用方式联系起来，实现快速可重用的系统开发和部署。SOA 可提高物联网架构的扩展性，提升应用开发效率，充分整合和复用信息资源。</p><p><strong>4.</strong> <strong>支撑技术。</strong></p><p>物联网支撑技术包括嵌入式系统、微机电系统（Micro ElectroMechanical Systems，MEMS）、软件和算法、电源和储能、新材料技术等。</p><p><strong>5.</strong> <strong>共性技术。</strong></p><p>物联网共性技术涉及网络的不同层面，主要包括架构技术、标识和解析、安全和隐私、网络管理技术等。</p><p><strong>四、物联网标准化体系</strong></p><p>物联网标准是国际物联网技术竞争的制高点。由于物联网涉及不同专业技术领域、不同行业应用部门，物联网的标准既要涵盖面向不同应用的基础公共技术，也要涵盖满足行业特定需求的技术标准；既包括国家标准，也包括行业标准。</p><p>物联网标准体系相对庞杂，若从物联网总体、感知层、网络层、应用层、共性关键技术标准体系等五个层次可初步构建标准体系。</p><p><strong>物联网总体性标准</strong>：包括物联网导则、物联网总体架构、物联网业务需求等。</p><p><strong>感知层标准体系</strong>：主要涉及传感器等各类信息获取设备的电气和数据接口、感知数据模型、描述语言和数据结构的通用技术标准、RFID 标签和读写器接口和协议标准、特定行业和应用相关的感知层技术标准等。</p><p><strong>网络层标准体系</strong>：主要涉及物联网网关、短距离无线通信、自组织网络、简化IPv6 协议、低功耗路由、增强的机器对机器（Machineto Machine，M2M）无线接入和核心网标准、M2M 模组与平台、网络资源虚拟化标准、异构融合的网络标准等。</p><p><strong>应用层标准体系</strong>：包括应用层架构、信息智能处理技术、以及行业、公众应用类标准。应用层架构重点是面向对象的服务架构，包括SOA 体系架构、面向上层业务应用的流程管理、业务流程之间的通信协议、元数据标准以及SOA 安全架构标准。信息智能处理类技术标准包括云计算、数据存储、数据挖掘、海量智能信息处理和呈现等。云计算技术标准重点包括开放云计算接口、云计算开放式虚拟化架构（资源管理与控制）、云计算互操作、云计算安全架构等。</p><p><strong>共性关键技术标准体系</strong>：包括标识和解析、服务质量（Quality ofService，QoS）、安全、网络管理技术标准。标识和解析标准体系包括编码、解析、认证、加密、隐私保护、管理，以及多标识互通标准。安全标准重点包括安全体系架构、安全协议、支持多种网络融合的认证和加密技术、用户和应用隐私保护、虚拟化和匿名化、面向服务的自适应安全技术标准等。</p><p><strong>2.4 物联网产业</strong></p><p><strong>一、产业体系</strong></p><p>物联网相关产业是指实现物联网功能所必需的相关产业集合，从产业结构上主要包括服务业和制造业两大范畴。<br><img src="../物联网的概念及其发展前景（转载）/hd006.jpg" alt=""><br>物联网制造业以感知端设备制造业为主。感知端设备的高智能化与嵌入式系统息息相关，设备的高精密化离不开集成电路、嵌入式系统、微纳器件、新材料、微能源等基础产业支撑。部分计算机设备、网络通信设备也是物联网制造业的组成部分。</p><p>物联网服务业主要包括物联网网络服务业、物联网应用基础设施服务业、物联网软件开发与应用集成服务业以及物联网应用服务业四大类，物联网应用基础设施服务主要包括云计算服务、存储服务等，物联网软件开发与集成服务又可细分为基础软件服务、中间件服务、应用软件服务、智能信息处理服务以及系统集成服务，物联网应用服务又可分为行业服务、公共服务和支撑性服务。</p><p>物联网产业绝大部分属于信息产业，但也涉及其它产业，如智能电表等。物联网产业的发展不是对已有信息产业的重新统计划分，而是通过应用带动形成新市场、新业态，整体上可分三种情形：</p><p>一是因物联网应用对已有产业的提升，主要体现在产品的升级换代。如传感器、RFID、仪器仪表发展已数十年，由于物联网应用使之向智能化网络化升级，从而实现产品功能、应用范围和市场规模的巨大扩展，传感器产业与RFID 产业成为物联网感知终端制造业的核心； 二是因物联网应用对已有产业的横向市场拓展，主要体现在领域延伸和量的扩张。如服务器、软件、嵌入式系统、云计算等由于物联网应用扩展了新的市场需求，形成了新的增长点。仪器仪表产业、嵌入式系统产业、云计算产业、软件与集成服务业，不独与物联网相关，也是其它产业的重要组成部分，物联网成为这些产业发展新的风向标；</p><p>三是由于物联网应用创造和衍生出的独特市场和服务，如传感器网络设备、M2M 通信设备及服务、物联网应用服务等均是物联网发展后才形成的新兴业态，为物联网所独有。物联网产业当前浮现的只是其初级形态，市场尚未大规模启动。</p><p><strong>二、产业链条</strong></p><p>梳理产业体系能够对物联网产业的内容有全局性了解，但想明确自身企业在产业链中的位置以及做相应战略规划，就必须知道整个物联网上下游产业链。</p><p>以我国为例，在物联网概念热炒之前，物联网产业链已经存在，主要以集成商为主角，但集成商又分布在各个行业、地域中。所以目前的物联网产业链基本可以理解为战国时代，同样的模式在不同的地域、行业被不同的集成商控制。<br><img src="../物联网的概念及其发展前景（转载）/hd007.jpg" alt=""><br>产业链上各部分的产业价值占比大约为：</p><p>（1）传感器/芯片厂商+通信模块提供商→15%；</p><p>（2）电信运营商提供的管道→15%；</p><p>（3）中间件及应用供应商+系统集成商+服务提供商→70%；</p><p>由此可见，在整个物联网产业价值链中，上游硬件厂商所占价值较小，绝大部分由中下游集成商/服务提供商分享，而这类占产业价值大头的公司通常都集多种角色为一体，以系统集成商的角色出现。电信运营商竭力在向两端延伸价值，但产业链的演变不是以运营商的意志为转移的，运营商可以在其中努力扩大产业链的自身价值，通过构建M2M平台和模块/终端标准化来逐步实现，但在实际的商业模式中，要让广大的集成商使用运营商标准的模块和平台，必须价值让利，通过模块的补贴、定制、集采逐步让集成商接纳运营商的标准，进而将行业应用数据流逐步迁移到运营商的平台上。</p><p>附：全球产业链各环节主要参与者产业定位和规模：<br><img src="../物联网的概念及其发展前景（转载）/hd008.jpg" alt=""><br><strong>三、资源体系</strong></p><p>物联网发展中的关键资源主要包括标识资源和频谱资源。</p><p><strong>1.</strong> <strong>标识。</strong></p><p>目前，物联网物体标识方面标准众多，很不统一。但大致有条码表示、智能物体标识、RFID标识、通信标识这四种。</p><p><strong>2.</strong> <strong>频谱资源。</strong></p><p>物联网的发展离不开无线通信技术，因此频谱资源作为无线通信的关键资源，同样是物联网发展的重要基础资源。目前在物联网感知层和网络层采用的无线技术包括RFID、近距离无线通信、无线局域网（IEEE 802.11）、蓝牙、蜂窝移动通信、宽带无线接入技术等。目前物联网应用大部分还在发展之中，物联网业务模型尚未完全确定，因此根据物联网业务模型和应用需求对频谱资源需求的分析、对多种无线技术体制“物联”带来的干扰问题分析、对频谱检测技术的研究、对提高空闲频谱频率利用率的方法研究、物联网频谱资源管理方式等方面将是物联网频谱资源研究的关键所在。</p><p><strong>四、我国物联网产业概况</strong></p><p><strong>1.</strong> <strong>产业保持较快增长，部分领域取得局部突破。</strong></p><p>从2009年至今，我国物联网产业迅猛发展，从1700多亿元增长到6000多亿元，年复合增长率超过三成。同时物联网产业链不断健全，政策环境日趋完善、示范项目示范区建设取得较大成效，使我国物联网产业在量增的基础上实现了质的提升。<br><img src="../物联网的概念及其发展前景（转载）/hd009.jpg" alt=""><br>物联网制造业中，我国感知制造获得局部突破，与国外差距在逐步缩小。</p><p>（1）在光纤传感器在高温传感器和光纤光栅传感器方面获得了重大突破，在石油、钢铁、运输、国防等行业实现了批量应用，产品质量达到国际先进水平。</p><p>（2）在RFID 领域，我国中高频RFID 技术产品在安全防护、可靠性、数据处理能力等方面接近国际先进水平，产业链业已成熟，在国内市场占据90%的份额。我国已成功研发出自主的超高频产品并打进了国际市场。</p><p>（3）在工业物联网领域研制成功了面向工业过程自动化的工业无线通信芯片。</p><p>物联网服务业中，我国三大运营商的M2M服务一直是产业亮点。</p><p>中国移动和中国电信分别把物联网业务基地升级成为物联网分公司进行市场化经营。中国联通各类近场支付卡发卡量已经超过200 万张，基于WCDMA 网络的企业专网提供智能公交行车监控及调度系统，用户规模超过100 万，覆盖城市已超过200 个。</p><p><strong>2.</strong> <strong>产业体系相对完善，但不同产业环节所处阶段不同。</strong></p><p>我国物联网产业体系已基本齐全，包括以感知端设备和网络设备为代表的物联网制造业，以网络服务、软件与集成服务、应用服务为代表的物联网服务业。<br><img src="../物联网的概念及其发展前景（转载）/hd010.jpg" alt=""><br>整体看来，我国在M2M 服务、中高频RFID、二维码等产业环节具有一定优势，在基础芯片设计、高端传感器制造、智能信息处理等产业环节依然薄弱；网络通信相关技术和产业支持能力与国外差距相对较小，传感器、低频RFID 等感知端制造产业、高端软件与集成服务与国外差距相对较大。仪器仪表、嵌入式系统、软件与集成服务等产业虽已有较大规模，但真正与物联网相关的设备和服务尚在起步。</p><p>从全球来看，物联网大数据处理和公共平台服务处于起步阶段，物联网相关的终端制造和应用服务仍在成长培育。</p><p><strong>3.</strong> <strong>我国物联网产业已形成四大发展集聚区的空间格局</strong></p><p>已初步形成分别以北京、上海、深圳、重庆为核心的环渤海、长三角、珠三角、中西部地区四大物联网产业集聚区的空间格局，其中：</p><p>（1）环渤海区域以北京为核心，主要借助产学研资源和总部优势，成为我国物联网产业研发、设计、运营和公共服务平台的龙头区域；</p><p>（2）长三角区域以上海、无锡双核发展为带动，是我国物联网最早起步的区域，产业规模在国内也是最大的，整体发展比较均衡，尤其无锡市作为“国家传感网创新示范区”，集聚了大批物联网龙头企业，在技术研发与产业化、以及应用推广方面发挥了引领示范作用；</p><p>（3）珠三角区域以深圳为核心，延续其在传统电子信息领域的研发制造优势，成长为物联网产品制造、软件研发和系统集成的重要基地；</p><p>（4）中西部地区以重庆和武汉为代表，在软件、信息服务、传感器等领域发展迅猛，成为第四大产业基地。</p><p><strong>4.</strong> <strong>传统设备厂商借助物联网技术探索全新的产品服务模式。</strong></p><p>与国际上传统产业与信息产业跨界融合的趋势相辉映，我国也出现设备制造业与物联网、互联网融合，创新产品和服务新模式的现象。家电行业借力物联网技术，已经率先开展拓展价值空间并改善产品服务的模式探索。这种创新模式，不仅涉足智能家居领域和家居设备，还将催生融合物联网元素的多种智能产品，如可穿戴设备、智能汽车设备、医疗健康设备、智能玩具等等。传统产业通过与物联网技术深度融合，同时利用互联网的平台服务以及移动互联网的商业模式，形成开放产业生态创新产品和服务的模式，将成为物联网产业发展的重要方向。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 物联网 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>区块链是什么，如何简单易懂地介绍区块链</title>
      <link href="/2018/05/11/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E5%A6%82%E4%BD%95%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E4%BB%8B%E7%BB%8D%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
      <url>/2018/05/11/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%8C%E5%A6%82%E4%BD%95%E7%AE%80%E5%8D%95%E6%98%93%E6%87%82%E5%9C%B0%E4%BB%8B%E7%BB%8D%E5%8C%BA%E5%9D%97%E9%93%BE/</url>
      
        <content type="html"><![CDATA[<p>知乎用户，一颗匠心</p><p><em><strong>特别提醒</strong>：比特币采用区块链技术，但是区块链并不等同于比特币；全篇基于比特币底层区块链技术讲述，所以，部分模型可能不适用于以太坊等。另外，由于文章采用了一定的抽象、类举的叙事方式，中间或多或少有些地方会跟区块链底层严谨的技术实现有出入，如果让你觉得困惑，可以在评论下方留言或者私信我一起探讨。最后，也是受限于自己知识结构的不完整，这篇文章会随着我对区块链更深入认识后，随时进行修订，最后更新时间可参考该回答下方的时间戳。</em></p><p><em>—</em></p><p>首先不要把区块链想的过于高深，他是一个分布在全球各地、能够协同运转的数据库存储系统，区别于传统数据库运作——读写权限掌握在一个公司或者一个集权手上（中心化的特征），区块链认为，任何有能力架设服务器的人都可以参与其中。来自全球各地的掘金者在当地部署了自己的服务器，并连接到区块链网络中，成为这个分布式数据库存储系统中的一个节点；一旦加入，该节点享有同其他所有节点完全一样的权利与义务（去中心化、分布式的特征）。与此同时，对于在区块链上开展服务的人，可以往这个系统中的任意的节点进行读写操作，最后全世界所有节点会根据某种机制的完成一次又依次的同步，从而实现在区块链网络中所有节点的数据完全一致。</p><p><img src="http://pic4.zhimg.com/70/v2-718333f106a3ba4597bafd533d7e9f77_b.jpg" alt=""><br><a id="more"></a><br><strong># 问题的由来</strong></p><p>我们反复提到区块链是一个去中心化的系统，确实，「去中心化」在区块链世界里面是一个很重要的概念，很多模型（比如账本的维护、货币的发行、时间戳的设计、网络的维护、节点间的竞争等等等等）的设计都依赖于这个中心思想，那到底什么是去中心化呢？在解释真正去中心化之前，我们还是先简单了解下什么是中心化吧。</p><p><strong>中心化？</strong></p><p>回忆一下你在网上购买一本书的流程：</p><ol><li>第一步，你下单并把钱打给<strong>支付宝</strong>；</li><li>第二步，<strong>支付宝</strong>收款后通知卖家可以发货了；</li><li>第三步，卖家收到<strong>支付宝</strong>通知之后给你发货；</li><li>第四步，你收到书之后，觉得满意，在<strong>支付宝</strong>上选择确认收货；</li><li>第五步，<strong>支付宝</strong>收到通知，把款项打给卖家。流程结束。</li></ol><p>你会发现，虽然你是在跟卖家做交易，但是，所有的关键流程都是在跟支付宝打交道。这样的好处在于：万一哪个环节出问题，卖家和买家都可以通过支付宝寻求帮助，让支付宝做出仲裁。这就是一个最简单的基于中心化思维构建的交易模型，它的价值显著，就是建立权威，通过权威背书来获得多方的信任，同时依赖权威方背后的资本和技术实力确保数据的可靠安全。</p><p>你一定会摆出一个巨大的问号脸 ⊙.⊙?——“通过权威背书来获得多方的信任，同时依赖权威方背后的资本和技术实力确保数据的可靠安全”，真的可以嘛？！</p><p>假如说，支付宝程序发生重大 BUG，导致一段时间内的转账记录全部丢失，或者更彻底一点，支付宝的服务器被金三胖的一个导弹全部炸毁了。而我刚刚转出去的 100 元找谁说理去，这个时候，你就成了刀殂上的鱼肉；支付宝有良心，会勉为其难承认你刚刚转账的事实，但他不承认你也没辙，因为确实连他自己也不知道这笔转账是否真实存在。</p><p>上述就是中心化最大的弊端——过分依赖中心和权威，也就意味着逐渐丧失自己的话语权。</p><p><strong>去中心化？</strong></p><p>那么去中心化的形态是什么样子呢？还是拿刚才那个例子继续，我们构建一个极简的去中心化的交易系统，看看我们是如何在网络上从不认识的卖家手里买到一本书的。</p><ol><li>第一步，你下单并把钱打给卖家；</li><li>第二步，你将这条转账信息记录在自己账本上；</li><li>第三步，你将这条转账信息广播出去；</li><li>第四步，卖家和支付宝在收到你的转账信息之后，在他们自己的账本上分别记录；</li><li>第五步，卖家发货，同时将发货的事实记录在自己的账本上；</li><li>第六步，卖家把这条事实记录广播出去；</li><li>第七步，你和支付宝收到这条事实记录，在自己的账本上分别记录；</li><li>第八步，你收到书籍。至此，交易流程走完。</li></ol><p>刚才“人为刀俎我为鱼肉”的情况在这个体系下就比较难发生，因为所有人的账本上都有着完全一样的交易记录，支付宝的账本服务器坏了，对不起卖家的账本还存在，我的账本还存在；这些都是这笔交易真实发生的铁证。</p><p>当然，在这套极简的交易系统中，你已经发现了诸多漏洞和不理解，比如说三方当中有一个是坏人，他故意记录了对他更有利的转账信息怎么办；又比如说消息在传递过程中被黑客篡改了怎么办等等等等。这在以往的计算机概论或者计算机网络书本上中可能都有提及到——“类两军”和“拜占庭将军”问题。这里就不打算赘述，因为暂时跟主线不相关，感兴趣的同学可以去 Google 或者百度一下，你只需要知道，在我们下面即将展开讲到的区块链系统中，通过巧妙的设计，足以解决上述存在的 BUG。</p><p>既然话已说到这份上，相信了解一点技术、特别是有运维背景的同学大概能够从极简交易系统中窥视到了更多区块链的一些影子——</p><ol><li>分布式存储，通过多地备份，制造数据冗余</li><li>让所有人都有能力都去维护共同一份数据库</li><li>让所有人都有能力彼此监督维护数据库的行为</li></ol><p>在我看来，你猜测的基本上没错。其实这些就是区块链技术最核心的东西，外人看起来高大上、深不可测，但探究其根本发现就是这么简单和淳朴。当然，这里面肯定会有很多很多很多细枝末节的技术需要重构。</p><p>如果你差不多认同上面的观点，那我们应该基本上可以达成共识，分布式部署肯定是构建去中心化网络理所当然的解决方向——通过 P2P 协议将全世界所有节点计算机彼此相互连接，形成一张密密麻麻的网络；以巧妙的机制，通过节点之间的交易数据同步来保证全球计算机节点的数据共享和一致。</p><p>哈哈，说的轻巧，“交易数据这么重要的东西，在一个完全不信任的 P2P 网络节点中以一种错综复杂的方式传递，数据的一致性和安全性谁来保证，如果说互相监督，他们到底怎么做到？”</p><p>好了，不卖关子了，下面让我们围绕这个最最最最直接的问题开始进入到真正区块链的世界，抽丝剥茧看看它到底是如何一步一步形成的，又是如何一步一步稳定运转。</p><p><strong># 从全球节点到交易数据</strong></p><p><img src="http://pic3.zhimg.com/70/v2-a5b175bd7956cb7eb58cd2bfd3cf4f2e_b.jpg" alt=""></p><p>这张图的制作的意义为的是帮助你在宏观上先快速理解区块链中所涉及到的相关名词以及他们的层级关系。同时，文章的知识结构和设计思路也大抵上也会按照：</p><ol><li>首先，将区块作为最小单位体，讲述极简区块链系统是如何运转的；</li><li>接着，进入到比区块更小单位体——交易记录，理解区块链是如何处理数据的；</li><li>最后，将所有知识点柔和在一起，重回到区块和区块链，完整讲述整个工作流程。</li></ol><p>希望你在这个引导和结构下有一个比较好的阅读体验。Let’s go~</p><p><strong># 区块，混沌世界的起源</strong></p><p>既然已经达成共识，所以，我们事先构建好了一个去中心化的 P2P 网络；同时，为了让读者朋友们听起来更轻松，我先粗暴的规定在这个极简的区块链系统里，每十分钟有且仅产生一笔交易。</p><p>故事继续，在节点的视野里，大概每十分钟会凭空产生一个建立在自己平行宇宙世界的神奇区块（你可以将区块想象为一个盒子），这个区块里放着一些数字货币以及一张小纸条，小纸条上记录了这十分钟内产生的那唯一一笔交易信息，比如说——“小 A 转账给了小 B100 元”；当然，这段信息肯定是被加密处理过的，为的就是保证只有小 A 和小 B（通过他们手上的钥匙）才有能力解读里面真正的内容。</p><p>这个神奇的区块被创造出来之后，很快被埋在了地底下，至于埋在哪里？没有一个人不知道，需要所有计算机节点一起参与进来掘地三尺后才有可能找到（找到一个有效的工作量证明）。显然，这是一件工作量巨大、成果随机的事件。但是呢，对于计算机节点来说，一旦从地底下挖出这个区块，他将获得区块内价值不菲的数字货币，以及“小 A 转账给了小 B100 元”过程中小 A 所支付的小费。同时，对于这个节点来说，也只有他才有权利真正记录小纸条里的内容，这是一份荣耀，而其他节点相当于只能使用它的复制品，一个已经没有数字货币加持的副本。当然这个神奇的区块还有一些其他很特别的地方，后面我们会再细细聊。</p><p>为了更好的描述，我们将计算机节点从地底下挖出区块的过程叫做「挖矿」，刚才说了，这是一件工作量巨大、运气成分较多、但收益丰厚的事儿。</p><p>过了一会儿，来自中国上海浦东新区张衡路上的一个节点突然跳出来很兴奋的说：“ 我挖到区块了！里面的小纸条都是有效的！奖励归我！” 。虽然此刻张衡路节点已经拿到了数字货币，但对于其他计算机节点来说，因为这里面还涉及到其他一些利益瓜葛，他们不会选择默认相信张衡路节点所说的话；基于陌生节点彼此不信任的原则，他们拿过张衡路节点所谓挖到的区块（副本），开始校验区块内的小纸条信息是否真实有效等等。在区块链世界里，节点们正是通过校验小纸条信息的准确性，或间接或直接判断成功挖出区块的节点是否撒谎。（如何定义小纸条信息真实有效，后面会讲解，这里暂不做赘述）。</p><p>在校验过程中，各个节点们会直接通过下面两个行为表达自己对张衡路节点的认同（准确无误）和态度：</p><ul><li>停止已经进行了一半甚至 80%的挖矿进程；</li><li>将张衡路节点成功挖出的区块（副本）追加到自己区块链的末尾。</li></ul><p>你可以稍微有点困惑：停止可能已经执行了 80%的挖矿行为，那之前 80%的工作不是就白做了嘛？！然后，区块链的末尾又是个什么鬼东西？</p><p>对于第一个困惑。我想说，你说的一点没错，但是没办法，现实就是这么残酷，即便工作做了 80%，那也得放弃，这 80%的工作劳苦几乎可以视为无用功，绝对的伤财劳众。第二个困惑，区块链和区块链的末尾是什么鬼？这里因为事先并没有讲清楚，但是你可以简单想象一下：区块是周期性不断的产生和不断的被挖出来，一个计算机节点可能事先已经执行了 N 次“从别人手上拿过区块 -&gt; 校验小纸条有效性”的流程，肯定在自己的节点上早已经存放了 N 个区块，这些区块会按照时间顺序整齐的一字排列成为一个链状。没错，这个链条，就是你一直以来认为的那个区块链。如果你还是不能够理解，没关系，文章后面还会有很多次机会深入研究。</p><p><strong># 走进区块内，探索消息的本质</strong></p><p>上面我们构建了一个最简单的区块链世界的模型，相信大多数同学都已经轻松掌握了。但是别骄傲也别着急，这还只是一些皮毛中的皮毛，坐好，下面我们准备开车了。</p><p>前面我们说到“大概每十分钟会凭空产生一个神奇的区块，这个区块里放了一张小纸条，上面记录了这十分钟内产生的这唯一一笔交易信息”。显然，十分钟内产生的交易肯定远不止一条，可能是上万条，这上万条数据在区块链世界是如何组织和处理的呢？另外，为什么在纸条上记录的只是某一次的交易信息，而不是某一个人的余额？余额好像更符合我们现实世界的理解才对。</p><p>既然存在这样那样的疑问。现在我们就把视线暂时从“区块”、“区块链”这些看起来似乎较大实体的物质中移开，进入到区块内更微观的世界里一探究竟，看看小纸条到底是怎么一回事，它的产生以及它终其一生的使命：</p><ol><li>发起交易的时候，发起人会收到一张小纸条，他需要将交易记录比如说“盗盗转账给张三 40 元”写在纸上。说来也神奇，当写完的那一刹那，在小纸条的背面会自动将这段交易记录格式化成至少包含了“输入值”和“输出值”这两个重要字段；“输入值”用于记录数字货币的有效来源，“输出值”记录着数字货币发往的对象。</li><li>刚刚创建的小纸条立马被标记成为“<strong>未确认</strong>”的小纸条。从地下成功挖出区块并最终连接到区块链里的小纸条一开始会被标记为“<strong>有效</strong>”。若这条有效的小纸条作为其他交易的输入值被使用，那么，这个有效的小纸条很快会被标记为“<strong>无效</strong>”。因为各种原因，区块从链上断开、丢弃，曾经这个区块内被标记为“有效”的小纸条会被重新标记为“未确认”。</li><li>区块链里面没有账户余额的概念，你真正拥有的数字资产实际上是一段交易信息；通过简单的加减法运算获知你数字钱包里的余额。</li></ol><p>上面的 1、2、3 仅仅作为结论一开始强行灌输给你的知识点，其中有几个描述可能会有点绕，让你觉得云里雾里，没有关系，因为我们立刻、马上就开始会细说里面的细枝末节。</p><p><img src="http://pic2.zhimg.com/70/v2-9b6d8ac951f48d7cd31b7d4f3e266c69_b.jpg" alt=""></p><p>上图就是从无数打包进区块内的小纸条中，抽取出来的一张，以及它最终被格式化后的缩影。单看右侧的图可能很容易产生误会，虽然看起来有多行，但实际上就是“盗盗转账给张三 40 个比特币”这一条交易数据另外的一种呈现形态。因为区块链世界里面这么规定，每一条交易记录，必须有能力追溯到交易发起者 发起这笔交易、其中所涉及金额的上一笔全部交易信息；即这笔钱从何而来的问题。这其实很容易理解，在去中心化的网络中，通过建立交易链、和通过交易链上的可溯源性间接保证数据安全和有效。</p><p>我们继续看，在区块链世界里，我们是如何仅通过“盗盗转账给张三 40 个比特币” 这条交易信息完成转账流程的。其实跟现实中你在路边买一个包子的流程大抵上相同。</p><p><strong>第一步：判断是否有足够的余额完成交易</strong></p><p>这里我们再一次重申，区块链世界是没有余额的概念，余额是通过简单数字的加减最终获得，你拥有所谓的数字货币实际上是因为你拥一条交易记录，即 “盗盗转账给张三 40 个比特币”！这里，我们还是拿这条记录说事：</p><p>追溯“输出值”是“盗盗”相关的<strong>全部有效交易记录</strong>作为，对有效交易中的数字进行简单求和，判断是否大于等于 40，如果确实大于等于，则将这些有效的交易记录合并形成一条新的交易记录（如下图）。如果小于 40，其实可以不需要再继续往下探讨。</p><p><img src="http://pic3.zhimg.com/70/v2-90202117662d6ffef784185525b7573e_b.jpg" alt=""></p><p>就上图的例子，我们追溯到曾经转账给盗盗的<strong>有效交易记录</strong>有“小 A 转账给盗盗 10 btc”、“小 B 转账给盗盗 20 btc”、“小 C 转账给盗盗 25 btc”，我们需要将这三条交易记录合并成一条更复杂描述的交易记录，即 “( 小 A 转账给盗盗 10 btc + 小 B 转账给盗盗 20 btc + 小 C 转账给盗盗 25 btc ) 转账给张三 40 btc ”</p><p><strong>第二步：判断是否需要找零</strong></p><p>对追溯到的有效交易数字求和，如果发现大于需要支付的金额，需要将多出的数字重新支付给自己，相当于找零。对应生成了一条全新的交易记录（如下图）。</p><p><img src="http://pic3.zhimg.com/70/v2-7b564661d4e1d6693ed89b71e0c68192_b.jpg" alt=""></p><p>就上图例子来说，我们最后合并成的交易记录 “( 小 A 转账给盗盗 10 btc + 小 B 转账给盗盗 20 btc + 小 C 转账给盗盗 25 btc + 盗盗转账给盗盗 15 btc ) 转账给张三 40 btc ” 事实上等同于“盗盗转账给张三 40 btc”。其中“盗盗转账给盗盗 15 btc”就可以理解找零。</p><p><strong>第三步：发出去，让全球节点认同和备份小纸条</strong></p><p>这条内部重新处理过的复杂交易记录被塞进区块，埋到地下，等待节点挖出来，一旦区块被挖矿成功，并且该区块最终被连在了区块链的主链上。张三将最终拥有了这条交易记录，而先前的“小 A 转账给盗盗 10 btc” 、“小 B 转账给盗盗 20 btc” 、“小 C 转账给盗盗 25 btc”都将被视为已经使用过的交易记录——从此被贴上“无效”的标签，意味着这些交易记录将永远不会再被追溯到。</p><p>我们最后一次重申，只是希望让你加深印象：拥有数字货币=拥有交易记录！</p><p><strong>通过设计巧妙的精巧密码学保证数据安全</strong></p><p>记录着交易信息的小纸条借助区块这个载体，在分布式的网络中以不同的轨迹错综复杂的传递，我们前面说了，你真正拥有的数字资产实际上是一段交易信息，而不是你常规意义上理解的货币。所以这个过程就需要重点解决两个问题：</p><ul><li>接受到的这条交易记录在传输过程没有被其他人所篡改</li><li>接受到的这条交易记录确实是由发起交易的人所创造</li></ul><p>在这里，我们需要事先引入两个知识点，可能稍微有点难消化，但都是计算机领域较为成熟的和基础的概念。</p><p>第一个知识点：Hash()函数。你只需要知道，任意长度的字符串、甚至文件体本身经过 Hash 函数工厂的加工，都会输出一个固定长度的字符串；同时，输入的字符串或者文件稍微做一丢丢的改动，Hash() 函数给出的输出结果都将发生翻天覆地的改变。注意，Hash()函数是公开的，任何人都能使用。</p><p><img src="http://pic4.zhimg.com/70/v2-c9ae46fb889b14065f7988e3db803777_b.jpg" alt=""></p><p>第二个知识点：非对称加密。你也只需要了解，任何人手里都有两把钥匙，其中一把只有自己知道，叫做“私钥”，以及一把可以公布于众，叫做“公钥”；通过私钥加密的信息，必须通过公钥才能解密，连自己的私钥也无解。公钥可以通过私钥生成多把。</p><p>有了这些知识点的加持，上面两个问题开始变得有解。下面我们来看下内部是如何扭转和工作的吧，这里拿“小 A 转账给了小 B 100 元钱” 举例：</p><p><img src="http://pic2.zhimg.com/70/v2-5491adb338c4d680f05797525b452e6d_b.jpg" alt=""></p><ol><li><strong>第一步</strong>：小 A 会先用 Hash 函数对自己的小纸条进行处理，得到一个固定长度的字符串，这个字符串就等价于这张小纸条。</li><li><strong>第二步</strong>：小 A 使用只有自己知道的那一把私钥，对上面固定长度的字符串进行再加密，生成一份名叫数字签名的字符串，这份数字签名能够充分证明是基于这张小纸条的。你可以这么理解，在现实中，你需要对某一份合同的签署，万一有人拿你曾经在其他地方留下的签名复制粘贴过来怎么办？！最好的办法，就是在你每一次签名的时候，故意在字迹当中留下一些同这份合同存在某种信息关联的小细节，通过对小细节的观察可以知道这个签名有没有被移花接木。步骤一和步骤二的结合就是为了生成这样一份有且仅针对这条小纸条有效的签名。</li><li><strong>第三步：</strong>小 A 将「明文的小纸条」、刚刚加密成功的「数字签名」，以及自己那把可以公布于众的「公钥」打包一起发给小 B。</li><li><strong>第四步：</strong>当小 B 收这三样东西，首先会将明文的小纸条进行 Hash()处理，得到一个字符串，我们将其命名为“字符串 1”。然后，小 B 使用小 A 公布的公钥，对发过来的数字签名进行解密，得到另外一个“字符串 2”。通过比对“字符串 1”和“字符串 2”的一致性，便可充分证明：小 B 接受到的小纸条就是小 A 发出来的小纸条，这张小纸条在中途没有被其他人所篡改；且这张小纸条确实是由小 A 所编辑。</li></ol><p>可以看得出来，加解密的过程几乎是一环套一环，中途任何环节被篡改，结果都是大相径庭。借助这一连串的机制，其实已经能够很好的在公开、匿名、彼此不信任的分布式网络环境中解决数字交易过程中可能遇到的很多问题。这个环节可能确实有点难理解，现在，我需要你停下来，静下心，花上几分钟闭目慢慢回味其中设计精湛的地方。</p><p>掌握了这部分知识以后，我们在这里回答一下前面没有解释清楚的问题，「节点对区块的检验」检验的到底是什么？实际上就是：</p><ul><li>检验区块内的交易记录签名是否准确（是否被篡改）</li><li>检验区块内的交易记录输入值是否“有效”（是否使用过）</li><li>检验区块内的交易记录输入值的数字之和是否大于等于输出值的数字</li><li>…</li></ul><p><strong># 重回“区块”和“区块链”的世界</strong></p><p>好了，对小纸条和交易记录的研究我们点到为止，其实信息量已经是巨大的了，让我们合上盖子，重回较大实体、继续聊聊“区块”和“区块链”的话题。还记得，咱们在一开始讲到关于区块的特征吗？区块创造后被埋在地下，需要经过节点们马不停蹄的挖采、而且是凭运气的挖采才有可能获得——不仅仅如此，事实上他还有其他很多神奇的地方，比如说：</p><ol><li>凭空产生的区块在刚刚创建的时候会形成一股强大的黑洞效应，它会尝试将这段时间全世界各个节点上产生的所有小纸条（交易记录）统统吸进来；在合上区块盖子之前，同时会在区块内放上一些数字货币以及其他一些东西。</li><li>区块拥有一个唯一的 ID，但它只会在这个区块被节点成功从地下挖出来之后创建。这个 ID 至少会跟「区块内所有小纸条的集合」、「即将与之相连的上一个区块 ID」以及「挖矿节点的运气值」等因素相关。既然前面我们已经简单了解了“Hash()函数”这个东西，这里不妨透露给大家：“区块 ID = Hash(‘区块内所有小纸条的集合’+’即将与之相连的上一个区块 ID’+‘挖矿节点的运气值’+’…’)” ；基于先前掌握的知识，然后你应该知道区块内任意一张小纸条的信息稍微做改动、或者节点挖矿运气好一点坏一点等等，当前区块的 ID 都会 “ biu~ ”的发生改变。</li></ol><p>基于上述 1、2 点，如果阅读足够仔细的同学可能会有些头大。在文章开头为了更好的描述，我在设计简化区块链系统的时候故意模棱两可了几个概念，这也许已经误导到了部分同学。这里不得不停下来和你一起修正下之前在你大脑中已经构建的区块链世界观。我们前面讲道，“在节点的视野里，大概每十分钟会凭空产生一个建立在自己平行宇宙世界的神奇区块”。如何正确去理解这句话呢？——拥有上帝之眼的你，可以这么拆解问题、看待问题：</p><ol><li>同一个周期内，全网并不是产生唯一的一个区块等待挖掘；每个节点事实上都在周期性的创造区块和挖出区块；只是在某一个节点的视野里，它不能感知到另外一个节点上区块的产生。为何这里要特别强调“在某一个节点的视野里”，就是因为我们刚刚讲到，从区块的视角来说，区块的凭空产生，是基于即将与之相连的上一个区块 ID；而从节点的视角来看，区块的凭空产生是基于当前节点区块链末尾的那个区块 ID 产生的。</li><li>全网会尽力控制在一个周期内只有一个节点能够成功挖出区块，但是不能够完全避免多个节点同时挖出区块的可能性；如何尽力控制？比如说，当大伙挖矿的热情高涨、工作效率提高，区块会被埋在更深更广的地方等。简而言之，通过提高工作难度，来维持这个平衡。另外，值得注意的：产生区块、挖出区块、校验区块，他们的时间周期近乎相同。</li></ol><blockquote><p><strong><em>挖矿，本质是通过与或运算，去碰撞一个出一个满足规则的随机数。这个部分要细讲的话，估计又可以写出两三千字来。我觉得到目前为止，并不影响主轴知识点的讲解，这一块会作为后续知识结构的完善被撰写（计划 1 月 24 日前完成）。感兴趣的同学可以 Google 百度查阅下什么叫“挖矿”、“工作量证明 POW”等。</em></strong></p></blockquote><p><strong>分叉</strong></p><p>现在，我们终于对“区块”这个概念有了更全面的认识，文章开头讲的故事就可以继续展开来絮叨絮叨：</p><p>假如几乎同一时间，「中国上海浦东新区张衡路」上的节点和「美国纽约曼哈顿第五大道」上的节点异口同声喊出来：“我挖到区块了！里面的小纸条都是有效的！奖励归我！”。其他节点也几乎同时参与了对这两个区块的校验，结果发现这俩都没毛病，各节点也开始犯困，因为在他们的视野里他们并不清楚最后哪一个区块应该会被主链接纳。算了！都连在自己区块链尾巴上吧，这时尴尬了，区块链硬生生的被分叉了！</p><p><img src="http://pic1.zhimg.com/70/v2-adf7653af863541e4cc811b04b86bd20_b.jpg" alt=""></p><p>你肯定在想，那还得了，这种情况继续下去，每个节点的区块以及他们整理维护的小纸条都将变得不一样，这已经严重违背了区块链世界里第一大最基本原则——所有节点共同维护同一份数据。所以，为了解决这个问题，区块链世界引入了一条新的规则——拥有最多区块的支链将是真正被认可有价值的，较短的支链将会被直接 Kill 掉。</p><p>我们大伙都知道挖矿的过程存在巨大的工作量（如果没有任何难度，把区块扔在人群中，必然同一时间发现区块的节点数量将大大增加，也就会产生无数的支链，通过这个例子，你大概也就能够明白，区块链世界为什么需要设置工作难度了吧），并且在计算机的硅基世界里，不可能出现所谓 “同时” 的概念，哪怕纳秒的差别，那也总是会有先后顺序。所以理论上，“分叉”的这种僵局很快会在下一个区块被挖掘出来（以及校验区块）的时候被打破，实在不行下下个，或者下下下个……总之机制可以让整个分叉的区块链世界迅速稳定下来。</p><p><img src="http://pic2.zhimg.com/70/v2-ea23da89340837df3e6e0f6cd2d072d1_b.jpg" alt=""></p><p>“分叉”这种僵局在确认下一个区块（以及校验小纸条）的时候被打破，从而整个区块链世界迅速稳定下来</p><p>就上图而言，所有基于张衡路节点挖矿获得的区块以及后续区块的那条分支被视为有价值，最终会全部保留了下来；其他节点会统一效仿那个拥有更长分支链的节点所做的决策。另外，值得一提的是，同一时间，较短分支上的区块会立即丢弃，而里面的小纸条也会随之释放出来，被重新标记上“未确认”。</p><p><strong>“双花”与“51% 攻击”</strong></p><p>你可能已经开始困惑或者有点兴奋，末尾几个区块的排序在修复过程中，因为时间差肯定会产生一些模棱两可的地方，这往往会给数据安全埋下一颗雷。一个最简单的假设——我记录的一张小纸条很不巧地被归在了一条较短的支链上，这条支链在竞争过程中理所当然输掉了比赛，区块被丢弃、小纸条被无情的贴上“未确认”的标签。在等待下次区块重新确认的过程中，这个时间差内，我，好像、似乎可以做点什么坏事 ԅ(¯﹃¯ԅ) ，就比如说“双花”（双花，花两次，双重支付的意思）</p><p>你脑海中也许很快浮过的这样的构想，可不可能通过下面这种方法触发双花问题的产生，从而让我不劳而获：</p><p><img src="http://pic4.zhimg.com/70/v2-82f01cc16cbd1f3227ae107fb75723db_b.jpg" alt=""></p><ol><li>假设有一个名叫 X-Man 的坏家伙，他控制了一个计算机节点，这个节点拥有比地球上任何一个节点算力都强大的计算机集群。</li><li>首先，X-Man 事先创造了一条独立的（不去广而告之）、含有比较多区块的链条。其中一个区块里放着“X-Man 转账给 X-Man 1000 元”的纸条。</li><li>接着，X-Man 跟张三购买了一部手机，他在小纸条上记录下“X-Man 转账给张三 1000 元”；这条信息被三次确认后（即三个区块被真实挖出、校验和连接），然后，张三把手机给了 X-Man。</li><li>X-Man 拿到手机之后，按下机房的开关，试图将先前已经创造的区块链条连接在自己这个节点区块链的末尾。</li><li>大功告成，X-Man 拥有了一条更长的区块链条，那些较短、存放着“X-Man 转账给张三 1000 元”的区块链，以及在区块链世界里那则真实转账行为被一同成功销毁。（?）</li></ol><p>事实真的如此吗？在这里我可以很负责任的说，too young too simple，区块链世界规则的制定远比我们想象的要健全很多，还记得我们之前讲的“区块的 ID 至少会跟区块内所有小纸条的集合、即将与之相连的上一个区块 ID 以及挖矿节点的运气值等因素相关”。 在这里，正是因为打算连接到主链的时候，事先准备的链子会意识到马上要连接上的那个区块 ID 发生了改变，随之而来的是后面所有区块 ID 都瞬间。节点不得不重新对后续区块的解锁以及对区块内小纸条的校验。</p><p><img src="http://pic3.zhimg.com/70/v2-5ac7eb9be1b35362b75443020e59974a_b.jpg" alt=""></p><p>在区块链的世界，重新计算的行为等同于把自己（节点）置身于同一个起跑线，跟世界上其他所有的节点一同竞争挖矿。你会说，我拥有更强大的计算能力，但是对不起，跟你竞争的对象并不是第五大道、南京西路、香榭丽舍大道上的某一个节点，而是全球所有算力的集合，在这个集合中，你拥有的算力永远都只是一个很小的子集。所以，根据区块链算力民主、少数服从多数的基本原则，这个构想将永远不会成立。</p><p><strong>除非….</strong></p><p>你控制着全球 51%的算力，这也就是区块链世界里另外一个著名的概念，叫做“51% 攻击”，但这也仅仅是一个理论值，在真实世界里这样的攻击我个人觉得是很难发动起来的，这里面就牵涉到很多经济、哲学甚至政治的因素。举个最简单的例子：X-Man 为了回滚刚刚发生的一笔交易记录，成功发起了 51% 攻击，这意味着很快整个区块链系统将会崩盘，因为这次攻击已经严重伤害到人们对这套系统的信任，接着比特币开始暴跌至几乎一文不值；但是这个拥有 51% 算力的 X-Man 原本完完全全可以通过挖矿的方式获取更多收益，购买无数的 iPhone 手机。那他不是脑袋不是坏了还能是啥？对 51% 攻击话题感兴趣的同学可以阅读这篇文章《<a href="http://8btc.com/article-1949-1.html" target="_blank" rel="noopener">什么是比特币 51% 攻击？</a>》。</p><p>至此，我觉得区块链最基础、最核心的知识已经全部讲完了（除了挖矿内部实现原理，作为一个遗憾留在这里，有时间会完善掉），相信你已经对它有了一个宏观的认识。另外，由于这篇文章采用了适当抽象、类举的叙事方式，中间或多或少有些地方会跟区块链底层严谨的技术实现有出入，欢迎大家来纠错。另外，也是受限于自己知识结构的缺失，这篇文章会随着我对区块链更深入认识后，随时进行修订，最后更新时间可参考该回答下方的时间戳。</p><p>–</p><p><strong>问答部分</strong></p><p><strong>去中心化的系统中，到底是谁在发行货币？是无限量发行吗？</strong></p><p>比特币的货币是通过挖矿（工作量证明）来发行的，总数量是通过程序写死了 2100 万个，而第一笔区块奖励也是硬编码写死的。矿工挖出一个区块所获得的奖励，每隔 21 万个区块将减少一半，按照平均 10 分钟挖出一个区块的执行效率，也就就说差不多每四年会锐减一次。2009 年 1 月起每个区块奖励 50 个比特币，2012 年 11 月减半为每个区块 25 个比特币，2016 年 7 月减半为 12.5 个比特币。基于这个规则，到 2140 年，所有比特币(20,999,999,980)将全部发行完毕，之后不会再有新的比特币产生。</p><p><strong>矿工节点的收益除了挖出区块以外还有哪些？</strong></p><p>矿工节点的收益主要由两部分组成：1）挖出新区块的奖励；2）挖出新区块内所含交易的交易费。但就目前来说，一个区块内的交易费大概只占到矿工总收入的 0.5%甚至更少，大部分收益主要还是来自于挖矿所得的比特币奖励。然而，随着挖矿奖励的递减，以及每个区块中包含的交易数量增加，交易费在矿工收益中所占的比重将会逐渐增加。在 2140 年之后，所有的矿工收益将完全由交易费构成。</p><p><strong><strong>参考</strong></strong></p><ul><li>中心化与去中心化  <a href="https://www.douban.com/note/624421270/" target="_blank" rel="noopener">https://www.douban.com/note/624421270/</a></li><li>图说区块链  <a href="https://book.douban.com/subject/27084306/" target="_blank" rel="noopener">https://book.douban.com/subject/27084306/</a></li><li>区块链是什么，如何简单易懂地介绍区块链？  <a href="https://www.zhihu.com/question/37290469" target="_blank" rel="noopener">https://www.zhihu.com/question/37290469</a></li><li>什么是比特币 51%攻击？  <a href="http://8btc.com/article-1949-1.html" target="_blank" rel="noopener">http://8btc.com/article-1949-1.html</a></li><li>区块链与新经济：数字货币 2.0 时代  <a href="https://book.douban.com/subject/26804497/" target="_blank" rel="noopener">https://book.douban.com/subject/26804497/</a></li><li>詳解比特幣原理和運行機制  <a href="https://www.youtube.com/watch?v=P4seQcP77H4" target="_blank" rel="noopener">https://www.youtube.com/watch?v=P4seQcP77H4</a></li><li>区块链是什么：从技术架构到哲学核心  <a href="https://v.qq.com/x/page/x0518nuh2z7_0.html" target="_blank" rel="noopener">https://v.qq.com/x/page/x0518nuh2z7_0.html</a></li><li>区块链核心算法解析  <a href="https://book.douban.com/subject/27081206/" target="_blank" rel="noopener">https://book.douban.com/subject/27081206/</a></li><li>深入理解比特幣的安全性及程式交易安全性與相關的密碼學原理  <a href="https://www.youtube.com/watch?v=3w1Tg3B_oKQ" target="_blank" rel="noopener">https://www.youtube.com/watch?v=3w1Tg3B_oKQ</a></li><li>深度了解区块链——拜占庭将军问题深入探讨  <a href="https://wallstreetcn.com/articles/338061" target="_blank" rel="noopener">https://wallstreetcn.com/articles/338061</a></li><li>精通比特币 - 挖矿与共识  <a href="http://zhibimo.com/read/wang-miao/mastering-bitcoin/Chapter08.html" target="_blank" rel="noopener">http://zhibimo.com/read/wang-miao/mastering-bitcoin/Chapter08.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 区块链 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据核心技术介绍（转载）</title>
      <link href="/2018/05/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/"/>
      <url>/2018/05/08/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%BB%8B%E7%BB%8D%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>作者：网易云<br>链接：<a href="https://www.zhihu.com/question/27696290/answer/381993207" target="_blank" rel="noopener">https://www.zhihu.com/question/27696290/answer/381993207</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  </p><p>大数据技术的体系庞大且复杂，基础的技术包含数据的采集、数据预处理、分布式存储、NoSQL数据库、数据仓库、机器学习、并行计算、可视化等各种技术范畴和不同的技术层面。首先给出一个通用化的大数据处理框架，主要分为下面几个方面：<strong>数据采集与预处理、数据存储、数据清洗、数据查询分析和数据可视化。</strong></p><img src="/2018/05/08/大数据核心技术介绍（转载）/01.jpg"><a id="more"></a><h2 id="一、数据采集与预处理"><a href="#一、数据采集与预处理" class="headerlink" title="一、数据采集与预处理"></a>一、数据采集与预处理</h2><p>对于各种来源的数据，包括移动互联网数据、社交网络的数据等，这些结构化和非结构化的海量数据是零散的，也就是所谓的数据孤岛，此时的这些数据并没有什么意义，数据采集就是将这些数据写入数据仓库中，把零散的数据整合在一起，对这些数据综合起来进行分析。数据采集包括文件日志的采集、数据库日志的采集、关系型数据库的接入和应用程序的接入等。在数据量比较小的时候，可以写个定时的脚本将日志写入存储系统，但随着数据量的增长，这些方法无法提供数据安全保障，并且运维困难，需要更强壮的解决方案。</p><p>Flume NG作为实时日志收集系统，支持在日志系统中定制各类数据发送方，用于收集数据，同时，对数据进行简单处理，并写到各种数据接收方(比如文本，HDFS，Hbase等)。Flume NG采用的是三层架构：Agent层，Collector层和Store层，每一层均可水平拓展。其中Agent包含Source，Channel和 Sink，source用来消费（收集）数据源到channel组件中，channel作为中间临时存储，保存所有source的组件信息，sink从channel中读取数据，读取成功之后会删除channel中的信息。</p><p>NDC，<a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/FL63Zv9Zou86950w/article/details/78125017" target="_blank" rel="noopener">Netease Data Canal</a>，直译为网易数据运河系统，是网易针对结构化数据库的数据实时迁移、同步和订阅的平台化解决方案。它整合了网易过去在数据传输领域的各种工具和经验，将单机数据库、分布式数据库、OLAP系统以及下游应用通过数据链路串在一起。除了保障高效的数据传输外，NDC的设计遵循了单元化和平台化的设计哲学。</p><p>Logstash是开源的服务器端数据处理管道，能够同时从多个来源采集数据、转换数据，然后将数据发送到您最喜欢的 “存储库” 中。一般常用的存储库是Elasticsearch。Logstash 支持各种输入选择，可以在同一时间从众多常用的数据来源捕捉事件，能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。</p><p>Sqoop，用来将关系型数据库和Hadoop中的数据进行相互转移的工具，可以将一个关系型数据库(例如Mysql、Oracle)中的数据导入到Hadoop(例如HDFS、Hive、Hbase)中，也可以将Hadoop(例如HDFS、Hive、Hbase)中的数据导入到关系型数据库(例如Mysql、Oracle)中。Sqoop 启用了一个 MapReduce 作业（极其容错的分布式并行计算）来执行任务。Sqoop 的另一大优势是其传输大量结构化或半结构化数据的过程是完全自动化的。</p><p>流式计算是行业研究的一个热点，流式计算对多个高吞吐量的数据源进行实时的清洗、聚合和分析，可以对存在于社交网站、新闻等的数据信息流进行快速的处理并反馈，目前大数据流分析工具有很多，比如开源的strom，spark streaming等。</p><p>Strom集群结构是有一个主节点（nimbus）和多个工作节点（supervisor）组成的主从结构，主节点通过配置静态指定或者在运行时动态选举，nimbus与supervisor都是Storm提供的后台守护进程，之间的通信是结合Zookeeper的状态变更通知和监控通知来处理。nimbus进程的主要职责是管理、协调和监控集群上运行的topology（包括topology的发布、任务指派、事件处理时重新指派任务等）。supervisor进程等待nimbus分配任务后生成并监控worker（jvm进程）执行任务。supervisor与worker运行在不同的jvm上，如果由supervisor启动的某个worker因为错误异常退出（或被kill掉），supervisor会尝试重新生成新的worker进程。</p><p>当使用上游模块的数据进行计算、统计、分析时，就可以使用消息系统，尤其是分布式消息系统。Kafka使用Scala进行编写，是一种分布式的、基于发布/订阅的消息系统。Kafka的设计理念之一就是同时提供离线处理和实时处理,以及将数据实时备份到另一个数据中心，Kafka可以有许多的生产者和消费者分享多个主题，将消息以topic为单位进行归纳；Kafka发布消息的程序称为producer，也叫生产者，预订topics并消费消息的程序称为consumer，也叫消费者；当Kafka以集群的方式运行时，可以由一个服务或者多个服务组成，每个服务叫做一个broker，运行过程中producer通过网络将消息发送到Kafka集群，集群向消费者提供消息。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。Kafka可以和Flume一起工作，如果需要将流式数据从Kafka转移到hadoop，可以使用Flume代理agent，将Kafka当做一个来源source，这样可以从Kafka读取数据到Hadoop。</p><p>Zookeeper是一个分布式的，开放源码的分布式应用程序协调服务，提供数据同步服务。它的作用主要有配置管理、名字服务、分布式锁和集群管理。配置管理指的是在一个地方修改了配置，那么对这个地方的配置感兴趣的所有的都可以获得变更，省去了手动拷贝配置的繁琐，还很好的保证了数据的可靠和一致性，同时它可以通过名字来获取资源或者服务的地址等信息，可以监控集群中机器的变化，实现了类似于心跳机制的功能。</p><h2 id="二、数据存储"><a href="#二、数据存储" class="headerlink" title="二、数据存储"></a>二、数据存储</h2><p>Hadoop作为一个开源的框架，专为离线和大规模数据分析而设计，HDFS作为其核心的存储引擎，已被广泛用于数据存储。</p><p>HBase，是一个分布式的、面向列的开源数据库，可以认为是hdfs的封装，本质是数据存储、NoSQL数据库。HBase是一种Key/Value系统，部署在hdfs上，克服了hdfs在随机读写这个方面的缺点，与hadoop一样，Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。</p><p>Phoenix，相当于一个Java中间件，帮助开发工程师能够像使用JDBC访问关系型数据库一样访问NoSQL数据库HBase。</p><p>Yarn是一种Hadoop资源管理器，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。Yarn由下面的几大组件构成：一个全局的资源管理器ResourceManager、ResourceManager的每个节点代理NodeManager、表示每个应用的Application以及每一个ApplicationMaster拥有多个Container在NodeManager上运行。</p><p>Mesos是一款开源的集群管理软件，支持Hadoop、ElasticSearch、Spark、Storm 和Kafka等应用架构。</p><p>Redis是一种速度非常快的非关系数据库，可以存储键与5种不同类型的值之间的映射，可以将存储在内存的键值对数据持久化到硬盘中，使用复制特性来扩展性能，还可以使用客户端分片来扩展写性能。</p><p>Atlas是一个位于应用程序与MySQL之间的中间件。在后端DB看来，Atlas相当于连接它的客户端，在前端应用看来，Atlas相当于一个DB。Atlas作为服务端与应用程序通讯，它实现了MySQL的客户端和服务端协议，同时作为客户端与MySQL通讯。它对应用程序屏蔽了DB的细节，同时为了降低MySQL负担，它还维护了连接池。Atlas启动后会创建多个线程，其中一个为主线程，其余为工作线程。主线程负责监听所有的客户端连接请求，工作线程只监听主线程的命令请求。</p><p>Kudu是围绕Hadoop生态圈建立的存储引擎，Kudu拥有和Hadoop生态圈共同的设计理念，它运行在普通的服务器上、可分布式规模化部署、并且满足工业界的高可用要求。其设计理念为fast analytics on fast data。作为一个开源的存储引擎，可以同时提供低延迟的随机读写和高效的数据分析能力。Kudu不但提供了行级的插入、更新、删除API，同时也提供了接近Parquet性能的批量扫描操作。使用同一份存储，既可以进行随机读写，也可以满足数据分析的要求。Kudu的应用场景很广泛，比如可以进行实时的数据分析，用于数据可能会存在变化的时序数据应用等。</p><p>在数据存储过程中，涉及到的数据表都是成千上百列，包含各种复杂的Query，推荐使用列式存储方法，比如parquent,ORC等对数据进行压缩。Parquet 可以支持灵活的压缩选项，显著减少磁盘上的存储。</p><h2 id="三、数据清洗"><a href="#三、数据清洗" class="headerlink" title="三、数据清洗"></a>三、数据清洗</h2><p>MapReduce作为Hadoop的查询引擎，用于大规模数据集的并行计算，”Map（映射）”和”Reduce（归约）”，是它的主要思想。它极大的方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统中。</p><p>随着业务数据量的增多，需要进行训练和清洗的数据会变得越来越复杂，这个时候就需要任务调度系统，比如oozie或者<a href="https://link.zhihu.com/?target=http%3A//azkaban.github.io/azkaban/docs/latest/%23solo-setup" target="_blank" rel="noopener">azkaban</a>，对关键任务进行调度和监控。</p><p>Oozie是用于Hadoop平台的一种工作流调度引擎，提供了RESTful API接口来接受用户的提交请求(提交工作流作业)，当提交了workflow后，由工作流引擎负责workflow的执行以及状态的转换。用户在HDFS上部署好作业(MR作业)，然后向Oozie提交Workflow，Oozie以异步方式将作业(MR作业)提交给Hadoop。这也是为什么当调用Oozie 的RESTful接口提交作业之后能立即返回一个JobId的原因，用户程序不必等待作业执行完成（因为有些大作业可能会执行很久(几个小时甚至几天)）。Oozie在后台以异步方式，再将workflow对应的Action提交给hadoop执行。</p><p>Azkaban也是一种工作流的控制引擎，可以用来解决有多个hadoop或者spark等离线计算任务之间的依赖关系问题。azkaban主要是由三部分构成：Relational Database，Azkaban Web Server和Azkaban Executor Server。azkaban将大多数的状态信息都保存在MySQL中，Azkaban Web Server提供了Web UI，是azkaban主要的管理者，包括project的管理、认证、调度以及对工作流执行过程中的监控等；Azkaban Executor Server用来调度工作流和任务，记录工作流或者任务的日志。</p><p><a href="https://link.zhihu.com/?target=https%3A//myslide.cn/slides/712%3Fvertical%3D1" target="_blank" rel="noopener">流计算任务的处理平台Sloth</a>，是网易首个自研流计算平台，旨在解决公司内各产品日益增长的流计算需求。作为一个计算服务平台，其特点是易用、实时、可靠，为用户节省技术方面（开发、运维）的投入，帮助用户专注于解决产品本身的流计算需求。</p><h2 id="四、数据查询分析"><a href="#四、数据查询分析" class="headerlink" title="四、数据查询分析"></a>四、数据查询分析</h2><p>Hive的核心工作就是把SQL语句翻译成MR程序，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能。Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce。可以将Hive理解为一个客户端工具，将SQL操作转换为相应的MapReduce jobs，然后在hadoop上面运行。Hive支持标准的SQL语法，免去了用户编写MapReduce程序的过程，它的出现可以让那些精通SQL技能、但是不熟悉MapReduce 、编程能力较弱与不擅长Java语言的用户能够在HDFS大规模数据集上很方便地利用SQL 语言查询、汇总、分析数据。</p><p>Hive是为大数据批量处理而生的，Hive的出现解决了传统的关系型数据库(MySql、Oracle)在大数据处理上的瓶颈 。Hive 将执行计划分成map-&gt;shuffle-&gt;reduce-&gt;map-&gt;shuffle-&gt;reduce…的模型。如果一个Query会被编译成多轮MapReduce，则会有更多的写中间结果。由于MapReduce执行框架本身的特点，过多的中间过程会增加整个Query的执行时间。在Hive的运行过程中，用户只需要创建表，导入数据，编写SQL分析语句即可。剩下的过程由Hive框架自动的完成。</p><p>Impala是对Hive的一个补充，可以实现高效的SQL查询。使用Impala来实现SQL on Hadoop，用来进行大数据实时查询分析。通过熟悉的传统关系型数据库的SQL风格来操作大数据，同时数据也是可以存储到HDFS和HBase中的。Impala没有再使用缓慢的Hive+MapReduce批处理，而是通过使用与商用并行关系数据库中类似的分布式查询引擎（由Query Planner、Query Coordinator和Query Exec Engine三部分组成），可以直接从HDFS或HBase中用SELECT、JOIN和统计函数查询数据，从而大大降低了延迟。Impala将整个查询分成一执行计划树，而不是一连串的MapReduce任务，相比Hive没了MapReduce启动时间。</p><p>Hive 适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询，Impala给数据人员提供了快速实验，验证想法的大数据分析工具，可以先使用Hive进行数据转换处理，之后使用Impala在Hive处理好后的数据集上进行快速的数据分析。总的来说：Impala把执行计划表现为一棵完整的执行计划树，可以更自然地分发执行计划到各个Impalad执行查询，而不用像Hive那样把它组合成管道型的map-&gt;reduce模式，以此保证Impala有更好的并发性和避免不必要的中间sort与shuffle。但是Impala不支持UDF，能处理的问题有一定的限制。</p><p>Spark拥有Hadoop MapReduce所具有的特点，它将Job中间输出结果保存在内存中，从而不需要读取HDFS。Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。</p><p>Nutch 是一个开源Java 实现的搜索引擎。它提供了我们运行自己的搜索引擎所需的全部工具，包括全文搜索和Web爬虫。</p><p>Solr用<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/Java" target="_blank" rel="noopener">Java</a>编写、运行在<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/Servlet" target="_blank" rel="noopener">Servlet</a>容器（如<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/Apache_Tomcat" target="_blank" rel="noopener">Apache Tomcat</a>或<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/Jetty" target="_blank" rel="noopener">Jetty</a>）的一个独立的企业级搜索应用的全文搜索服务器。它对外提供类似于Web-service的API接口，用户可以通过http请求，向搜索引擎服务器提交一定格式的XML文件，生成索引；也可以通过Http Get操作提出查找请求，并得到XML格式的返回结果。</p><p>Elasticsearch是一个开源的全文搜索引擎，基于Lucene的搜索服务器，可以快速的储存、搜索和分析海量的数据。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。</p><p>还涉及到一些机器学习语言，比如，Mahout主要目标是创建一些可伸缩的机器学习算法，供开发人员在Apache的许可下免费使用；深度学习框架Caffe以及使用数据流图进行数值计算的开源软件库TensorFlow等，常用的机器学习算法比如，贝叶斯、逻辑回归、决策树、神经网络、协同过滤等。</p><h2 id="五、数据可视化"><a href="#五、数据可视化" class="headerlink" title="五、数据可视化"></a>五、数据可视化</h2><p>对接一些BI平台，将分析得到的数据进行可视化，用于指导决策服务。主流的BI平台比如，国外的敏捷BI Tableau、Qlikview、PowrerBI等，国内的SmallBI和新兴的<a href="https://link.zhihu.com/?target=https%3A//bigdata.163yun.com/youdata%3Fchannel%3DM_zhihu_27696290" target="_blank" rel="noopener">网易有数（可点击这里免费试用）</a>等。</p><p>在上面的每一个阶段，保障数据的安全是不可忽视的问题。</p><p>基于网络身份认证的协议Kerberos，用来在非安全网络中，对个人通信以安全的手段进行身份认证，它允许某实体在非安全网络环境下通信，向另一个实体以一种安全的方式证明自己的身份。</p><p>控制权限的ranger是一个Hadoop集群权限框架，提供操作、监控、管理复杂的数据权限，它提供一个集中的管理机制，管理基于yarn的Hadoop生态圈的所有数据权限。可以对Hadoop生态的组件如Hive，Hbase进行细粒度的数据访问控制。通过操作Ranger控制台，管理员可以轻松的通过配置策略来控制用户访问HDFS文件夹、HDFS文件、数据库、表、字段权限。这些策略可以为不同的用户和组来设置，同时权限可与hadoop无缝对接。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>国家应急管理部成立背景和发展前景展望</title>
      <link href="/2018/05/07/%E5%9B%BD%E5%AE%B6%E5%BA%94%E6%80%A5%E7%AE%A1%E7%90%86%E9%83%A8%E6%88%90%E7%AB%8B%E8%83%8C%E6%99%AF%E5%92%8C%E5%8F%91%E5%B1%95%E5%89%8D%E6%99%AF%E5%B1%95%E6%9C%9B/"/>
      <url>/2018/05/07/%E5%9B%BD%E5%AE%B6%E5%BA%94%E6%80%A5%E7%AE%A1%E7%90%86%E9%83%A8%E6%88%90%E7%AB%8B%E8%83%8C%E6%99%AF%E5%92%8C%E5%8F%91%E5%B1%95%E5%89%8D%E6%99%AF%E5%B1%95%E6%9C%9B/</url>
      
        <content type="html"><![CDATA[<p>2018年3月13日，十三届全国人大一次会议听取了全国人大常委会关于关于国务院机构改革方案的说明。3月17日，全国人大表决通过了国务院机构改革方案。3月21日中共中央印发了《深化党和国家机构改革方案》。改革后，国务院除办公厅外，设置组成部门26个，现有正、副部级机构分别减少8个和7个。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/40091522189494811.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><a id="more"></a><p>这次机构改革被认为是近20年来力度最大的一次，在很多方面超出了外界此前的预期。自上世纪70年代末中国改革开放以来，国务院至今已进行了7次机构改革。这次改革涉及的范围之广，职能调整之深刻，是新中国成立以来前所未有的。现结合此次国务院机构改革，就有关应急管理部的成立背景和发展前景进行阐述。</p><p><strong>一、为什么要组建国家应急管理部</strong></p><p>正如人大报告所言，我国是灾害多发频发的国家，频发到什么程度根据联合国公布数字，全球近年来54起严重灾害事故中，有8起发生在中国，约占全球15%。根据国内各职能部门数据统计，2017年我国发生各类生产安全事故5.3万起、死亡3.8万人，有影响的各类地震810次，森林火灾3000余起，草原火灾50余起,受害草原面积3万公顷。此外，各类洪涝、雪灾、泥石流等自然灾害共造成全国1.4亿人次受灾，881人死亡，98人失踪。乍一看，官方统计数据中，地震和自然灾害等事故远远超过美国和俄罗斯，其他方面与美俄大致持平。</p><p>但众所周知，凡是我国主管行业部门负责统计的数据往往受制因素较多，不完全可信，仅以交通事故统计为例，交管部门统计，2017年我国交通事故亡人数为6.3万人，仅比美国和俄罗斯略高，而实际上，仅卫生部门2014年统计，交通事故亡人数超过16万人。同样的，美国消防协会统计，美国每年火灾总数约为120-140万起，亡人数约为3000-5000人；俄罗斯紧急情况部统计，俄罗斯每年火灾总数约为20余万起，亡人数超过1万人。而我国公布的数据显示每年发生火灾近30万起，死亡人数仅为1000人左右。相信如果据实统计，我国的各类事故总量一定远远高于统计数据，国家决策层对此应该也有深刻的认识。因此摆在我们面前的现实是我国的安全形势远比数据显示的严峻。</p><p>我国的各类事故频发，与之相对应的是防灾和减灾能力的薄弱，首先从政府层次缺乏统一的领导指挥机构，具体的防灾和救灾部门众多，每次遇到大灾难往往习惯于临时组成国家救灾领导小组，由国务院的副总理或省部级领导挂帅，具体协调各相关部门。仅以2008年汶川地震为例，救援队伍中既有政府应急办、安监局、民政局和地震局的工作人员，也有公安民警、消防和武警内卫、水电、交通和森林部队，还有解放军工程、空降和野战部队。温家宝总理那句经典的“是人民在养你们，你们自己看着办”透露出了多少协调的无奈，现场如果没有国家主席和政府总理作为现场指挥，可以想象具体工作的协调难度。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/62371522189509698.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>2014年3月马航事件牵动了全国人民的心，当年的3月10日，两会上有一位政协委员，即北京航空航天大学徐世杰教授曾经提出汲取马航事件的教训，建议国家建立一个国家紧急情况部，但这个提议当时并没有引起大家的重视（这个建议的思路是从俄罗斯那得到的，俄罗斯紧急情况部处理过伊尔库茨克等多起国内外飞机失事事故，曾经有一架载有219人的客机曾经在埃及失事，该国的紧急情况部立即派出3架飞机和救援人员前往事发地，其救援组织效率之高让世界刮目相看，参见视频<a href="https://v.qq.com/x/page/l0018tdjq5n.html）。" target="_blank" rel="noopener">https://v.qq.com/x/page/l0018tdjq5n.html）。</a></p><p>随后2015年6月1日，“东方之星”号客轮由于遭遇突发罕见的强对流天气导致翻沉，整个事故救援过程中，湖南、湖北两个省以及长江航务局、长江干线水上搜救协调中心、长江防总多方协调各个部门，前后参与的部门多达40余个，最终在中央政治局的集体决定下调动了海军三大舰队和海军工程大学参与救援，事故最终导致442人遇难，仅12人生还。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/95671522189494760.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>这一事件充分暴露了我国的应急救援力量建设的短板，仅以统计生还和亡人数为例， 据报道6月3日已搜救出33人，其中14人生还；6月4日报道已有15人生还；6月8日报道又变成了14人生还；6月13日，事故发生两周后，经反复核查才确定生还的为12人。导致获救人员数据变化的主要原因就是由于参与搜救力量多，且来自不同系统和不同单位，在汇总数据时出现了重复现象。</p><p>如果说上述的这些事件还不足以让中央下定决心进行机构整合，2015年8月12日天津港的那一次大爆炸，最终让中央政治局下定决心对防灾和减灾部门进行改革。天津港事故共造成165人遇难，8人失踪，798人受伤，核定的直接经济损失近70亿元。这一事故不仅将天津港口各个行业部门对安全生产的监管责任问题暴露出来，同时引发了全社会对消防员以及其他各方应急处置力量救援能力建设的问题的深思。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/11611522189494871.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>这一事件发生后，中央最终意识到从国家政府层面整合安全生产监督和应急救援力量势在必行。</p><p><strong>二、国外应急管理工作的经验做法</strong></p><p>在如何整合相关部门，组建更有效的应急管理机构方面，我们能够借鉴的经验并不多，且必须学习其他发达国家的做法。作为一个大国，我国的应急救援力量体系建设不可能参考日本、香港或新加坡等小国或地区的经验，也不能参考巴西、印度等国家的经验，而只能参考美国和俄罗斯这两个国家的经验。那么美国和俄罗斯是怎么做的呢</p><p>美国这样一个三权分立的国家，在应急管理上，采取的确是权力绝对集中的做法。1971年美国加州圣费尔南多发生6.6级地震,震中烈度8度强，事故处理过程中，暴露出美国政府各部门各自为政等严重问题，引发民众强烈不满。最终在1979年，卡特总统在任期间，美国联邦政府整合了分散在11个部门的应急管理职能，组建了美国联邦应急管理局（FEMA），以统筹各类灾害的管理。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/8061522189494958.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>FEMA机构设置</p><p>其使命是在任何危险目前，领导和支持全国范围内抵抗风险的应急管理综合程序，通过实施减灾、准备、响应和恢复四项业务，减少生命财产损失，维护社会稳定。至克林顿内阁时期，美国联邦应急管理局通过加大对减灾和准备的投入，有效地减轻了几次重大灾害造成的损失，赢得了公众的认可。</p><p>美国FEMA拍摄的《假日防火宣传片》</p><p>2001年“9·11”事件之后，美国应急管理实践的重点转向反恐，成立了国土安全部，整合了当时联邦22个部门的相关职能，也包括美国联邦应急管理局。2005年“卡特里娜”飓风之后，民众对政府的救灾能力怨声载道，美国又对应急管理体系进行了微调，联邦应急管理局仍然保留在国土安全部的框架之内，只是其局长可直接对接美国总统负责，资源保障和行动能力都得到了提升（详情参见美国FEMA官方网站<a href="https://www.fema.gov/）。" target="_blank" rel="noopener">https://www.fema.gov/）。</a></p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/53241522189494969.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>俄罗斯紧急情况部诞生于苏联解体后，当时俄国内各类突发事件频发，为应对这一现实状况，1991年4月俄罗斯组建国家民防、紧急状态和消除自然灾害后果委员会，负责协调各职能部门联合应对处置突发事件，但由于该机构协调乏力（该委员会不掌握下属机构人事权和财权，仅为议事机构，相当于协调小组），1994年叶利钦总统下令改组该委员会，重新组建紧急情况部。重组后，该部门成为俄罗斯处理突发事件的组织核心，是俄罗斯政府5大“强力”部门之一（另外4个分别是国防部、内务部、联邦安全局和对外情报局）。今天的俄罗斯国防部长绍伊古元帅，曾经担任过国家紧急情况部的部长。</p><p>紧急情况部的工作职能主要为救援救灾，同时是自然灾害预测预报的中心，该部非常重视救灾知识教育，紧急情况部在各州各区各市都设有部门，担任各地日常的事故救援，而在发生重大灾祸的时候，就由联邦总部调动和协调全部的力量，俄罗斯消防部队的22万人都隶属紧急情况部，还有一个总数2万人的国家搜救队，专门负责发生灾害时的一般搜救工作，大多数的队员同时掌握多种的专业技能，具有在水下陆上和空中任何复杂地理和气候条件下完成救助任务的本领。</p><p>紧急情况部下设11个处、8个局，包括消防局、搜寻与救护局、飞行事故处置局、国家紧急状态处理中心等。该部下设专门委员会用以协调和实施某些行动，包括打击森林火灾、水灾、海上和水域突发事件、海事协调等多个跨机构委员会。紧急情况部还可以通过总理办公室可以请求获得私人、国防部或内务部队的支持，也就是说，该部拥有国际协调权及在必要时调用本地资源的权限。实际上，前苏联其他国家也都设有和俄罗斯一样的紧急情况部。</p><p>俄罗斯的紧急情况部设有高风险救援行动中心，专门负责处理具有高风险的各种紧急情况，比如核事故还有化学放射污染事故等，他们拥有一流的工兵、训犬专家、机器人专家、化学和放射性物质防护专家、潜水和登山专家，还广泛使用机器人救助。各地紧急情况部的航空救助队、小型船只救助队、心理医疗救助队等等一应俱全（参见俄罗斯紧急情况部救援教学视频<a href="https://v.qq.com/x/page/e01782mtj5u.html）。" target="_blank" rel="noopener">https://v.qq.com/x/page/e01782mtj5u.html）。</a></p><p>曾经从也门共和国疏散俄罗斯公民，曾向设在坦桑尼亚和扎伊尔的卢旺达难民营输送过人道主义救援物资，曾经远征挪威海救援出事的“共青团员号”核潜艇，处理过列宁斯克和哈萨克斯坦发生的煤气爆炸曾在暴雨引发大坝决口的巴什科托斯坦进行搜救，也曾向前南斯拉夫地区输送人道主义物资。他们也参加了对2000年莫斯科连环爆炸案的处理、对2002年4月俄罗斯知名政界人士、克拉斯诺亚尔斯克边疆区行政长官列别德所乘直升机失事的救援、对远东森林大火的抢救以及对阿富汗战后重建的援助活动等（详情参见俄罗斯紧急情况部官方网站<a href="https://www.fema.gov/）。" target="_blank" rel="noopener">https://www.fema.gov/）。</a></p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/82841522189495089.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>可以看出，我国这次组建应急管理部也在一定程度上借鉴了美国和俄罗斯的先进经验，但需要注意的是我国此次组建的应急管理部并不是完全照抄美俄，而是结合我国的国情，整合优化应急力量和资源，按照中央的设想，最终将形成<font color="red">“<strong>统一指挥、专常兼备、反应灵敏、上下联动、平战结合</strong>”</font>的中国特色应急管理体制。</p><p><strong>三、国家应急管理部内部机构将如何设置</strong></p><p>按照方案，新组建的应急管理部门整合了10个不同部门的职责和5支应急救援队伍。10个不同部门的职责分别涉及2个正部级机构全部职责（国家安全生产监督管理总局、国务院办公厅应急管理办公室）、1个正军级机构职责（公安部消防局，第7局，相当于副部级）、3个副部级机构职责【即国家防汛抗旱总指挥部（设在水利部，副部级）和水利部的水旱灾害防治职责；国家减灾委员会（设在民政部，副部级）和民政部救灾司的救灾职责；国务院抗震救灾指挥部（设在国家地震局，副部级）和中国地震局的震灾应急救援职责】、3个正司局级部门职责【国家森林防火指挥部 （正司局级，公安部第16局）和国家林业局的森林防火相关职责；国土资源部地质环境司（又称国家地质灾害应急管理办公室，正司局级）的地质灾害防治职责；农业部草原监理中心防火处和畜牧司（正司局级）的草原防火职责】。</p><p>5支应急救援队伍分别涉及1个正部级、2个正军级和2个副部级机构【即正部级的国家安全生产监督管理总局（下设国家安全生产应急救援指挥中心），正军级的公安消防部队和武警森林指挥部，副部级的国家煤矿安全监察局（下设矿山救护队）和中国地震局（下设国家地震应急搜救中心）】。可以这样讲，这样的整合力度历史罕见，当属建国以来首次。</p><p>从目前政府公布的信息来看，今后国家应急管理部基本上是以国家安全生产监督总局为基础，在此之上整合其他他部门，从前期国务院公布的《国务院关于部委管理的国家局设置的通知》（国发〔2018〕7号）来看，国家地震局、消防局、森林防火指挥部等均未能成为应急管理部管理的国家局，这也就意味着这些部门将成为应急管理部的司级内设机构。</p><p>根据《国务院行政机构设置和编制管理条例》第二十一条规定，国务院组成部门的司级内设机构的领导职数为一正二副；国务院组成部门管理的国家行政机构的司级内设机构的领导职数根据工作需要为一正二副或者一正一副。</p><p>这也就意味着，现在存在于各个部门中的不少正职领导或副职将随着机构设置而被裁减，例如森林武警指挥部中设有2名正军职干部，若干副军职干部，改制后将仅保留一正两副或一正一副。</p><p>据此，国家应急管理部的机构设置基本清晰可见。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/78801522189495080.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>按照国务院机构改革方案，以上这些部门和机构将重新整合，涉及到的人员将全部转隶为应急救援管理部门。</p><p>需要注意的是，按照方案要求，安全监督工作，也就是从事预防灾害工作的人员将构成今后应急管理部门的机关工作人员，而救灾人员将作为基层，由应急管理部管理，这一点与我国一贯坚持的对各类事故灾害“预防为主”的思想一脉相承。</p><p><strong>四、现役部队转制后前景展望</strong></p><p>按照方案，公安消防部队、武警森林部队转制后，与安全生产等应急救援队伍一并作为综合性常备应急骨干力量，由应急管理部管理。这就意味着这支重新组建的综合性应急救援国家队，将承担国家急难险重的救援任务，成为应急救援的主力军。</p><p>从这三支队伍的基本情况来看，共计约20万人，其中占比最大的是消防部队，从公开数据来看，其部队按照国家行政区划下设31个正师级总队，300余个旅级和正团级支队，2800余个大队和中队，总人数约18万人。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/65121522189495178.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>其次是森林武警部队，下设北京、新疆、内蒙古、黑龙江、吉林、四川、云南、福建、甘肃等9个正师级总队，㺲余个旅级和正团级支队，总人数约2万余人。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/72401522189495184.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>第三是国家安全生产应急救援队，这支队伍主要由国家安监局下属的国家安全生产应急救援指挥中心统一指挥，主要由中石油、中石化的消防队和国家矿山应急救援队组成，目前全国共有50余个应急救援队，总人数约2000人。</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/82241522189495288.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>图为黑龙江省国家安全生产应急救援队分布</p><p>从中央的顶层设计来看，消防、森林和安监的应急救援队伍将实行专门管理和政策保障，国务院机构改革方案还专门提出 “制定符合其自身特点的职务职级序列和管理办法，提高职业荣誉感，保持有生力量和战斗力”。众所周知，要提高某一职业的荣誉感只有两个办法，一个是提高其社会地位，一个是提高工资福利待遇。</p><p>因此，这支队伍今后虽然属于行政编制，但必将享受比一般党政机关公务员更好地福利和待遇，从中央专门组建应急救援部的决心来看，其待遇有可能超过军队。需要注意的是消防部门原有的监督执法人员隶属应急救援部管理后，其防火监督职责合并入应急管理部，不再自成体系，其人员待遇势可能发生相应变化。</p><p><strong>五、国家应急管理部内设机构将如何运转</strong></p><p>中央在此次组建应急管理部的过程中，专门提出分级负责的原则，即一般性灾害由地方各级政府负责，应急管理部代表中央统一响应支援；发生特别重大灾害时，应急管理部作为指挥部，协助中央指定的负责同志组织应急处置工作，保证政令畅通、指挥有效。</p><p>从这个思路展开，可以预见，今后应急管理部门的防灾工作（包括安全生产监督、消防管理等监督执法体系）将可能采取“条块结合”的办法。由地方政府具体负责，上级部门仅予以监督和指导，即与各级行政执法管理机构类似，财政业务经费均由地方政府保障，罚款等收缴资金直接缴纳地方财政。</p><p>而应急管理部门的救灾工作，也就是各级应急救援队的管理则势必采取垂直管理的做法，全国各地应急救援队将采取统一的训练执勤标准，由中央财政统一予以保障供给。一旦遇重大突发事件，中央直接调派，确保第一时间抢险救灾。</p><p>从一定意义上来讲，这种做法既参考了俄罗斯紧急情况部垂直管理的经验做法，又结合了我国现在的实际国情，是一条名副其实的中国特色应急救援道路。</p><p>从职业发展发面来看，今后应急管理势必走向更加专业化和职业化的道路。参考美国和俄罗斯的经验来看，美俄应急管理部门均拥有多所院校，美国应急管理学院（EMI）位于马里兰州的埃米茨堡，与国家消防学院（NFS）共用一个校园，每年大约有17000名学员就读该中心的住校学习课程。俄罗斯自紧急情况部成立以来，先后建立了俄罗斯国家消防学院、伊万诺沃国立消防学院、圣彼得堡国立消防学院、俄罗斯民防学院等。这些教学机构源源不断地为俄紧急情况部输送专业人才，从而使俄预防和处理灾害事故的能力得到极大加强。</p><p>目前我国只有一所直接隶属于国家安全生产监督管理总局的安全专业院校，即华北科技学院（原为北京煤炭管理干部学院），</p><p><img src="http://ego-file.soperson.com/itver/13969158619/201712081350/78381522189495504.jpeg" alt="" title="国家应急管理部成立背景和发展前景展望"></p><p>一所国家安监局与地方政府共建高校，即河南理工大学，一所消防院校，即公安消防部队高等专科学校，以及若干散落在各个高等院校中的安全工程、消防工程等专业学生，每年培养的安全专业人才总计不过千人。可以预见，应急管理部成立后，国家势必提高应急管理防灾和减灾人员的素质，势必建立一所种类齐全的国家防灾减灾大学，发达省市将逐步建立起自己的防灾减灾学院，以利于人才培养。</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 应急 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10年后的第一篇博客</title>
      <link href="/2018/05/03/10%E5%B9%B4%E5%90%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2018/05/03/10%E5%B9%B4%E5%90%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p>大概10年前，在北京写下了第一篇技术博客。后来陆陆续续在CSDN上写了几十篇原创，上百篇转载。访问排名一度进入8000名。由于工作的关系，没有坚持下去。最近心血来潮想写一篇，突然发现CSDN的账号居然已经无法登录。于是用GitHub搭建了一个博客。</p><p>挺cool ~~~</p><img src="/2018/05/03/10年后的第一篇博客/01.png" title="图1">]]></content>
      
      
      <categories>
          
          <category> 原创 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/05/02/hello-world/"/>
      <url>/2018/05/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
